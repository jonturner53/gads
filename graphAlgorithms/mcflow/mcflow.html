<html>
<head>
<title>Minimum Cost Flows</title>
<link type="text/css" rel="stylesheet" href="../../main.css">
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</head>
<body bgcolor=ffffff>
\(
\newcommand{\cost}{\textit{cost}}
\newcommand{\excess}{\textit{excess}}
\newcommand{\lab}{\lambda}
\)

<h1>Minimum Cost Flows<sup>&copy</sup></h1>
The minimum cost flow problem is a generalization of the maximum flow
problem in which each edge $e=(u,v)$ is assigned a real-valued <i>cost</i>
denoted $\cost_e(u,v)$. The cost of a flow $f$
is defined as $\sum_{e=(u,v)} f_e(u,v) cost_e(u,v)$, so edge costs
represent a cost per unit of flow.
A flow is said to be a minimum cost flow, if no other flow with the same
value has a smaller cost.
The objective of the problem is to find a minimum cost flow of maximum value.
Costs are defined to be skew-symmetric,
so $\cost_e(v,u) = -cost_e(u,v)$.
An example of a flow graph with edge costs is shown below.
<p>
<div  style="text-align:center;">
<img width="40%" src="figs/mcflow1.png"><br>
</div>
<p>
Observe that the while the flow shown is a maximum flow,
it is not a minimum cost flow. It can be converted to a
minimum cost flow by adding one unit of flow around the cycle
$(s,a,d,b,s)$. Note that the sum of the edge costs in this cycle
(in the direction indicated) is negative.
This observation is the basis for one approach to finding minimum cost flows.

<h2>Negative Cycle Reduction</h2>
The negative cycle reduction method [Klein67] can convert any flow into a minimum cost
flow with the same flow value, by repeatedly finding negative cost cycles
with positive residual capacity, then pushing enough flow around the
cycle to saturate one or more edges.
When no unsaturated negative cycles remain, the flow is a minimum cost flow.
(Note that a graph may have a minimum cost flow, even when it has negative cycles;
however, if the graph contains a negative cycle with <i>unbounded capacity</i>,
then there is no minimum cost flow.)
The theorem below provides the required justification for the
cycle reduction method.
<p>
<i>Theorem 1</i>. A flow has minimum cost if and only if there is no
negative cost cycle that is unsaturated.
<p>
<i>Proof</i>.
Clearly if there is a negative cost cycle that is unsaturated,
a lower cost flow of the same value can be obtained by adding flow to the
cycle. It remains to show that a flow that is not minimum must have some
unsaturated negative cost cycle. Let $f$ be a flow
on a graph $G$
that is not minimimum cost and let $f^*$ be a flow with the same value
that has a lower cost. Let $D$ be the graph defined by the edges for
which $f^*-f$ is positive. Now, find a cycle in $D$ (by flow conservation,
there must be one) and add enough flow to the cycle to saturate at least one
of its edges. Repeat until there are no more cycles. At least one of the
cycles identified in this process must have negative cost. $\Box$
<p>
If cycles are chosen arbitrarily, the number of steps
used by the cycle reduction method can be quite large. Indeed, all that can be
said in general is that the number of steps is finite, because the cycle
reduction process maintains integrality of flows and there is a finite number
of integer flows with a given value.
If costs are also integral, the number of steps is at most
$mCU$ where $C$ is the largest cost magnitude and $U$ is the
maximum edge capacity.
The quantity $mCU$ bounds the maximum difference between the largest cost flow
and smallest cost flow. The cycle bound follows from the fact that each cycle
reduces the cost by at least 1. 
Using the Bellman-Moore shortest path algorithm
each cycle-finding step takes $O(mn)$ time,
so the worst-case running time is $O(m^2nCU)$.
Surprisingly, the algorithm can perform reasonably well in practice,
in spite of its very poor worst-case bound.
Several factors account for this.
In practice, the difference between the cost of the initial flow and the 
actual minimium flow cost is much smaller than $mCU$ and each step reduces
the cost by more than 1 (usually much more).
<p>
A <i>Javascript</i> implementation of the negative cycle reduction method
(or Klein's algorithm) appears below.
<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
let g;        // shared reference to flow graph
let Cost;     // Cost[u] is cost of path to u from source in findCycle
let link;     // link[] is parent edge of u
let q;        // queue used in findCycle
let cycleIds; // array used to label cycles with an integer identifier

export default function ncrK(fg) {
    g = fg;
    Cost = new Float32Array(g.n+1);
    link = new Int32Array(g.n+1);
    q = new List(g.n);
    cycleIds = new Int8Array(g.n+1);

    let u = findCycle();
    while (u != 0) {
        augment(u, trace); u = findCycle(); cycles++;
    }
}

function findCycle() {
    Cost.fill(0); link.fill(0); q.clear();
    for (let u = 1; u <= g.n; u++) q.enq(u);

    let last = q.last(); // each pass completes when last removed from q
    while (!q.empty()) {
        let u = q.deq();
        for (let e = g.firstAt(u); e != 0; e = g.nextAt(u,e)) {
            if (g.res(e,u) == 0) continue;
            let v = g.mate(u,e);
            if (Cost[v] > Cost[u] + g.costFrom(e,u)) {
                link[v] = e;
                Cost[v] = Cost[u] +  g.costFrom(e,u);
                if (!q.contains(v)) q.enq(v);
            }
        }

        if (u == last) {
            let v = cycleCheck();
            if (v != 0) return v;
            last = q.last();
        }
    }
    return 0;
}

function cycleCheck() {
    cycleIds.fill(0);
    let u = 1; let id = 1;
    while (u <= g.n) {
        // follow parent pointers from u, labeling new vertices
        // seen with the value of id, so we can recognize a loop
        let v = u; let e;
        while (cycleIds[v] == 0) {
            cycleIds[v] = id;
            e = link[v];
            if (e == 0) break;
            v = g.mate(v,e);
        }
        if (cycleIds[v] == id && e != 0) return v;
        
        // find next unlabeled vertex 
        while (u <= g.n && cycleIds[u] != 0) u++;
        id++;
    }
    return 0;
}

function augment(z) {
    // determine residual capacity of cycle
    let u = z; let e; let f = Infinity;
    do {
        e = link[u]; u = g.mate(u,e);
        f = Math.min(f,g.res(e,u));
    } while (u != z);

    // add flow to saturate cycle
    u = z;
    do {
        e = link[u]; u = g.mate(u,e); g.addFlow(e,u,f);
    } while (u != z);
}
</textarea> <p>
Notice that the <code>findCycles</code> method checks for cycles
in the parent pointers at the end of every pass. Since the
<code>cycleCheck</code> method is $O(n)$, this does not increase the
worst-case performance beyond $O(mn)$ per cycle found, and short cycles
are typically found long before pass $n$.
The following script can be used to run <code>mcflowK</code> in the web app.
<p>
<pre style="padding-left:5%">
let g = randomFlograph(12,3);
g.randomCapacities(randomInteger, 1, 19);
g.randomCosts(randomInteger, -4, 9);
let [ts] = ncrK(g,1);
log(ts)
</pre>
<p>
This produces the output below.
<p> <textarea rows="14" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
{
a->[b:5@1 d:21@6 e:14@7]
b[c:12@4 d:15 e:9@5 h:10@-1]
c[b:7@-3 e:16 f:12@8]
d[c:15@6 e:13@8]
e[c:11@3 d:4@8 f:4@-4]
f[c:8@-1 d:10@-1 e:19@7]
g[d:5@9 f:1@4 i:15@-1]
h[i:7@-4 l:21@7]
i[c:2@1 h:5]
j[e:13@5 f:1@1 l:4@1]
k[c:9@1 f:13@-3 g:17@6 i:17@-3 l:18@7]
->l
}

initial cost: 0
cycles with cycle capacity, totalCost
[h:7 i:5 h] 5 -20
[c:16 e:4 f:8 c] 4 -40
[b:10 h:2 i:2 c:7 b] 2 -54

{
a->[b:5@1 d:21@6 e:14@7]
b[c:12@4 d:15 e:9@5 h:10@-1/2]
c[b:7@-3/2 e:16/4 f:12@8]
d[c:15@6 e:13@8]
e[c:11@3 d:4@8 f:4@-4/4]
f[c:8@-1/4 d:10@-1 e:19@7]
g[d:5@9 f:1@4 i:15@-1]
h[i:7@-4/7 l:21@7]
i[c:2@1/2 h:5/5]
j[e:13@5 f:1@1 l:4@1]
k[c:9@1 f:13@-3 g:17@6 i:17@-3 l:18@7]
->l
}
</textarea> <p>
The trace output shows each cycle found, followed by its residual
capacity and the total flow cost after flow is added to the cycle.
In the flow graph, non-zero costs are shown preceded by an @-symbol;
non-zero flows are preceded by a forward slash.
In this example, the initial flow values are all zero.
If a max value flow is computed first,
the final flow is a min cost, max flow.
The following script can be used to observe the performance
experimentally on random graphs.
<p><pre style="padding-left:5%">
let n=40; let d=4;
let g = randomFlograph(n,d);
g.randomCapacities(randomInteger, 1, 999);
g.randomCosts(randomInteger, -99, 99);

let c0 = g.totalCost();
let t = Date.now(); let [,stats] = ncrK(g); t = Date.now() - t;
let bound = ~~((c0-g.totalCost())/1000);
let cycles = stats.cycles;
let passes = stats.passes;
let steps = ~~(stats.steps/1000);
log(`n=${g.n} m=${g.m} bound=${bound}K cycles=${cycles} passes=${passes} ` +
    `steps=${steps}K ${t}ms`);
let n=40; let d=4;
let g = randomFlograph(n,d);
g.randomCapacities(randomInteger, 1, 999);
g.randomCosts(randomInteger, -99, 99);

let c0 = g.totalCost();
let t = Date.now(); let [,stats] = ncrK(g); t = Date.now() - t;
let bound = ~~((c0-g.totalCost())/1000);
let cycles = stats.cycles;
let passes = stats.passes;
let steps = ~~(stats.steps/1000);
log(`n=${g.n} m=${g.m} bound=${bound}K cycles=${cycles} passes=${passes} ` +
    `steps=${steps}K ${t}ms`);</pre><p>
In the sample output below, the first three lines show the effect
of doubling the number of edges, while holding the number of vertices constant.
The last three show the effect of doubling the number of vertices while
holding the number of edges constant (approximately).
<p><pre style="padding-left:5%">
n= 40 m=156 bound= 663K cycles= 45 passes= 164 steps=  30K   4ms 
n= 40 m=312 bound=2286K cycles=171 passes= 477 steps= 218K  24ms 
n= 40 m=624 bound=6192K cycles=473 passes= 770 steps= 802K  77ms 
n= 80 m=632 bound=4413K cycles=442 passes=1434 steps=1340K 130ms 
n=160 m=636 bound=1643K cycles=161 passes=1102 steps= 853K  90ms 
</pre><p>
Also, note that roughly half the edges
in the random graphs have negative costs, so negative cycles are plentiful.
Still, the number of cycles found is 
far smaller than the bound on the number of cycles.
Also, the number of passes per cycle in <code>findCycle</code> is usually
less than 4.
The total number of steps and the running time
appear to increase roughly quadratically with the number of edges.
While these are very limited results, they suggest that performance in practice
can be much better than the worst-case analysis suggests.

<h2>Minimum Mean-Cost Cycle Reduction</h2>
Better worst-case bounds can be obtained for the cycle reduction method
if negative cycles are selected with more care.
The <i>mean cost</i> of a cycle is the cycle cost divided by the number of
edges in the cycle. Goldman and Tarjan [GolTar87] show that if the
negative cycle with the smallest (most negative) mean cost is selected,
the number of cycle selection steps is $O(mn \min(\log(nC),m\log n))$
where $C$ is the largest edge cost magnitude. 
Karp [Karp78] proved the following characterization of
the minimum mean cycle cost for any strongly connected digraph with edge costs.
$$
\lambda^* = \min_u \max_{0\leq i < n} \frac{C_i(u) - C_n(u)}{n-i}
$$
where $\lambda^*$ is the minimum mean cycle cost and $C_i(u)$ is the
length of a minimum cost path (not necessarily simple)
from a fixed vertex $s$ to $u$ with exactly $i$ edges
(or $\infty$ if there is no such path).
This can be computed in $O(mn)$ time using the recurrence
$$ C_{i+1}(v) = \min_{(u,v)} \{ C_i(u) + cost(u,v) \} $$
Karp does not give an explicit method for identifying a cycle,
but Chaturvedi and McConnell [ChatMc17] show that if
&ldquo;parent pointers&rdquo; $P_i(u)$ are computed,
along with $C_i(u)$, one can find a cycle by proceeding back along the length
$n$ path defined by the parent pointers from any vertex $u$ that satisfies
the condition in the theorem.
Any cycle on this path is guaranteed to have minimum mean cost.
<p>
A <i>Javascript</i> implementation of the algorithm is shown below.
<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
let g;          // shared reference to flow graph

// data used by findCycle
let C;      // C[i][u]=cost of min cost path (mcp) of length i to u in g
let P;      // P[i][u]=edge to parent of u in mcp of length i to u

export default function mcflowKGT(fg) {
    g = fg;
    C = new Array(); P = new Array();
    for (let i = 0; i <= g.n; i++) {
        C.push(new Float32Array(g.n+1));
        P.push(new Int32Array(g.n+1));
    }

    let [u,i] = findCycle();
    while (u != 0) {
        augment(u,i);
        [u,i] = findCycle();
    }
}

/** Find a negative cost cycle in the residual graph.
 *  @return a pair [u,i] where u is a vertex on a min mean cost cycle and
 *  i is a value for which the path starting at P[i][u] and continuing up
 *  the tree defined by the parent points contains a min mean cost cycle.
 */
function findCycle() {
    let n = g.n;
    // First, compute shortest path lengths of length i <= n
    C[0].fill(0); P[0].fill(0);
    for (let i = 1; i <= n; i++) {
        for (let u = 1; u <= n; u++) {
            // compute C[i][u] the cost of min cost path to u with i edges
            C[i][u] = Infinity; 
            for (let e = g.firstAt(u); e != 0; e = g.nextAt(u,e)) {
                let v = g.mate(u,e);
                if (g.res(e,v) > 0 && C[i-1][v] + g.cost(e,v) < C[i][u]) {
                    C[i][u] = C[i-1][v] + g.cost(e,v); P[i][u] = e;
                }
            }
        }
    }

    // Now apply Karp's equation to find cost of least mean cost cycle
    let meanCost = new Array(n+1);
    let umin = 1;
    for (let u = 1; u <= n; u++) {
        meanCost[u] = [0, (C[n][u] - C[0][u]) / n];
        for (let i = 0; i < n; i++) {
            findCycleSteps++;
            let mc = (C[n][u] - C[i][u]) / (n - i);
            if (mc > meanCost[u][1]) {
                meanCost[u] = [n-i,mc];
            }
        }
        if (meanCost[u][1] < meanCost[umin][1]) umin = u;
    }
    let mmc = meanCost[umin][1];
    if (mmc >= 0) return [0,0];

    // Now follow parent pointers from umin, while checking for cycle
    let mark = new Int32Array(n+1);
    let u = umin; let i = n; mark[u] = i;
    while (i > 0) {
        let e = P[i][u]; let v = g.mate(u,e);
        if (mark[v]) return [v, mark[v]];
        mark[v] = i-1; u = v; i--;
    }
    assert(false, 'findpath: program error');
}

/** Add flow to a negative-cost cycle.
 *  Adds as much flow as possible to the cycle, reducing the cost
 *  without changing the flow value.
 *  @param z is a vertex on a min mean cost cycle
 *  @param i is an integer for which path at length i to z
 *  back up the parent pointers to z is the required cycle
 */
function augment(z,i) {
    // C[i][z] is min mean cycle cost and P values give parent edges
    let u = z; let j = i; let f = Infinity;
    do {
        let e = P[j--][u];
        let v = g.mate(u,e);
        f = Math.min(f,g.res(e,v));
        u = v;
    } while (u != z);

    // now add flow to the path to saturate cycle
    u = z; j = i;
    do {
        let e = P[j--][u];
        let v = g.mate(u,e);
        g.addFlow(e,v,f);
        u = v;
    } while (u != z);
}
</textarea> <p>
While this version does use fewer cycles than the basic version of Klein's
method, the improvement is fairly modest on random graphs, and the time required
to find each cycle is much larger, making it slower than the basic version,
at least for this class of graphs.
The following script can be used to demonstrate this.

<p><pre style="padding-left:5%">
let n=40; let d=4;
let g = randomFlograph(n,d);
g.randomCapacities(randomInteger, 1, 999);
g.randomCosts(randomInteger, -99, 99);

let t = Date.now(); let [,stats] = ncrKGT(g); t = Date.now() - t;
let bound = ~~((-g.totalCost())/1000);
let cycles = stats.cycles;
let steps = ~~(stats.steps/1000);
log(`n=${g.n} m=${g.m} bound=${bound}K cycles=${cycles} ` +
    `steps=${steps}K ${t}ms`);
</pre><p>
Sample results appear below.
<p><pre style="padding-left:5%">
n= 40 m=156 bound= 919K cycles= 46 steps=  662K   67ms 
n= 40 m=312 bound=2235K cycles=123 steps= 3295K  305ms 
n= 40 m=624 bound=5702K cycles=268 steps=13862K 1228ms 
n= 80 m=632 bound=4461K cycles=246 steps=26563K 2353ms 
n=160 m=636 bound=2265K cycles=147 steps=33913K 3063ms 
</pre><p>
Comparing these to the results from the last section,
the new algorithm does require fewer negative cycles
but for random graphs, the difference is not nearly enough
to offset the extra time spent finding the cycles.

<h2>Jewell's Algorithm</h2>

If the current flow in a flow graph has minimum cost, then adding flow
to an augmenting path $P$ of minimum cost produces another minimum cost flow.
To understand why, suppose that the new flow is not minimum cost.
In that case, it has a negative cycle that is unsaturated. Since the original flow did
not have a negative cycle, the path augmentation must have caused some
previously saturated edges to become unsaturated,
and these must be reversals of edges on $P$. Let's start with a
simple case where a single edge $e=(u,v)$ appears on $P$
and its reversal $(v,u)$ appears on $C$. Let $H$ be the graph
consisting of $P$ together with $C$; any edge that is in both $P$ and $C$
appears twice in $H$. Since $C$ is a negative cost
cycle, the cost of $H$ is less than the cost of $P$.
If $(u,v)$ and $(v,u)$ are both removed from $H$, the resulting
graph has the same cost as $H$ (by skew symmetry of edge costs).
This graph forms an augmenting path from $s$ to $t$ and has a lower
cost than $P$, contradicting the fact that $P$ is a minimum cost
augmenting path.
<p>
The general case is handled in a similar way. Again, let $H$ be the
graph obtained by combining $P$ and $C$. Observe that this graph
is Eulerian. That is, every vertex but $s$ and $t$ has the same
number of incoming edges as outgoing edges, while $s$ has one
extra outgoing edge and $t$ has one extra incoming edge.
Because the graph is Eulerian, it can be decomposed into a single
$s$-$t$ path and 0 or more cycles. Moreover, the graph obtained
by removing all matched pairs $(u,v)$ on $P$ and $(v,u)$ on $C$ is also Eulerian
and has the same cost as $H$. Since the cycles in this graph consist
of edges that were unsaturated in the original flow graph,
they are non-negative, so the remaining path is an augmenting
path with lower cost than $P$,
again yielding a contradiction. This argument yields the following theorem
<p>
<i>Theorem 2</i>. Given a flow graph with a minimum cost flow,
adding flow to a minimum cost augmenting path yields another
minumum cost flow.
<p>
This theorem is the basis for a minimum cost augmenting path algorithm,
which can be applied to any flow graph in which there are no negative cost
cycles. With integer edge capacities, each augmenting path step increases
the flow by at least 1, so the number of steps is bounded by the maximum flow value.
If the Bellman-Moore algorithm is used to find the least-cost augmenting path,
the running time is $O(mnP)$ where $P$ is the number of paths that
must be found to reach a maximum flow; it is bounded by the maximum flow value,
but is often much smaller.
<p>
This algorithm was first described in [Jewell1958].
A <i>Javascript</i> implementation appears below.
<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
let g;        // shared reference to flow graph
let link;     // link[u] is parent edge of u
let Cost;     // Cost[u] is pathcost to u, used by findpath
let q;        // queue of vertices, used by findpath

export default function mcflowJ(fg, leastCost=false) {
    g = fg;
    link = new Int32Array(g.n+1);
    Cost = new Float32Array(g.n+1);
    q = new List(g.n);

    while (findpath()) {
        let [rcap,pathCost] = pathProps();
        if (leastCost && pathCost >= 0) break;
        augment(rcap); paths++;
    }
}

function findpath() {
    q.clear(); link.fill(0); Cost.fill(Infinity);
    Cost[g.source] = 0; q.enq(g.source);
    let pass = 0; let last = q.last();
    while (!q.empty()) {
        let u = q.deq();
        for (let e = g.firstAt(u); e; e = g.nextAt(u,e)) {
            if (g.res(e,u) == 0) continue;
            let v = g.mate(u,e); steps++;
            if (Cost[v] > Cost[u] + g.costFrom(e,u)) {
                Cost[v] = Cost[u] + g.costFrom(e,u); link[v] = e;
                if (!q.contains(v)) q.enq(v);
            }
        }
        if (u == last) {
            fassert(pass < g.n, 'mcflowJ: negative cost cycle detected');
            pass++; last = q.last();
        }
    }
    return link[g.sink] ? true : false;
}

function pathProps() {
    let u = g.sink; let rcap = Infinity; let cost = 0;
    for (let e = link[u]; e != 0; e = link[u]) {
        u = g.mate(u,e);
        rcap = Math.min(rcap, g.res(e,u));
        cost += g.costFrom(e,u);
    }
    return [rcap,cost];
}

function augment(rcap) {
    let u = g.sink; let ts = '';
    for (let e = link[u]; e; e = link[u]) {
        steps++; u = g.mate(u,e); g.addFlow(e,u,rcap);
    }
}
</textarea> <p>
Observe that in this implementation, the path search terminates the
first time it finds an augmenting path of non-negative cost
if the <code>leastCost</code> flag is true.
The resulting flow is a least cost flow among all flows (regardless of their value).
This turns out to be useful in some algorithms for bipartite matching,
as will be discussed in the next section.
If the flag is false, the returned flow is a maximum flow with minimum cost.
The following script can be used to demonstrate the algorithm.
<p><pre style="padding-left:5%">
let g = randomFlograph(12,3);
g.randomCapacities(randomInteger, 1, 19);
g.randomCosts(randomInteger, -9, 9);
let gg = new Flograph(g.n,g.edgeRange); gg.assign(g);
ncrK(g); let [ts] = mcflowJ(g,0,1);  log(ts);
log('-----------------');
ncrK(gg);    [ts] = mcflowJ(gg,1,1); log(ts);
</pre><p>
Observe that the program is run twice on the same graph.
In the first case, it produces a min cost maximum flow
and in the second case it produces a min cost flow that may not
have a maximum value.
An example demonstrating the difference between these
cases appears below.
<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
{
a->[c:25@1 e:17@-1 f:6@-2]
b[c:14@-5 d:6@6 e:8@-1 f:17@-2/9]
c[b:1@-7/1 d:19@-1/19 e:14@-2]
d[e:19@-1 f:19@-2/8 h:17@-7/17]
e[b:2@-9/2 c:12@-4/12]
f[c:7@-5/3 d:6@-8/6 e:17@-8/14]
g[b:17@-8/6 f:3 l:2@-9]
h[c:6@-4/5 f:6@-2/6 i:10@8 k:11@-1/7 l:24@6]
i[b:9@7 k:6/2 l:22@-9]
j
k[g:6@1/6 h:1@-9/1 i:2@-8/2 j:19@7]
->l
}

paths with added flow and resulting flow cost
[a:6 f:4 c:5 h:4 k:2 i:22 l] 2 -552
[a:4 f:2 c:3 h:10 i:20 l] 2 -560
[a:2 f:9 b:14 c:1 h:8 i:18 l] 1 -562
[a:1 f:8 b:6 g:2 l] 1 -563
[a:25 c:1 b:5 g:1 l] 1 -558
[a:24 c:7 f:6 h:7 i:17 l] 6 -516
[a:18 c:1 f:7 b:4 g:6 k:9 h:1 i:11 l] 1 -501
[a:17 e:14 f:6 b:3 g:5 k:8 h:24 l] 3 -432

{
a->[c:25@1/8 e:17@-1/3 f:6@-2/6]
b[c:14@-5 d:6@6 e:8@-1 f:17@-2/3]
c[b:1@-7/1 d:19@-1/19 e:14@-2]
d[e:19@-1 f:19@-2/8 h:17@-7/17]
e[b:2@-9/2 c:12@-4/12]
f[c:7@-5 d:6@-8/6 e:17@-8/11]
g[b:17@-8 f:3 l:2@-9/2]
h[c:6@-4 f:6@-2 i:10@8/10 k:11@-1/5 l:24@6/3]
i[b:9@7 k:6 l:22@-9/12]
j
k[g:6@1/2 h:1@-9/1 i:2@-8/2 j:19@7]
->l
}
 
----------------- 

{
a->[c:25@1 e:17@-1 f:6@-2]
b[e:8@-1 f:17@-2/9 d:6@6 c:14@-5]
c[d:19@-1/19 e:14@-2 b:1@-7/1]
d[h:17@-7/17 e:19@-1 f:19@-2/8]
e[b:2@-9/2 c:12@-4/12]
f[d:6@-8/6 e:17@-8/14 c:7@-5/3]
g[l:2@-9 b:17@-8/6 f:3]
h[l:24@6 k:11@-1/7 f:6@-2/6 c:6@-4/5 i:10@8]
i[l:22@-9 b:9@7 k:6/2]
j
k[g:6@1/6 h:1@-9/1 i:2@-8/2 j:19@7]
->l
}

paths with added flow and resulting flow cost
[a:6 f:4 c:5 h:4 k:2 i:22 l] 2 -552
[a:4 f:2 c:3 h:10 i:20 l] 2 -560
[a:2 f:9 b:14 c:1 h:8 i:18 l] 1 -562
[a:1 f:8 b:6 g:2 l] 1 -563

{
a->[c:25@1 e:17@-1 f:6@-2/6]
b[e:8@-1 f:17@-2/7 d:6@6 c:14@-5/1]
c[d:19@-1/19 e:14@-2 b:1@-7/1]
d[h:17@-7/17 e:19@-1 f:19@-2/8]
e[b:2@-9/2 c:12@-4/12]
f[d:6@-8/6 e:17@-8/14 c:7@-5/7]
g[l:2@-9/1 b:17@-8/5 f:3]
h[l:24@6 k:11@-1/9 f:6@-2/6 c:6@-4 i:10@8/3]
i[l:22@-9/5 b:9@7 k:6]
j
k[g:6@1/6 h:1@-9/1 i:2@-8/2 j:19@7]
->l
}
</textarea> <p>
The trace output starts with the graph showing the initial flow
computed by <code>ncrK</code>.
For each path search, it shows the
selected augmenting path with its residual capacities, the flow
added to the path and the flow cost immediately following the augmentation.
Note that in the first case, the smallest flow cost is achieved after the
first four augmenting path steps. The remaining steps increase the flow value,
while increasing the cost. Also note that the second run halts after the
first four steps.
<p>
The following script can be used to evaluate the performance of the algorithm.
<p><pre style="padding-left:5%">
let n=40; let d=4;
let g = randomFlograph(n,d);
g.randomCapacities(randomInteger, 1, 999);
g.randomCosts(randomInteger, -99, 99);
ncrK(g);
let t = Date.now(); let [,stats] = mcflowJ(g); t = Date.now() - t;
let paths = stats.paths;
let steps = ~~(stats.steps/1000);
log(`n=${g.n} m=${g.m} flow=${g.totalFlow()} paths=${paths} steps=${steps}K ${t}ms`);
</pre><p>
Note that the generated graphs include negative cost edges,
so Klein's algorithm is used first to eliminate unsaturated negative cycles.
The statistics shown exclude this preprocessing step.
<p><pre style="padding-left:5%">
n= 40 m=156 flow=  528 paths=  8 steps=  4K  1ms 
n= 40 m=312 flow= 4608 paths= 53 steps=  8K  7ms 
n= 40 m=624 flow=18005 paths=178 steps=271K 40ms 
n= 80 m=632 flow= 6184 paths=115 steps=279K 42ms 
n=160 m=636 flow=  958 paths= 27 steps= 62K 10ms 
</pre><p>

<h2>Speeding Up Jewell's Algorithm</h2>
One can speedup Jewell's algorithm by using the Edmonds-Karp edge transform,
previously used to apply Dijkstra's algorithm to the all-pairs shortest path problem.
As flow is added to augmenting paths, the edge transform is adjusted to
keep the transformed edge costs of the unsaturated edges from becoming negative.
To facilitate this adjustment, it's
helpful to look at the edge cost transformation in a more general way.
For each vertex $u$ in a flow graph, let $\lab(u)$ be a numeric label.
Any such labeling can be used to define an alternate edge cost function
for an edge $e$.
$$
cost_e^{\lab}(u,v) = cost_e(u,v) + \lab(u) - \lab(v)
$$
Note that this cost function shifts the cost of a $u$-$v$ path by
$\lab(u) - \lab(v)$, so a path $p$ from $u$ to $v$ is
a shortest path with respect to $\cost^\lab$ if and only if it
is a shortest path with respect to the original costs.
Thus, shortest paths can be computed based on any such labeling.
If $\cost_e^\lab(u,v) \geq 0$ for all unsaturated edges $e=(u,v)$, one can compute
shortest paths using Dijkstra's algorithm.
If the vertex labels are all shortest path distances from some source
vertex $s$, then $\cost_e^\lab(u,v) \geq 0$ is guaranteed by the
<a href="./shortPaths.html#sptTheorem">shortest path tree</a> theorem.
Moreover, any edge $e=(u,v)$ in a shortest path tree rooted at $s$ has
$\cost_e^\lab(u,v)=0$.
<p>
In the context of the min cost flow problem, a vertex labeling is said
to be <i>valid</i> with respect to a flow $f$ if
$\cost_e^\lab(u,v)\geq 0$ for all edges $e=(u,v)$ with $res_e(u,v)>0$.
Given a valid labeling, a minimum cost augmenting path can be computed
using Dijkstra's algorithm. Let $c(u)$ be the cost of the path from $s$ to $u$.
Before flow is added to the augmenting path
$\cost_e^\lab(u,v) + c(u) - c(v)\geq 0$
for all edges $e=(u,v)$ with positive residual capacity,
by the shortest path tree theorem and for edges on a shortest path
$\cost_e^\lab(u,v) + c(u) - c(v)=0$. After flow is added to the augmenting
path, the only new edges are reverse path edges.
Consequently, after flow is added, all unsaturated edges $(u,v)$ satisfy
$$
cost_e^\lab(u,v) + c(u) - c(v) \geq 0
$$
or equivalently
$$
cost_e(u,v) + (\lab(u) + c(u)) - (\lab(v) + c(v)) \geq 0
$$
Thus, one can maintain the validity of the labeling, by adding $c(u)$ to the
current label for all vertices $u$.
Now this works if all vertices are reachable from a source vertex,
but what if some are not? For these unreachable vertices, it suffices to
add to their labels a value that is at least as large as the largest finite path cost.
<p>
The initial labels can be computed using the Bellman-Moore algorithm,
as was done for the all-pairs shortest path problem.

From that point on, shortest paths can be computed
using Dijkstra's algorithm, updating
the labels after each step as described above.
This yields a running time of
$O(mn + S(m,n)P)$, where $S(m,n)$ is the time required for the shortest path
computation (which may vary depending on the data structures used) and
$P$ is the number of paths.
As before $P$ is bounded by the maximum flow value.
<p>
A <i>Javascript</i> implementation of this version of Jewell's algorithm
(referred to here as the JEK algorithm, for Jewell, Edmonds and Karp)
appears below.
<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
let g;        // shared reference to flow graph
let link;     // link[u] is parent edge of u
let lambda;   // lambda[u] is vertex label used to make costs non-negative

let border;   // heap used by findpath
let Cost;     // array of path costs, used by findpath
let q;        // queue of vertices, used by findpath

export default function mcflowJEK(fg, leastCost=false) {
    g = fg; 
    link = new Int32Array(g.n+1);
    lambda = new Float32Array(g.n+1);
    border = new ArrayHeap(g.n,2);
    Cost = new Float32Array(g.n+1);
    q = new List(g.n);

    initLabels();
    while (findpath()) {
        let [rcap,pathCost] = pathProps();
        if (leastCost && pathCost >= 0) break;
        augment(rcap); paths++;
    }
}

function initLabels() {
    q.clear(); link.fill(0); Cost.fill(0);

    // put all vertices in queue, effectively searching from pseudo-source
    for (let u = 1; u <= g.n; u++) q.enq(u);
    let pass = 0; let last = q.last;
    while (!q.empty()) {
        let u = q.deq();
        for (let e = g.firstAt(u); e; e = g.nextAt(u,e)) {
            if (g.res(e,u) == 0) continue;
            let v = g.mate(u,e); steps++;
            if (Cost[v] > Cost[u] + g.costFrom(e,u)) {
                Cost[v] = Cost[u] + g.costFrom(e,u); link[v] = e;
                if (!q.contains(v)) q.enq(v);
            }
        }
        if (u == last) {
            assert(pass < g.n, 'mcflowJEK: negative cost cycle detected');
            pass++; last = q.last;
        }
            
    }
    for (let u = 1; u <= g.n; u++) lambda[u] = Cost[u];
}

function findpath() {
    border.clear(); link.fill(0); Cost.fill(Infinity);

    let cmax = -Infinity;    // maximum finite path cost
    Cost[g.source] = 0; border.insert(g.source,0);
    while (!border.empty()) {
        let u = border.deletemin();
        cmax = Math.max(cmax, Cost[u]);
        for (let e = g.firstAt(u); e != 0; e = g.nextAt(u,e)) {
            if (g.res(e,u) == 0) continue;
            let v = g.mate(u,e);
            if (Cost[v] > Cost[u] + g.costFrom(e,u) + (lambda[u]-lambda[v])) {
                link[v] = e;
                Cost[v] = Cost[u] + g.costFrom(e,u) + (lambda[u]-lambda[v]);
                if (!border.contains(v)) border.insert(v,Cost[v]);
                else border.changekey(v,Cost[v]);
            }
        }
    }
    if (!link[g.sink]) return false;
    for (let u = 1; u <= g.n; u++) {
        lambda[u] += Math.min(Cost[u],cmax);
    }
    return true;
}

function pathProps() {
    let u = g.sink; let rcap = Infinity; let cost = 0;
    for (let e = link[u]; e != 0; e = link[u]) {
        u = g.mate(u,e);
        rcap = Math.min(rcap, g.res(e,u));
        cost += g.costFrom(e,u);
    }
    return [rcap,cost];
}

function augment(rcap) {
    let u = g.sink; let ts = '';
    for (let e = link[u]; e; e = link[u]) {
        u = g.mate(u,e); g.addFlow(e,u,rcap);
    }
}
</textarea> <p>
Sample performance data appears below.
<p><pre style="padding-left:5%">
n= 40 m=156 flow= 1120 paths= 15 steps= 10K  1ms 
n= 40 m=312 flow= 5194 paths= 72 steps= 74K  6ms 
n= 40 m=624 flow=17101 paths=194 steps=336K 22ms 
n= 80 m=632 flow= 4127 paths= 78 steps=171K 10ms 
n=160 m=636 flow=  948 paths= 24 steps= 62K  4ms 
</pre><p>

<h2>Negative Cycle Reduction Revisited</h2>
In a graph that does have unsaturated negative cost cycles, these cycles must
be eliminated before minimum cost augmentation can be applied.
An extension of the JEK algorithm can be used to eliminate negative cycles,
providing an alternative to Klein's algorithm.
This extension uses <i>preflows</i> instead of flows.
Recall that a preflow is similar to a flow in that
it respects the capacity constraints, but differs by allowing the flow balance
condition to be violated. That is, the flow entering a (non-source/sink) vertex
need not be balanced by the flow leaving.
The <i>excess flow</i> at a vertex is defined to be the difference between
the flow entering and the flow leaving, and may be either positive or negative.
<p>
Preflows can be used to eliminate negative cycles by simply saturating
every edge with negative cost, at the start of the algorithm. This yields
a preflow in which some vertices have positive excess and some have negative excess.
Those in the first group are called <i>sources</i> while those in the second
group are called <i>sinks</i>.
From this initial state, negative cycles can be eliminated by repeatedly finding
an augmenting path from any source to any sink and adding enough flow to either
saturate the path or balance the flow at the source or sink.
Once all excesses are reduced to zero, the preflow is a flow with no
unsaturated negative cycles.
<p>
A <i>Javascript</i> implementation of this method appears below.
<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
let g;        // shared reference to flow graph
let link;     // link[u] is parent edge of u
let lambda;   // lambda[u] is vertex label used to make costs non-negative
let excess;   // excess[u] is excess flow entering u
let sources;  // list of sources (nodes with positive excess)
let sinks;    // list of sinks (nodes with negative excess)

let border;   // heap used by findpath
let Cost;     // array of path costs used by findpath

export default function ncrJEK(fg) {
    g = fg; 
    link = new Int32Array(g.n+1);
    lambda = new Float32Array(g.n+1);
    excess = new Int32Array(g.n+1);
    sources = new List(g.n); sources.addPrev(); // doubly linked
    sinks = new List(g.n); sinks.addPrev();
    border = new ArrayHeap(g.n,2);
    Cost = new Float32Array(g.n+1);

    // saturate negative cost edges that are not already saturated
    for (let u = 1; u <= g.n; u++) {
        for (let e = g.firstAt(u); e; e = g.nextAt(u,e)) {
            if (g.costFrom(e,u) < 0 && g.res(e,u) > 0) {
                let r = g.res(e,u); g.addFlow(e,u,r);
                excess[u] -= r; excess[g.mate(u,e)] += r;
            }
        }
    }

    // initialize source and sinks based on excess
    sources.clear(); sinks.clear();
    for (let u = 1; u <= g.n; u++) {
        if (excess[u] > 0) sources.enq(u);
        else if (excess[u] < 0) sinks.enq(u);
    }

    let t = findpath();
    while (t) { augment(t); t = findpath(); }
}

function findpath() {
    border.clear(); link.fill(0); Cost.fill(Infinity);

    // search from all sources in parallel
    for (let s = sources.first(); s != 0; s = sources.next(s)) {
        Cost[s] = 0; border.insert(s,0); steps++;
    }
    let t = 0; let cmax = -Infinity;
    while (!border.empty()) {
        let u = border.deletemin();
        cmax = Math.max(cmax,Cost[u]);
        if (sinks.contains(u)) t = u;
            // don't stop yet as need all c values to update lambda
        for (let e = g.firstAt(u); e != 0; e = g.nextAt(u,e)) {
            steps++;
            if (g.res(e,u) == 0) continue;
            let v = g.mate(u,e);
            if (Cost[v] > Cost[u] + g.costFrom(e,u) + (lambda[u]-lambda[v])) {
                link[v] = e;
                Cost[v] = Cost[u] + g.costFrom(e,u) + (lambda[u]-lambda[v]);
                if (!border.contains(v)) border.insert(v,Cost[v]);
                else border.changekey(v,Cost[v]);
            }
        }
    }
    if (t != 0) { // adjust labels
        for (let u = 1; u <= g.n; u++) {
            lambda[u] += Math.min(Cost[u],cmax);
        }
        steps += g.n;
    }
    return t;
}

function augment(t) {
    let u = t; let delta = Infinity;
    for (let e = link[u]; e != 0; e = link[u]) {
        u = g.mate(u,e); delta = Math.min(delta, g.res(e,u));
    }
    delta = Math.min(delta, excess[u]);
    delta = Math.min(delta, -excess[t]);

    u = t; let ts = ''; let cost = 0;
    for (let e = link[u]; e != 0; e = link[u]) {
        u = g.mate(u,e); g.addFlow(e,u,delta);
    }
    excess[u] -= delta; excess[t] += delta;
    if (excess[u] == 0) sources.delete(u);
    if (excess[t] == 0) sinks.delete(t);
}
</textarea> <p>

An example run showing the trace output appears below.

<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
{
a->[b:8@2 d:25@1 f:17]
b[d:12@8 e:18@-2/18]
c[e:14@1 f:17@3]
d[b:1@6 c:19@1 e:12@1 f:9@9]
e[b:15@5 d:6@-4/6 f:15@2 k:10@2]
f[c:8@5 d:19@1 e:12@4]
g[h:19 i:10@5 l:16@8]
h[b:3@1 f:14 g:16@-3/16]
i[j:14@6]
j[b:16@-3/16 c:3@7 g:2@6 h:13@6 l:21@5]
k[d:11@7 j:4@1 l:25@2]
->l
}

sources, sinks and paths with added flow and resulting flow cost
[d:6 e:12 g:16] [b:-2 h:-16 j:-16]
  [e:10 k:4 j] 4 -144
[d:6 e:8 g:16] [b:-2 h:-16 j:-12]
  [g:19 h:3 b:16 j] 3 -132
[d:6 e:8 g:13] [b:-2 h:-16 j:-9]
  [e:18 b:13 j] 8 -92
[d:6 g:13] [b:-2 h:-16 j:-1]
  [d:12 e:10 b:5 j] 1 -86
[d:5 g:13] [b:-2 h:-16]
  [d:11 e:9 b] 2 -80
[d:3 g:13] [h:-16]
  [g:16 h] 13 -80
[d:3] [h:-3]
  [d:9 e:7 b:3 h] 3 -74

{
a->[b:8@2 d:25@1 f:17]
b[d:12@8 e:18@-2/4]
c[e:14@1 f:17@3]
d[b:1@6 c:19@1 e:12@1/6 f:9@9]
e[b:15@5 d:6@-4/6 f:15@2 k:10@2/4]
f[c:8@5 d:19@1 e:12@4]
g[h:19/16 i:10@5 l:16@8]
h[b:3@1 f:14 g:16@-3/16]
i[j:14@6]
j[b:16@-3/4 c:3@7 g:2@6 h:13@6 l:21@5]
k[d:11@7 j:4@1/4 l:25@2]
->l
}
</textarea> <p>
Note that the sources and sinks are shown before each augmenting
path step, along with their excess values.
Since each augmenting path step reduces the positive excess
of some vertex by at least 1, the number of steps is bounded
by the sum of the positive excess values at the start of execution.
This is just the sum of the residual capacities of the unsaturated negative
edges at the start of execution.
<p>
Here is some sample output comparing <code>ncrK</code> to <code>ncrJEK</code>
on random graphs.
<p><pre style="padding-left:5%">
k   n= 40 m=156 steps=  34K   5ms 
jek n= 40 m=156 steps=  59K   5ms 
k   n= 40 m=312 steps= 287K  29ms 
jek n= 40 m=312 steps= 187K  12ms 
k   n= 40 m=624 steps= 841K  75ms 
jek n= 40 m=624 steps= 505K  33ms 
k   n= 80 m=632 steps=1364K 123ms 
jek n= 80 m=632 steps= 931K  54ms 
k   n=160 m=636 steps=1021K  99ms 
jek n=160 m=636 steps=1565K  78ms 
</pre><p>
Note that one can integrate preflows into Jewell's algorithm, so that it
can directly accommodate negative cycles. 
However separating the negative cycle elimination provides some
useful flexibility that is lost when it is combined with the maximum flow
computaton.

<h2>Orlin's Scaling Algorithm</h2>

Because the worst-case time required for Jewell's algorithm's grows
in proportion with the edge capacities, it is not considered a polynomial
time algorithm.
Orlin's scaling algorithm [AMO93] can be viewed as a generalization of
Jewell's algorithm, which does run in polynomial time.
It uses a scale factor $\Delta$,
and limits augmenting paths to those edges with a residual capacity
$\geq \Delta$ (referred to as <i>eligible</i> edges).
Once all augmenting paths for a particular value of $\Delta$ are found,
$\Delta$ is halved and the augmenting path searches are resumed using
the newly eligible edges, in addition to those that remain eligible from
the previous phase.
This allows it to avoid augmenting path steps that make only a small improvement
in the flow.
If the period between successive changes to $\Delta$ is called a <i>phase</i>,
the number of augmenting path steps per phase
is $O(m)$ (this is justified below), and if $U$ is the maximum edge capacity,
the number of phases is at most $\lceil \log_2 U \rceil$.
Conseqently, the total number of augmenting path steps is
$O(m \log U)$ and the running time is $O(S(m,n) m \log U)$ where again $S(m,n)$
is the time for the shortest path computation.
<p>
Within each phase, the vertex labels $\lab(u)$ are maintained
as with the second version of Jewell's algorithm described earlier.
The labeling must ensure that edges with residual capacity $\geq\Delta$,
have $\cost^\lab\geq 0$. Preflows can be used to facilitate this.
At the end of a phase, the reduction in $\Delta$
makes more edges eligible for use, potentially making
$\cost^\lab$ negative for some newly eligible edges.
This is addressed by adding $\Delta$ units of flow to each
edge for which $\cost^\lab$ is negative, changing the excess flows
at the endpoints and possibly creating new sources and sinks. 
<p>
The algorithm can be stated as follows.
Initialize $\lab(u)=0$ for all $u$,
$\excess(s) = F$ (where $F$ is the value of a maximum flow),
$\excess(t)=-F$
and $\excess(u)=0$ for all other vertices $u$.
Let $\Delta$ be the largest power of 2 that does not exceed
the maximum edge capacity.
Now, repeat the following step so long as $\Delta\geq 1$.
<p style="padding-left:5%">
For each edge $(u,v)$ with $\cost_e^\lab (u,v)<0$ and $res_e(u,v)\geq \Delta$,
add $\Delta$ units of flow from $u$ to $v$, add $\Delta$ to $\excess(v)$
and subtract $\Delta$ from $\excess(u)$.
<p style="padding-left:5%">
While there is a path with residual capacity $\geq \Delta$ from a vertex $\sigma$
with $\excess(\sigma)\geq \Delta$ to a vertex
$\tau$ with $\excess(\tau)\leq -\Delta$,
add $\Delta$ units of flow to the path, subtract $\Delta$ from $\excess(\sigma)$
and add $\Delta$ to $\excess(\tau)$; 
also, for each vertex $u$, add $\min\{c(u),c_{max}\}$ to $\lab(u)$
where $c(u)$ is the minimum
path cost from $\sigma$ to $u$ using eligible edges
and $c_{max}$ is the largest finite value of $c(u)$.
<p style="padding-left:5%">
When no augmenting paths remain, divide $\Delta$ by 2.
<p>
Observe that because each phase begins by saturating negative cost edges,
Orlin's algorithm can cope with graphs that have negative cost cycles,
so no preprocessing step is needed.
<p>
When a phase ends, either no vertex has positive excess $\geq\Delta$
or no vertex has negative excess $\leq\Delta$. Until this condition holds, there must
be an augmenting path with residual capacity $\geq\Delta$.
At the start of each phase, the total positive excess is $\lt 2n\Delta$.
Initially, this is true because the maximum flow value is less than $n$ times
the largest edge capacity.
At the start of a new phase with scaling factor $\Delta$,
every vertex has a positive excess $\lt 2\Delta$ or every vertex has
a negative excess $\gt 2\Delta$.
In either case, the total positive excess is $\lt 2n\Delta$.
<p>
In the first step of a phase, newly eligible edges increase the total positive
excess by at most $m\Delta$, so the total positive excess is $\lt (2n+m)\Delta$
after the first step. Since each augmenting path reduces the positive excess of
some source by $\Delta$, there are at most $2n+m$ augmenting path steps per phase.
This justifies the earlier statement that the number of path searches per phase
is $O(m)$.
<p>
A <i>Javascript</i> version of Orlin's algorithm appears below.
<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
let g;        // shared reference to flow graph
let Delta;    // scaling parameter
let link;     // link[u] is parent edge of u
let lambda;   // lambda[u] is vertex label used to make costs non-negative
let excess;   // excess[u] is excess flow entering u
let sources;  // list of sources (nodes with positive excess)
let sinks;    // list of sinks (nodes with negative excess)

let border;   // heap used by findpath
let Cost;     // array of path costs used by findpath

export default function mcflowO(fg) {
    g = fg;

    link = new Int32Array(g.n+1);
    lambda = new Float32Array(g.n+1);
    excess = new Int32Array(g.n+1);
    sources = new List(g.n); sources.addPrev(); // doubly linked
    sinks = new List(g.n); sinks.addPrev();
    border = new ArrayHeap(g.n,2);
    Cost = new Float32Array(g.n+1);

    // Initialize scaling factor
    let maxcap = 0;
    for (let e = g.first(); e != 0; e = g.next(e)) {
        maxcap = Math.max(maxcap, g.cap(e,));
    }
    for (Delta = 1; 2*Delta <= maxcap; Delta <<= 1) {}

    // Determine a max flow so that we can initialize excess
    // values at s and t
    maxflowD(g);
    excess[g.source] = g.totalFlow();
    excess[g.sink] = -g.totalFlow();
    g.clearFlow();

    while (Delta >= 1) {
        newPhase();
        let t = findpath();
        while (t) {
            augment(t); t = findpath();
        }
        Delta /= 2;
    }
}

function newPhase() {
    for (let e = g.first(); e != 0; e = g.next(e)) {
        let u = g.tail(e); let v = g.head(e);
        if (g.res(e,u) >= Delta) {
            if (g.costFrom(e,u) + (lambda[u] - lambda[v]) < 0) {
                g.addFlow(e,u,Delta);
                excess[u] -= Delta; excess[v] += Delta;
            }
        }
        if (g.res(e,v) >= Delta) {
            if (g.costFrom(e,v) + (lambda[v] - lambda[u]) < 0) {
                g.addFlow(e,v,Delta);
                excess[v] -= Delta; excess[u] += Delta;
            }
        }
    }

    // identify candidate sources and sinks
    sources.clear(); sinks.clear();
        steps++;
        if (excess[u] >= Delta) {
            sources.enq(u);
        } else if (excess[u] <= -Delta) {
            sinks.enq(u);
        }
    }
    return;
}

function findpath() {
    border.clear(); link.fill(0); Cost.fill(Infinity);

    // search from all sources in parallel
    for (let s = sources.first(); s != 0; s = sources.next(s)) {
        Cost[s] = 0; border.insert(s,0); steps++;
    }
    let t = 0;
    let cmax = -Infinity;
    while (!border.empty()) {
        let u = border.deletemin();
        cmax = Math.max(cmax,Cost[u]);
        if (t == 0 && sinks.contains(u)) t = u;
            // don't stop yet as need all c values to update lambda
        for (let e = g.firstAt(u); e != 0; e = g.nextAt(u,e)) {
            if (g.res(e,u) < Delta) continue;
            let v = g.mate(u,e);
            if (Cost[v] > Cost[u] + g.costFrom(e,u) + (lambda[u]-lambda[v])) {
                link[v] = e;
                Cost[v] = Cost[u] + g.costFrom(e,u) + (lambda[u]-lambda[v]);
                if (!border.contains(v)) border.insert(v,Cost[v]);
                else border.changekey(v,Cost[v]);
            }
        }
    }
    if (t != 0) { // adjust labels
        for (let u = 1; u <= g.n; u++) {
            lambda[u] += Math.min(Cost[u],cmax);
        }
    }
    return t;
}

function augment(t) {
    let u = t;
    for (let e = link[u]; e != 0; e = link[u]) {
        u = g.mate(u,e); g.addFlow(e,u,Delta);
    }
    excess[u] -= Delta; excess[t] += Delta;
    if (excess[u] < Delta) sources.delete(u);
    if (excess[t] > -Delta) sinks.delete(t);
}
</textarea> <p>
Some sample output from a demonstration run appears below.
<p> <textarea rows="14" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
sources, sinks and paths with added flow and resulting flow cost
[b:8 e:8 l:8] [a:-8 h:-8 j:-8]
  [b:8 a] 8 -32
[e:8 l:8] [h:-8 j:-8]
  [e:8 h] 8 -16
[l:8] [j:-8]
  [l:8 j] 8 0

[c:8 d:4 f:4] [e:-12 i:-4]
  [f:4 e] 4 8
[c:8 d:4] [e:-8 i:-4]
  [c:4 i] 4 8
[c:4 d:4] [e:-8]
  [d:4 e] 4 -8
[c:4] [e:-4]
  [c:4 e] 4 0

[d:2] [f:-2]
  [d:3 f] 2 -6

[d:1] [f:-1]
  [d:1 f] 1 -9

{
a->[b:9@-2 c:10@7 d:1@7]
b[f:8]
c[b:4@-3 e:4@2 f:1@2]
d[b:9@9 e:9@9 f:3@-2/3]
e[b:7@9 c:6@-2 d:6@4 f:7@-1 i:9@7]
f[b:1@5 c:2@6 d:3@-1/3]
g[b:4@-2 e:8@1 j:5@9 l:9@3]
h[c:2@5 d:3@-1 e:9@-2 f:5@6 g:9@7]
i[b:1@1 c:6]
j[c:1@4 i:8 l:12@-2]
k[l:5@9]
->l
}
</textarea> <p>
The trace output is similar to that shown for Jewell's algorithm.
The only real difference is the blank lines separating the phases.
<p>
Here is some sample data comparing Orlin's algorithm to the
JEK algorithm on random graphs. The results for JEK include
a preliminary step to eliminate negative cycles.
<p><pre style="padding-left:5%">
jek n= 40 m=156 steps=  76K  7ms 
o   n= 40 m=156 steps=  95K  8ms 
jek n= 40 m=312 steps= 256K 21ms 
o   n= 40 m=312 steps= 290K 18ms 
jek n= 40 m=624 steps= 810K 59ms 
o   n= 40 m=624 steps= 892K 56ms 
jek n= 80 m=632 steps=1037K 66ms 
o   n= 80 m=632 steps=1076K 60ms 
jek n=160 m=636 steps=1540K 85ms 
o   n=160 m=636 steps=1077K 52ms 
</pre><p>

<h2>References</h2>
<dl>
<dt> [AMO93]
<dd> <i>Network Flows, Theory, Algorithms and Applications</i>
     by R. K Ahuja, T. L. Magnanti and J. B. Orlin.
     Prentice Hall, 1993.
<dt> [ChatMc17]
<dd> &ldquo;A note on finding minimum mean cycles&rdquo; by
     Mmanu Chaturvedi and Ross M. McConnell. In
     <i>Information Processing Letters</i> 11/2017.
<dt> [GolTar87]
<dd> &ldquo;Finding Minimum-Cost Circulations by Cancelling
     Negative Cycles,&rdquo; by A. V. Goldberg and R. E. Tarjan. In
     <i>Proceedings of the ACM Symposiumm on Theoretical Computer Science</i>,
     1988.
<dt> [Karp78]
<dd> &ldquo;A characterization of the minimum cycle mean in a diagraph,&rdquo;
     by R. Karp. In <i>Discrete Mathematics</i>, 1978.
<dt> [Klein67]
<dd> &ldquo;A primal method for minimal cost flows.&rdquo;
     <i>Management Science</i>, 1967.
<dt> [Tarjan87]
<dd> <i>Network Algorithms and Data Structures</i> by Robert E. Tarjan.
     Society for Industrial and Applied Mathematics, 1987.
</dl>
<hr> <h4>&copy; Jonathan Turner - 2022</h4>
<script src="../googleAnalytics.js"></script>
</body>
</html>
