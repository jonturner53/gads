<html>
<head>
<title>Minimum Spanning Trees</title>
<link type="text/css" rel="stylesheet" href="../../main.css">
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-41SPK9725S"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-41SPK9725S');
</script>
</head>
<body bgcolor=ffffff>
<h1>Minimum Spanning Trees<sup>&copy</sup></h1>

A <i>spanning tree</i> in a connected, undirected graph is a subtree of
the graph that includes all its vertices. 
In an edge-weighted graph, the objective of 
the <i>minimum spanning tree problem</i>
is to find a spanning tree for which the sum of the
edge weights is as small as possible.
The diagram below shows an edge-weighted graph with
bold edges that highlight a minimum spanning tree.  
<p>
<div  style="text-align:center;">
<img width="17%" src="figs/mstExample.png"><br>
</div>
<p>

This problem arises frequently in applications and often appears as
a sub-problem within other optimization problems.

<h2>The Greedy Method</h2>

The <i>greedy method</i> is a general approach to finding a minimum
spanning tree, and is the basis for several specific algorithms.
(The version of the greedy method described here is a simplification
of the one described in [Tarjan87]).
A <i>cut</i> in a graph is a division of the vertices into
two subsets $X$ and $X'$.
An edge is said to <i>cross the cut</i> if one of its
endpoints is in $X$ and the other in $X'$.
For brevity, a minimum weight edge is called a <i>lightest</i> edge.
The greedy method selects tree edges by repeatedly applying the following rule.
<p style="padding-left:5%;">
Select a cut with at least one edge crossing the cut, but no tree edges
(a <i>tree edge</i> is any edge that has been selected in a previous step).
Select a lightest edge crossing the cut and add it to the tree.
<p>
The method terminates when there are no eligible cuts.
At this point, the tree edges define a minimum spanning tree if the
graph is connected and a minimum spanning forest,
if it is not.

Notice that the greedy method does not say what cut to select or how
to do it efficiently.
It turns out that no matter how these choices are made,
the greedy method produces a minimum spanning tree (or forest).
This establishes the correctness of any specific algorithm that implements
the general greedy method, meaning that one doesn't need to prove correctness
separately for each algorithm.
Defining algorithms in terms of a general method like this
sheds light on what is essential and
what is incidental, and helps clarify the fundamental
similarities and differences among algorithms.
The correctness of the greedy method can be established 
by showing that it maintains the following invariant.
<p style="padding-left: 5%">
There is a minimum spanning forest containing all of the tree edges.
<p>
The invariant is trivially true before the first tree edge is selected.
Suppose now, that it is true before one step of the greedy method.
Let $e=\{x,y\}$ be the selected edge, and let $F$ be a
minimum spanning forest containing all the previously selected tree edges.
If $e$ is an edge in $F$, then after the rule is applied, $F$ still contains
all the tree edges.
If $e$ is not in $F$, then there must be a path from $x$ to $y$ in $F$
and some edge $e'$ on this path must be in the cut from which $e$ was selected,
as shown below.
<p>
<div  style="text-align:center;">
<img width="20%" src="figs/mstGreedy1.png"><br>
</div>
<p>
Since the cut contains no tree edges, $e'$ cannot be a tree edge,
so $\textit{weight}(e) \leq \textit{weight}(e')$.
Consequently, the forest obtained from $F$ by replacing $e'$ with $e$
is also a minimum spanning forest.
This result is summarized in the following theorem.
<p style="padding-left: 5%;">
<i>Theorem 1</i>.
The edges selected by the greedy method define a minimum spanning tree if
the graph is connected and a minimum spanning forest if it is not.

<h2>Prim's Algorithm</h2>

Prim's algorithm [Prim57, Johnson75] is a particular instantiation of
the greedy method that applies to connected graphs and
constructs the minimum spanning tree incrementally, by repeatedly selecting
edges from the cut separating a partially constructed tree from the
rest of the graph.
Starting from some initial vertex $s$, the algorithm
repeatedly applies the following selection rule.
<p style="padding-left:5%;">
Select a lightest edge with exactly one endpoint in the tree containing
$s$ and add it to the set of tree edges.
<p>
When it is no longer possible to apply the selection rule, the selected
edges define a minimum spanning tree.
<p>
The key to an efficient implementation of Prim's algorithm is a
data structure that allows the lightest edge incident to the partial tree
to be quickly identified. One way to do this is to use a heap to
store the vertices that are on the <i>border</i> between the partial
tree and the remainder of the graph. A border vertex is one that
is not in the tree but does have one or more edges linking it to
tree vertices. A border vertex $u$ is stored in the heap, using a key
equal to the weight of the lightest edge joining $u$ to a tree vertex.
The identity of that lightest edge is stored as $\textit{light}(u)$.
Note that this implies that if $u$ has the smallest key of any vertex in
the heap, then $\textit{light}(u)$ is the lightest edge with one endpoint
in the tree.
With these definitions, the selection rule
can be rephrased as follows.
<p style="padding-left:5%;">
Remove the vertex $u$ with the smallest key from the border heap
and add $\textit{light}(u)$ to the set of tree edges.
<p>
To ensure that this leads to a valid algorithm, it's also necessary to
maintain the following invariant.
<p style="padding-left:5%;">
Before each selection step, the border heap
defines the set of border vertices and
for each border vertex $u$, $\textit{light}(u)$
is the lightest edge joining $u$ to a tree vertex.
<p>
<p>
Initially, the start vertex $s$ is the only tree vertex,
so the invariant can be established by
setting $\textit{light}(u)=\{s,u\}$ for each neighbor $u$ of $s$ and
adding $u$ to the border heap with a key value of 
$\textit{weight}(\textit{light}(u))$.
<p>
To maintain the invariant after removing $u$ from the heap,
the neighbors of $u$ are <i>scanned</i>. For each neighor $v$ that
is being scanned for the first time $\textit{light}(v)$ is set to $\{u,v\}$
and $v$ is added to the heap with a key value of
$\textit{weight}(\textit{light}(v))$.
For each neighbor that is already in the border set,
the edge $\{u,v\}$ is compared with $\textit{light}(v)$.
If $\{u,v\}$ is lighter than $\textit{light}(v)$, $\textit{light}(v)$ is
set to $\{u,v\}$ and the key of $v$ in the heap is reduced to
$\textit{weight}(\textit{light}(v))$.
<p>
When there are no more border vertices the algorithm completes,
and for each tree vertex $u\neq s$,
$\textit{light}(u)$ can be viewed as the edge connecting $u$
to its &ldquo;parent&rdquo; in the minimum spanning tree.
A <i>Javascript</i> implementation of the main part of the algorithm is shown
below.
<p> <textarea rows="14" cols="80" readonly
          style="font-size: 95%;background-color:lightCyan">
let light = new Int32Array(g.n+1).fill(-1);
let s = 1; light(s) = 0; border.insert(s, 0);
while (!border.empty()) {
    let u = border.deletemin();
    for (let e = g.firstAt(u); e != 0; e = g.nextAt(u,e)) {
        let v = g.mate(u,e);
        if (light[v] < 0) {
            border.insert(v, g.weight(e)); light[v] = e;
        } else if (h.contains(v) && g.weight(e) < h.key(v)) {
            border.changekey(v, g.weight(e)); light[v] = e;
        }
    }
}
</textarea>
<p>
In this implementation $\textit{light}(u)$ is initialized to $-1$
for all $u \neq s$ and $\textit{light}(s)$ is set to zero.
Once a vertex $u$ is scanned, $\textit{light}(u) \geq 0$.
Also, the heap is intialized to contained $s$.
This means that the first loop iteration serves to establish the
invariant, while the remaining iterations maintain the invariant as
described above.
<p>
Prim's algorithm can be implemented using any heap data structure,
but it is most commonly implemented using an
<a href="../../dataStructures/heaps/heaps.html#ArrayHeap">array heap</a>.
If the heap operations are excluded, the running time
is $O(m)$, since the innermost loop is executed no more than $2m$ times.
The heap operations include $n-1$ <i>insert</i> operations,
$n-1$ <i>deletemin</i> operations and up to $m-n$ <i>changekey</i>
operations, all of which reduce the key value.
This leads to a worst-case running time of
$O(m \log_d n + n d\log_d n)$ when an array heap is used.
The value of $d$ can be selected to yield the
best tradeoff between the operations using <i>siftup</i> and
those using <i>siftdown</i>
Specifically, if $d= 2 + \lfloor m/n \rfloor$,
the running time becomes $O(m\log_d n)$.
For dense graphs, say $m\geq n^{3/2}$,
it is $O(m \log n / \log (2+m/n)) = O(m)$.
For sparse graphs, say $m = n \log n$, it is
$O(n (\log n)^2 / \log\log n)$ and for extremely sparse graphs,
say $m = 3n$, it is simply $O(n\log n)$.
Hence, at all graph densities, Prim's algorithm is very efficient,
and is optimal for dense graphs, where arguably, it is
most important to be optimally efficient.
This, combined with its simplicity of implementation makes it
an attractive choice in practice.
<p>
The graph shown below demonstrates that there are worst-case
examples for which the
performance of Prim's algorithm closely matches the analysis.
<p>
<div  style="text-align:center;">
<img width="50%" src="figs/worstCasePrim.png"><br>
</div>
<p>
If the algorithm starts at the leftmost vertex and examines
the edges incident to each vertex in decreasing order of their weight,
every <i>insert</i> and <i>changekey</i> operation
causes a vertex to move from the
bottom of the heap to the top, maximizing the number of steps
performed by <i>siftup</i>. This example can be easily extended to
arbitrary values of $n$ and by taking subgraphs, one can construct
worst-case examples at any density.
It is worth noting that the example does depend critically
on the order in which the edges are examined. For example, if the
edges incident to each vertex are examined in <i>increasing</i> order of weight,
the number of <i>siftup</i> steps drops to one per <i>changekey</i>.
If the edges are examined in random order, the expected number of
<i>siftup</i> steps per <i>changekey</i> is $O(1)$.
So in this case, the best choice of $d$
is a small constant (say 2 or 4) and the
expected running time is $O(m + n \log n)$.
<p>
If Prim's algorithm is implemented using a
<a href="../../dataStructures/heaps/heaps.html#FibHeaps">Fibonacci heap</a>,
its worst-case running time is $O(m + n \log n)$, which does provide a modest
improvement over the worst-case when array heaps are used.
In practice however, the added complexity of
Fibonacci heaps puts it at a disadvantage in most situations.
<p>
A complete <i>Javascript</i> implementation of Prim's algorithm appears below.
This version has been generalized so that
it can be applied to graphs that are not fully connected. In that case,
it produces a minimum spanning forest of the graph.
<p>
<textarea id="components" rows="15" cols="80" readonly
          style="font-size: 95%;background-color:lightCyan">
export default function mstP(g) {
    let light = new Int32Array(g.n+1).fill(-1);
    let border = new ArrayHeap(g.n, d);
    for (let s = 1; s <= g.n; s++) {
        if (light[s] >= 0) continue;
        border.insert(s, 0); light[s] = 0;
        while (!border.empty()) {
            let u = border.deletemin();
            for (let e = g.firstAt(u); e != 0; e = g.nextAt(u,e)) {
                let v = g.mate(u,e);
                if (light[v] < 0) {
                    border.insert(v, g.weight(e)); light[v] = e;
                } else if (border.contains(v) && g.weight(e) < border.key(v)) {
                    border.changekey(v, g.weight(e)); light[v] = e;
                }
            }
        }
    }
    // convert light into a list of edge numbers
    let j = 0;
    for (let i = 1; i <= g.n; i++)
        if (light[i] > 0) light[j++] = light[i];
    light.length = j;
    return light;
}
</textarea>
<p>
<p>
In the common case of a connected graph, the body of the
outer <code>for</code>-loop
is executed one time, producing a minimum spanning tree starting
from the first vertex.
For a disconnected graph, each iteration of the outer loop
computes a minimum spanning tree of a distinct connnected component.
The version shown above has been abridged for clarity.
The complete version (available on the web app) supports execution tracing
and performance statistics.
<p>
If the following script is run in the scratch pad of the web app
<pre style="padding-left:5%;">
let g = new Graph();
g.fromString("{a[b:3 d:2 e:5] b[a:3 c:7 f:4] c[b:7 d:1 f:2] " +
             "d[a:2 c:1 e:3] e[a:5 d:3 f:1] f[b:4 c:2]}");
let [,ts] = mstP(g,2,1);
log(ts);
</pre>
it produces the following output.
<p>
<textarea id="components" rows="10" cols="80" readonly
          style="font-size: 95%;background-color:lightCyan">
{
a[b:3 d:2 e:5] b[a:3 c:7 f:4] c[b:7 d:1 f:2]
d[a:2 c:1 e:3] e[a:5 d:3 f:1] f[b:4 c:2 e:1]
}

selected vertex, tree edge, heap contents
a - {d:2 b:3 e:5}
d (a,d,2) {c:1 e:3 b:3}
c (c,d,1) {f:2 e:3 b:3}
f (c,f,2) {e:1 b:3}
e (e,f,1) {b:3}
b (a,b,3) {}
</textarea>
<p>
The graph is shown at the top.
On each iteration of the inner <code>while</code> loop,
the border vertex with the lightest edge
connecting it to the partially constructed forest
is shown, along with its lightest edge, and the heap.
<p>
To compare the experimental performance to the predicted worst-case
performance, enter the following script into the web app.
<pre style="padding-left:5%;">
let n = 1024; let m = n*(n-1)/2;
let g = hardcaseP(n, m); 
log(`n=${n} density=${g.m/g.n}`);
let t0, t1, steps, stats;

t0 = Date.now(); [,,stats] = mstP(g); t1 = Date.now();
log(`prim time=${t1-t0}ms steps/m=${(stats.steps/g.m).toFixed(2)}`);

t0 = Date.now(); [,,stats] = mstP(g,2); t1 = Date.now();
log(`prim_2 time=${t1-t0}ms steps/m=${(stats.steps/g.m).toFixed(2)}`);
</pre>
When $m=n(n-1)/2$, the function <code>hardcaseP(n,m)</code> produces
a graph like the worst-case example shown earlier,
with $n$ vertices and $m$ edges.
The optional third argument to <code>mstP</code> specifies the
heap parameter $d$.
When it is omitted, $d$ is chosen to match the edge density.
Running this code on graphs of increasing size produces the output
<pre style="padding-left:5%;">
n=128 density=63.5 
prim time=2ms steps/m=5.01 
prim_2 time=3ms steps/m=8.50
 
n=256 density=127.5 
prim time=7ms steps/m=5.01 
prim_2 time=8ms steps/m=9.43
 
n=512 density=255.5 
prim time=26ms steps/m=5.00 
prim_2 time=34ms steps/m=10.39
 
n=1024 density=511.5 
prim time=110ms steps/m=5.00 
prim_2 time=151ms steps/m=11.37 
</pre>
Observe that when $d$ matches the edge density,
the number of steps per edge remains constant, even as the edge density
grows, as predicted by the worst-case analysis.
When  $d=2$, the number of steps per edge increases by about one additional
step each time $n$ is doubled.
For the largest graph shown, the number of steps per edge is more than
double that for the case when $d$ matches the edge density.
The impact on running time is somewhat smaller, as the siftup steps take less
time than the siftdown steps.
The impact of $d$ is greatest when the graphs are maximally dense.
Reducing the density for the last test case shows a much smaller effect.
<pre style="padding-left:5%;">
n=1024 density=128 
prim time=24ms steps/m=4.72 
prim_2 time=26ms steps/m=5.87
</pre>
To evaluate the program's performance on random graphs, replace the second
line of the code in the web app with
<pre style="padding-left:5%;">
let g = randomGraph(n,m); g.randomWeights(randomFraction);
</pre>
The <code>randomWeights</code> function takes a random number generator
as its first argument. In this case, the specified function
generates uniform random numbers in $[0,1]$.
Running this on graphs of increasing size produces the output
<pre style="padding-left:5%;">
n=128 density=63.5 
prim time=2ms steps/m=2.93 
prim_2 time=1ms steps/m=2.33 

n=256 density=127.5 
prim time=25ms steps/m=2.88 
prim_2 time=17ms steps/m=2.19 

n=512 density=255.5 
prim time=59ms steps/m=2.88 
prim_2 time=37ms steps/m=2.12
 
n=1024 density=511.5 
prim time=259ms steps/m=2.86 
prim_2 time=217ms steps/m=2.07 
</pre>
Notice that in this case, fixing the heap parameter at 2 produces
fewer steps per edge and faster run times. This reflects the fact
that with random edge weights, the number of <i>changekey</i> operations
is much smaller than in the worst-case, greatly reducing the
number of siftup operations. Consequently, the running time is
largely determined by the overhead of the inner loop of Prim's
algorithm, not the data structure operations.

<h2>Kruskal's Algorithm</h2>
Kruskal's algorithm [Kruskal56] is another instantiation of the greedy 
method that
builds multiple subtrees in parallel, gradually linking them together
until there is one tree for each connected component of the graph.
It processes the edges in increasing order of their weights, using
the following edge selection step.
<p style="padding-left:5%;">
If the current edge $e$ has endpoints in different trees,
add $e$ to the set of tree edges.
<p>
Note that when $e$'s endpoints are in different trees, one can define
a cut that has one of the trees by itself on one side of the cut,
with all other vertices on the other side.
<p>
The
<a href="../../dataStructures/basic/basic.html#MergeSets">merge sets</a>
data structure can be used to keep track of the
vertices in the different trees. This leads directly to the following
implementation (abridged for clarity).
<p>
<textarea id="components" rows="15" cols="80" readonly
          style="font-size: 95%;background-color:lightCyan">
export default function mstK(g) {
    // first make a sorted list of the edges in g
    let edges = [];
    for (let e = g.first(); e != 0; e = g.next(e)) edges.push(e);
    edges.sort((e1, e2) => g.weight(e1) - g.weight(e2));

    // now examine edges in order, merging separate subtrees
    let treeEdges = []; let subtrees = new MergeSets(g.n);
    for (let i = 0; i < edges.length; i++) {
        let e = edges[i];
        let u = g.left(e); let v = g.right(e);
        let cu = subtrees.find(u); let cv = subtrees.find(v);
        if (cu != cv) {
            treeEdges.push(e); subtrees.merge(cu, cv);
        }
    }
    return treeEdges;
}
</textarea>
<p>
If an efficient comparison sorting algorithm is used to sort the edges,
then Kruskal's algorithm takes $O(m \log n)$ time.
If the edges can be sorted using a linear-time algorithm such as
radix sort, this drops to $O(m \alpha(m,n))$.
The simplicity of Kruskal's algorithm makes it an attractive choice,
but its performance is critically dependent on the quality of the
function used to sort the edges.

<h2>The Cheriton-Tarjan algorithm</h2>

The Cheriton-Tarjan algorithm [CT76, Tarjan87], like Kruskal's algorithm, 
builds multiple subtrees in parallel, gradually linking them together
until there is one tree for each connected component of the graph.
Specifically, it starts with each vertex defining its own tree
and inserts the trees into a queue. It then
repeats the following step so long as it is possible to do so.
<p style="padding-left:5%;">
Remove the first tree $T_1$ from the queue. If there is an edge joining
$T_1$ to some other tree, let $T_2$ be the tree with the lightest edge
linking it to $T_1$; remove $T_2$ from the heap, combine the two trees
and add the combined tree to the end of the queue.
<p>
To make this efficient, the merge sets data structure is used to
keep track of the vertices in each tree (as in Kruskal's algorithm), and
<a href="../../dataStructures/heaps/heaps.html#LeftistHeaps">leftist heaps</a>
are used to keep track of the edges incident to each tree.
By selecting the lightest edge in the heap for $T_1$, the algorithm
can quickly identify $T_2$, then <i>merge</i> the vertex sets for the two trees
and <i>meld</i> their heaps.
The melds are done using lazy melding.
Items in the heap are considered to be deleted if
they are dummy items introduced by lazy melds, or they correspond to edges whose
endpoints now belong to the same tree. This latter condition is determined
using a function call, eliminating the need to explicitly delete items
from the heaps. 
<p>
A <i>Javascript</i> implementation of the algorithm appears below.
<p>
<p> <textarea rows="14" cols="80" readonly
          style="font-size: 95%;background-color:lightCyan">
export default function mstCT(g) {
    let trees = new MergeSets(g.n); // one subset for each mst subtree

    // initialize collection of edge endpoint heaps
    // each heap contains edge endpoints touching one mst subtree
    let epHeap = new LazyHeaps(2*(2*g.edgeRange+1), 
        ee => ee == 1 || trees.find(g.left(~~(ee/2))) ==
                         trees.find(g.right(~~(ee/2))));
    for (let e = g.first(); e != 0; e = g.next(e)) {
        epHeap.key(2*e, g.weight(e)); epHeap.key(2*e+1, g.weight(e));
    }
    epHeap.key(1, 0); // unused heap item

    let h = new Int32Array(g.n+1);  // h[u] is heap for tree whose id is u
    let q = new List(g.n);   // queue of trees
    let hlist = new List(2*epHeap.n+1);
    for (let u = 1; u <= g.n; u++) {
        hlist.clear();
        for (let e = g.firstAt(u); e != 0; e = g.nextAt(u,e))
            hlist.enq(u == g.left(e) ? 2*e : 2*e+1);
        if (!hlist.empty()) {
            h[u] = epHeap.heapify(hlist); q.enq(u);
        }
    }
    let treeEdges = [];  // vector of edges in tree
    while (q.length > 1) {
        let t = q.first(); let ee = h[t] = epHeap.findmin(h[t]);
        if (ee == 0) { q.deq(); continue; }

        let e = Math.trunc(ee/2); treeEdges.push(e);
        let u = g.left(e); let v = g.right(e);
        let tu = trees.find(u); let tv = trees.find(v);

        q.delete(tu); q.delete(tv);
        h[trees.merge(tu, tv)] = epHeap.lazyMeld(h[tu], h[tv]);
        q.enq(trees.find(u));
    }
    return treeEdges;
}
</textarea>
<p>
Note that each tree in the queue is represented by the vertex that
identifies the tree's set within the merge sets data structure.
Also, in the array of heaps, $h[u]$ is the heap of edges incident
to the tree whose identifier in the merge sets data structure is $u$.
Hence, the line
<pre>
    h[trees.merge(tu, tv)] = eph.lazyMeld(h[tu], h[tv]);
</pre>
combines the heaps associated with the trees whose identifiers
are $tu$ and $tv$ and assigns the resulting heap to the identifier of
the new set formed when the sets identified by $tu$ and $tv$ are
combined.
<p>
To see the algorithm in action, enter the following text into the 
scratch pad of the web app.
<pre>
let g = new Graph();
g.fromString('{a[b:3 d:2] b[a:3 c:7] c[b:7 d:1] d[a:2 c:1] ' +
             'e[f:1 g:3] f[e:1 g:2 h:3] g[e:3 f:2 h:1] i[j:5] j[i:5]}');
let [,ts] = mstCT(g, 1);
log(ts);
</pre>
Running this code yields
<p>
<textarea rows="14" cols="80" readonly
          style="font-size: 95%;background-color:lightCyan">
{
a[b:3 d:2] b[a:3 c:7] c[b:7 d:1] d[a:2 c:1] e[f:1 g:3]
f[e:1 g:2 h:3] g[e:3 f:2 h:1] h[f:3 g:1] i[j:5] j[i:5]
}

selected edge, queue, tree vertex sets
{a,d,2} [b c e f g h i j a] {[a d]}
{a,b,3} [c e f g h i j a] {[a b d]}
{c,d,1} [e f g h i j a] {[a b c d]}
{e,f,1} [g h i j a e] {[a b c d] [e f]}
{g,h,1} [i j a e g] {[a b c d] [e f] [g h]}
{i,j,5} [a e g i] {[a b c d] [e f] [g h] [i j]}
{f,g,2} [i e] {[a b c d] [e f g h] [i j]}
</textarea>
<p>
The trace string returned by <code>mstCT</code>
shows the graph and for each iteration of the
main loop, it shows the edge selected in that iteration,
plus the resulting queue state and the vertex sets for the trees that
are not singletons.
If the penultimate line is changed to
<pre>
    let [,ts] = mstCT(g, 2);
</pre>
it also shows the heap of edges incident to each newly formed tree.
Note that dummy and retired heap items are not shown, so some of the
new heaps are effectively empty.
<p>
<textarea rows="9" cols="80" readonly
          style="font-size: 95%;background-color:lightCyan">
selected edge, queue, tree vertex sets, heap for new tree
{a,d,2} [b c e f g h i j a] {[a d]} [ab:3 cd:1]
{a,b,3} [c e f g h i j a] {[a b d]} [cd:1 bc:7]
{c,d,1} [e f g h i j a] {[a b c d]} 
{e,f,1} [g h i j a e] {[a b c d] [e f]} [fg:2 fh:3 eg:3]
{g,h,1} [i j a e g] {[a b c d] [e f] [g h]} [eg:3 fg:2 fh:3]
{i,j,5} [a e g i] {[a b c d] [e f] [g h] [i j]} 
{f,g,2} [i e] {[a b c d] [e f g h] [i j]} 
</textarea>
<p>
The running time of the algorithm is dominated by the
calls to <i>findmin</i> at the start of each iteration.
During each execution of <i>findmin</i>, the deleted items near the top
of the heap are removed and sub-heaps with roots that are not deleted are
combined using the <i>heapify</i> operation.
This processing dominates the running time.
All other operations can be done in $O(m)$ time.
<p>
To analyze the time required for the <i>findmins</i>, the execution of the
algorithm is divided into <i>passes</i> where a pass ends when all heaps
that were on the queue at the start of the pass have been removed and
combined with some other heap.
For now, let's ignore the time consumed by operations on the merge sets
data structure.
Observe that during a pass, every tree that was on the queue at the start
of the pass is combined with at least one other tree during the pass.
Consequently, the trees on the heap during the $k$-th pass
have at least $2^k$ vertices and the number of passes is $\leq \lg n$.
Now, note that any two trees selected during the same pass are vertex-disjoint,
since they are on the heap at the same time. This implies that the total size
of the heaps that processed in a single pass is at most $2m + n-1$,
so each each edge appears in at most two heaps and the number of dummy items
introduced by lazy melds is at most $n-1$.
<p>
To complete the analysis,
let $m_i$ be the size of the heap on the $i$-th call to 
<i>findmin</i> and let $k_i$ be the number of items removed from the
heap during that execution of <i>findmin</i>.
Note that $\sum_{i=1}^{n-1} m_i \leq (2m+n-1)\lg n$ and
that the time required for the <i>heapify</i> in the $i$-th <i>findmin</i> is
$O(k_i \max(1,\log(m_i/k_i))$ based on the
<a href="../../dataStructures/heaps/heaps.html#heapify">earlier analysis
of <i>heapify</i></a>.
<p>
A <i>findmin</i> is called <i>small</i> if $k_i \leq m_i/(\lg n)^2)-1$;
otherwise, it is called <i>large</i>.

Excluding time spent on operations on the merge sets data structure,
the total time spent on the small <i>findmin</i>s is
$$
O\left( \sum_{i=1}^{n-1} \frac{m_i}{(\log n)^2} \log m_i\right)
=O\left( \sum_{i=1}^{n-1} \frac{m_i}{\log n}\right)
=O(m)
$$
The total time spent on the large <i>findmin</i>s is
$$
O\left( \sum_{i=1}^{n-1} k_i \log \frac{m_i }{ m_i/(\log n)^2}\right)
=O\left( \sum_{i=1}^{n-1} k_i \log\log n\right)
=O( m\log\log n )
$$
This implies that the number of merge sets operations is
$O(m\log\log n)$ and so the time spent on merge sets operations is
$O(\alpha(m\log\log n, n)m\log\log n) = O(m \log\log n)$.

<h2>Performance Comparison</h2>
The figure below illustrates the relative growth rates of the functions
that characterize the asymptotic running times of the various algorithms
studied in this section.
<p>
<div  style="text-align:center;">
<img width="40%" src="figs/compare.png"><br>
</div>
<p>
The value of $n$ is fixed at $10^3$, while $m$ ranges from $10^3$ to $10^6$.
The chart shows the function values divided by $m$, to emphasize the
differences among them.
The two curves that decrease with $m$ are associated with Prim's algorithm.
The $m+n\log n$ curve is for the version that uses Fibonacci heaps.
While that version appears to have an advantage at moderate densities,
experimental commparisons show that the version using array heaps
is consistently faster, due to the relative simplicity of its data
structuures.
The Cheriton-Tarjan algorithm has an edge at the lowest densities,
but the advantage over Prim's algorithm is modest.
<p>
The results below compare Prim's algorithm (with $d$ matched to edge density)
with Kruskal's algorithm and the Cheriton-Tarjan algorithm on low
density random graphs.
<p> <textarea rows="14" cols="80" readonly
          style="font-size: 95%;background-color:lightCyan">
n=4096 density=1 
prim time=11ms steps/m=17.43 
kruskal time=9ms steps/m=14.00 
cheriton-tarjan time=23ms steps/m=8.02 

n=4096 density=2 
prim time=6ms steps/m=13.50 
kruskal time=8ms steps/m=14.00 
cheriton-tarjan time=16ms steps/m=8.77 

n=4096 density=4 
prim time=6ms steps/m=9.27 
kruskal time=15ms steps/m=14.00 
cheriton-tarjan time=20ms steps/m=8.05 

n=4096 density=8 
prim time=10ms steps/m=6.81 
kruskal time=32ms steps/m=14.00 
cheriton-tarjan time=28ms steps/m=7.02 

n=4096 density=12 
prim time=14ms steps/m=5.78 
kruskal time=48ms steps/m=14.00 
cheriton-tarjan time=37ms steps/m=6.58 

n=4096 density=16 
prim time=18ms steps/m=5.41 
kruskal time=67ms steps/m=14.00 
cheriton-tarjan time=53ms steps/m=6.30 
</textarea> <p>
In all but the very lowest density, Prim's algorithm out-performs the others.

<h2>References</h2>
<dl>
<dt> [CT76]
<dd> &ldquo;Finding minimum spanning trees,&rdquo;
    by D. Cheriton and R. E. Tarjan. <i>SIAM Journal on Computing</i>, 1976.
<dt> [Johnson75]
<dd> &ldquo;Priority queues with update and finding minimum
    spanning trees,&rdquo; <i>Information Processing Letters</i>, 1975.
<dt> [Kruskal56]
<dd> &ldquo;On the shortest spanning subtree of a graph
    and the traveling salesman problem,&rdquo; by J. B. Kruskal.
    <i>Proceedings of the American Mathematical Society</i>, 1956.
<dt> [Prim57]
<dd> &ldquo;Shortest connection networks and some generalizations,&rdquo;
    by R. C. Prim. <i>Bell System Technical Journal</i>, 1957.
<dt> [Tarjan87]
<dd> <i>Network Algorithms and Data Structures</i> by Robert E. Tarjan.
     Society for Industrial and Applied Mathematics, 1987.
</dl>
<hr> <h4>&copy; Jonathan Turner - 2022</h4>
<script src="../../googleAnalytics.js"></script>
</body>
</html>
