<html>
<head>
<title>Matching</title>
<link type="text/css" rel="stylesheet" href="../../main.css">
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-41SPK9725S"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-41SPK9725S');
</script>
</head>
<body bgcolor=ffffff>
\(
\newcommand{\base}{\textit{base}}
\newcommand{\mate}{\textit{mate}}
\newcommand{\match}{\textit{match}}
\newcommand{\bridge}{\textit{bridge}}
\newcommand{\link}{\textit{link}}
\newcommand{\state}{\textit{state}}
\newcommand{\first}{\textit{first}}
\newcommand{\next}{\textit{next}}
\newcommand{\at}{\textit{at}}
\newcommand{\contains}{\textit{contains}}
\newcommand{\add}{\textit{add}}
\newcommand{\drop}{\textit{drop}}
\newcommand{\size}{\textit{size}}
\newcommand{\weight}{\textit{weight}}
\newcommand{\Blossoms}{\textit{blossoms}}
\newcommand{\Matching}{\textit{matching}}
\newcommand{\euh}{\textit{euh}}
\newcommand{\eeh}{\textit{eeh}}
\newcommand{\outer}{\textit{outer}}
\newcommand{\firstOuter}{\textit{firstOuter}}
\newcommand{\nextOuter}{\textit{nextOuter}}
\newcommand{\firstIn}{\textit{firstIn}}
\newcommand{\nextIn}{\textit{nextIn}}
\newcommand{\addBranch}{\textit{addBranch}}
\newcommand{\addBlossom}{\textit{addBlossom}}
\newcommand{\expand}{\textit{expand}}
\newcommand{\expandOdd}{\textit{expandOdd}}
\newcommand{\Forest}{\textit{forest}}
\newcommand{\ArrayHeap}{\textit{array heap}}
\newcommand{\evh}{\textit{evh}}
\newcommand{\ovh}{\textit{ovh}}
\newcommand{\findmin}{\textit{findmin}}
\newcommand{\nextedge}{\textit{nextedge}}
\newcommand{\parent}{\textit{parent}}
\newcommand{\fromString}{\textit{fromString}}
\newcommand{\bloss}{\textit{bloss}}
\newcommand{\findEdge}{\textit{findEdge}}
\newcommand{\ebh}{\textit{ebh}}
\newcommand{\obh}{\textit{obh}}
\newcommand{\exh}{\textit{exh}}
\newcommand{\divide}{\textit{divide}}
\)

<h1>Matchings in Graphs<sup>&copy;</sup></h1>

A <i>matching</i> in an undirected graph is a subset of the edges,
in which each vertex is incident to at most one edge.
The objective of the matching problem is to find a matching of
maximum size, or maximum weight, in the case of edge-weighted graphs.
<p>
<div  style="text-align:center;">
<img width="50%" src="figs/matching.png"><br>
</div>
<p>
Edge weights are assumed to be non-negative. In applications where
negative edge weights arise, a simple weight transformation can be
applied that yields the desired matching without the need for
negative edge weights. Also, while edge weights may be real valued,
numerical problems can arise from inexact representation of real-valued
weights by floating point numbers. All test cases used here use
integer-valued weights.

<h3>Augmenting Paths in Matchings</h3>
Augmenting paths were introduced in the context of the maximum flow problem.
A similar idea is also useful for solving matching problems.
Let $M$ be a matching in a graph. An <i>alternating path</i> with respect
to $M$ is a path in which every other edge is in the matching and
the remainder are not. If both endpoints of an alternating path are
unmatched, the size of the matching can be increased by flipping the
status of the edges on the path, as illustrated below.
<p>
<div  style="text-align:center;">
<img width="50%" src="figs/augmentingPath.png"><br>
</div>
<p>
This observation suggests that maximum matchings can be found by
repeatedly identifying
augmenting paths and expanding the current matching by reversing the
status of the path's edges.
<p>
To justify this approach, let $M$ be a matching and $M'$ be a matching with
exactly $k$ more edges than $M$. Let $N$ be the graph consisting of edges
that are either in $M$ or $M'$ but not both. Since the vertices in $N$
have a maximum degree of 2, $N$ consists of vertex-disjoint paths that are
alternating with respect to $M$ and possibly some cycles that are
alternating with respect to $M$. Since $M'$ has $k$ more edges than $M$,
$k$ of the alternating paths must be augmenting paths with respect to $M$
so their edges can all be flipped, yielding a matching of the same
size as $M'$. This argument establishes the following theorem.
<p>
<i>Theorem 1</i>. A matching $M$ is a maximum size matching for a graph
$G$ if and only if $G$ has no augmenting paths with respect to $M$.
<p>
For weighted graphs, the next theorem suggests a similar approach for
finding maximum weighted matchings.
<p>
<i>Theorem 2</i>. If $M$ is a maximum weight matching among all matchings of
size $k$, then the matching obtained by augmenting $M$ using
a maximum weight augmenting path is a maximum weight matching among
all matchings of size $k+1$.
<p>
The algorithms described below all use the augmenting path strategy,
but they differ in how they identify augmenting paths.

<h2>Matchings in Bipartite Graphs</h2>
In bipartite graphs, finding matchings can be done more simply than
in general graphs, so there are specialized algorithms for this case.
The maximum size matching problem can be reduced to a maximum flow
problem using the transformation sketched below.
<p>
<div  style="text-align:center;">
<img width="60%" src="figs/match2flow.png"><br>
</div>
<p>
Given a bipartite graph with edges joining vertices in complementary
subsets $X$ and $Y$, a flow graph is created with directed edges from
$X$ to $Y$ corresponding to the original undirected edges. Edges are
also included from a source vertex to each vertex in $X$ and from
each vertex in $Y$ to a sink vertex. All edges are assigned a capacity
of 1, which creates a one-to-one correspondence between matchings in
the original graph and integer flows in the flow graph.
Also, the augmenting paths in the flow graph correspond
directly to augmenting paths in the original graph.
In the weighted case, the edge weights in the flow graph are negated,
turning the maximum weight matching problem into a minimum cost flow problem.
The minimum cost augmenting paths in the flow graph correspond to maximum
weight augmenting paths in the original graph.
The algorithm terminates early if it finds an augmenting path with
non-negative cost.
<p>
The maximum size matching problem can be solved using any max flow algorithm,
but Dinic's algorithm makes a particularly good choice since the graphs
used here have a special property that speeds up the execution of
Dinic's algorithm by a substantial factor.
Recall that for general graphs, Dinic's algorithm performs up to $n$
phases, which may each take $O(mn)$ time. The graphs used here belong to
the class of <i>unit graphs</i>, meaning that all edges have capacity 1
and every vertex either has $\leq 1$ incoming edge or $\leq 1$ outgoing edge.
For graphs in this class, the number of phases
performed by Dinic's algorithm is $O(\sqrt{n})$ and the time per phase
is $O(m)$. This reduces the run time from $O(mn^2)$ to $O(m\sqrt{n})$.
<p>
To analyze the time per phase, let $N_i$ be the number of edges examined
by the $i$-th top level call to <i>findpath</i> in a phase
and note that a <i>nextedge</i> pointer is advanced for every edge
that is not returned as part of the augmenting path. If $k_i$ is the length
of the returned path, the <i>nextedge</i> pointers are advanced
$N_i-k_i$ times during the $i$-th execution of <i>findpath</i> and
all $k_i$ edges on the path are saturated. Consequently, $\sum_i k_i\leq m$
and $\sum_i(N_i-k_i)\leq 2m$.
This implies that $\sum_i N_i\leq 2m+\sum_ik_i \leq 3m$ and so the
time per phase is $O(m)$.
<p>
To bound the number of phases, suppose that $k$ phases have been completed
and let $f$ be the current flow, $f^\ast$ be a maximum flow and
let $R$ be the flow graph defined by edges with positive residual capacity.
Note that $R$ is a unit network and $f^\ast - f$ defines a flow on $R$.
The number of remaining phases is at most $|f|-|f^\ast|$, so the total
number of phases is at most $k+|f|-|f^\ast|$.
<p>
The edges with a flow of 1 in $f^\ast - f$ can be partitioned into a collection
of paths from $s$ to $t$, and possibly some cycles. Since $R$ is a unit network,
no vertex is on more than one of these paths; hence the number of augmenting
paths in $R$ is at most $(n-2)/(|f|-|f^\ast|) +1$ edges. Since $k$ phases
have been completed, the next augmenting path has at least $k+1$ edges, so
$$
k+1\leq (n-2)/(|f|-|f^\ast|) +1 \quad \textrm{or} \quad
|f|-|f^\ast|\leq (n-2)/k
$$
Hence the total number of phases is $\leq k +(n-2)/k$ no matter what value
$k$ has. Choosing $k=\left\lceil(n-2)^{1/2}\right\rceil$ gives a bound of
$2\left\lceil(n-2)^{1/2}\right\rceil$ on the number of phases.
<p>
This algorithm can be implemented without actually reducing it to a flow
problem and it was first described by Hopcroft and Karp in this form.
Even and Tarjan later noted the connection to the maximum flow problem.
The Hopcroft-Karp version is significantly faster, as it avoids the
substantial overhead associated with the use of flow graphs.
The <i>Javascript</i> implementation of the Hopcroft-Karp algorithm
uses a special <i>matching</i> data structure that
includes the following methods.
<ul>
<li> $\at(u)$ returns the matching edge incident to vertex $u$.
<li> $\contains(e)$ returns true if the matching contains edge $e$.
<li> $\add(e)$ adds edge $e$ to the matching.
<li> $\drop(e)$ removes edge $e$ from the matching.
<li> $\size()$ returns the number of edges in the matching.
<li> $\weight()$ returns the sum of the weights of the edges in the matching.
</ul>
The implementation appears below.
<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
let g;            // shared copy of graph
let match;        // Matching object
let link;         // link[u] is parent edge of u in augmenting path
let roots;        // roots contains unmatched inputs
let level;        // level[u] is distance to u from a root vertex
let nextedge;     // nextedge[u] is next edge at u to be processed
let q;            // q is List used by newPhase

export default function bimatchHK(G) {
    g = G; match = initialMatch(g);

    link = new Int32Array(g.n+1);
    level = new Int32Array(g.n+1);
    nextedge = new Int32Array(g.n+1);
    roots = new List(g.n); roots.hasReverse = true;
    q = new List(g.n);

    // add unmatched vertices from inputs to set of tree roots
    for (let u = g.firstInput(); u; u = g.nextInput(u)) {
        if (!match.at(u) && g.firstAt(u) != 0) roots.enq(u);
    }
    while (newPhase()) {
        let r = roots.first();
        while (r) {
            link[r] = 0;
            let u = findpath(r);
            if (u == 0) {
                r = roots.next(r);
            } else {
                augment(u); r = roots.delete(r);
            }
        }
    }

    return [match ..];
}

/** Prepare for new phase. 
 *  @return true if there is an augmenting path.
 */
function newPhase() {
    for (let u = 1; u <= g.n; u++) {
        level[u] = g.n; nextedge[u] = g.firstAt(u);
    }
    q.clear();
    for (let u = roots.first(); u; u = roots.next(u)) {
        level[u] = 0; q.enq(u);
    }
    let stopLevel = g.n; // used to terminate early
    // label each vertex with its distance from the nearest root
    // in matching forest
    while (!q.empty()) {
        let u = q.deq(); // u is input
        for (let e = g.firstAt(u); e; e = g.nextAt(u,e)) {
            if (e == match.at(u)) continue;
            let v = g.mate(u,e); // v is output
            if (level[v] != g.n) continue;
            // first time we've seen v
            level[v] = level[u] + 1; 
            let ee = match.at(v);
            if (ee == 0) stopLevel = level[v]; // alt-path here too
            if (stopLevel == level[v]) continue;
            let w = g.mate(v,ee);
            level[w] = level[v] + 1;
            q.enq(w);
        }
    }
    return (stopLevel != g.n);
}

/** Find an augmenting path from specified vertex.
 *  @param u is an input
 *  @return an unmatched output, or 0 if there is no
 *  admissible path to such a vertex in the current phase;
 *  on successful return, the link array defines
 *  the augmenting path from the returned vertex back to u
 */
function findpath(u) {
    for (let e = nextedge[u]; e; e = g.nextAt(u,e)) {
        let v = g.mate(u,e);
        if (level[v] != level[u] + 1) continue;
        let ee = match.at(v);
        if (ee == 0) { nextedge[u] = e; link[v] = e; return v; }
        let w = g.mate(v,ee);
        if (level[w] != level[v] + 1) continue;
        let x = findpath(w);
        if (x) {
            nextedge[u] = e; link[v] = e; link[w] = ee; return x;
        }
    }
    nextedge[u] = 0; return 0;
}

/** Flip the edges along an augmenting path
 *  @param u is an endpoint of an augmenting path; the edges in
 *  the path can be found using the link pointers
 */
function augment(u) {
    while (true) {
        let e = link[u];
        if (!e) break;
        let v = g.mate(u,e); match.add(e);
        let ee = link[v];
        if (!ee) break;
        u = g.mate(v,ee); match.drop(ee);
    }
}
</textarea> <p>
The function <i>initialMatch</i> constructs an initial matching
by simply scanning the graph looking for edges that can be included
in the matching without a path search.
This is a simple optimization that can avoid a large fraction
of the path searches that would otherwise be required.
<p>
The program assumes that the input graph has its vertices divided between
<i>inputs</i> and <i>outputs</i> and this division defines the bipartition.
When path searches are required they start from unmatched inputs.
These are maintained in a list of <i>root</i> vertices.
When a path is found, it is specified by an array of
<i>links</i> identifying the edge joining a path vertex to its predecessor.
Observe that the code effectively mimics Dinic's maxflow algorithm.
<p>
There is a subtlety associated with the $\add$ method of
the <i>matching</i> class.
If an edge is added to the matching that conflicts with some other edge
already in the matching, the $\add$ method accepts the new edge but does
not remove the conflicting edge.
It is the client code's responsibility to do this.
<p>
The unabridged version of <i>bimatchHK</i> includes two more
arguments: an optional initial matching, a trace flag.
On success, it returns a trace string and
a statistics object, in addition to the matching.
The following script can be used to demonstrate the operation of the
algorithm.
<p> <pre style="padding-left:5%">
let g = randomBigraph(5,3);
let [match,ts] = bimatchHK(g,0,1);
log(ts);
</pre> <p>
Here is some sample output.
<p> <pre style="padding-left:5%">
{
a[j k l] b[h j k] c[h j] d[g h i] e[g h i j l] f[g i]
g[d e f] h[b c d e] i[d e f] j[a b c e] k[a b] l[a e]
}
initial matching: [aj bh dg ei]

augmenting paths
[c h b k]
[f i e l]

final matching: [aj dg bk ch el fi]
</pre> <p>
In this case, the initial matching has four edges,
requiring two augmenting path searches to match the
previously unmatched vertices $c$, $k$, $f$ and $l$.
Just the vertices of the paths are shown.
<p>
The following script can be used to evaluate the performance
on random graphs.
<p> <pre style="padding-left:5%">
let g = randomBigraph(1000,1);
let t0 = Date.now();
let [match,,stats] = bimatchHK(g);
let t1 = Date.now();
log(`${g.n} ${g.m} ${JSON.stringify(stats)} ${t1-t0}ms`);
</pre> <p>
The first group of results shown below shows how the
performance changes as the average vertex degree increases
from 1 to 10. Notice that the number of augmenting paths found
is far smaller than the matching size.
This indicates that a large fraction of the matching edges are
found in the initial scan.
As the vertex degree grows, the number of path searches first
increases, as the growing connectedness of the graph leads
to an increase in the matching size. As the degree grows further,
the number of path searches drops, as the growing abundance
of edges creates more opportunities for the initial scan to
find matching edges.
Note that the time savings afforded by the initial scan is
generally not large, since the path searches it avoids
would terminate quickly. Still, it's an easy optimization 
and seems worthwhile even for a modest improvement.
<p> <pre style="padding-left:5%">
2000  1000 {"size": 541,"phases":3,"paths": 35,"steps":15921} 1ms 
2000  2000 {"size": 788,"phases":7,"paths":100,"steps":33902} 2ms 
2000  3000 {"size": 919,"phases":9,"paths":156,"steps":45820} 2ms 
2000  4000 {"size": 975,"phases":6,"paths":134,"steps":44225} 2ms 
2000  8000 {"size":1000,"phases":4,"paths": 82,"steps":59690} 2ms 
2000 10000 {"size":1000,"phases":3,"paths": 71,"steps":62450} 3ms 

 2000  3000 {"size":  921,"phases":10,"paths": 152,"steps":  61042}  3ms 
 4000  6000 {"size": 1864,"phases":11,"paths": 289,"steps": 124804}  5ms 
 8000 12000 {"size": 3718,"phases":15,"paths": 581,"steps": 296196} 12ms 
16000 24000 {"size": 7385,"phases":18,"paths":1189,"steps": 810408} 32ms 
32000 48000 {"size":14866,"phases":22,"paths":2336,"steps":1881592} 73ms 
</pre> <p>
The second group of results shows how the performance changes
as the number of vertices increases while the average vertex degree
is held at 3. While the number of phases grows more slowly than $n^{1/2}$,
in other respects the performance is consistent with the worst-case analysis.
<p>
The following script can be used to compare the Hopcroft-Karp algorithm
to the algorithm that computes the matching by explicitly reducing
it to a max flow problem.
<p> <pre style="padding-left:5%">
let g = randomBigraph(1000,3);
let t0; let t1; let stats
t0 = Date.now(); [,,stats] = bimatchHK(g); t1 = Date.now();
let s = `${g.n} ${g.m} ${stats.size} ${stats.steps} ${t1-t0}ms`
t0 = Date.now(); [,,stats] = bimatchF(g); t1 = Date.now();
log(`${s} ${stats.steps} ${t1-t0}ms`);
</pre> <p>
Some results are shown below. They show that the Hopcroft-Karp
algorithm is substantially times faster than the flow-based algorithm.
<p> <pre style="padding-left:5%">
 2000  3000   929   51106  2ms  205901   9ms 
 4000  6000  1844  139847  5ms  415775  20ms 
 8000 12000  3705  336018 13ms 1104147  45ms 
16000 24000  7411  983245 35ms 3672292 154ms 
32000 48000 14839 1457323 53ms 4461024 193ms 
</pre> <p>
There are some advantages to the flow-based algorithm. In particular,
it is easy to implement and one can conveniently combine the maximum size
and maximum weight computations in the same program,
since there is so little difference between the two.
It can also readily be extended to solve a generalization of the matching
problem called the <i>degree-constrained subgraph problem</i>
as discussed in the <a href="../vmatch/vmatch.html#dcs">next chapter</a>.
<p>
An abridged <i>Javascript</i> implementation of the flow-based
algorithm is shown below.
<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
export default function bimatchF(g ..) {
    // create flow graph, taking care to maintain edge numbers
    let fg = new Flograph(g.n+2, g.n+g.edgeRange);
    fg.source = g.n+1; fg.sink = g.n+2;
    for (let e = g.first(); e; e = g.next(e)) {
        let u = g.isInput(g.left(e)) ? g.left(e) : g.right(e);
        fg.join(u,g.mate(u,e),e); fg.cap(e,1);
        if (g.hasWeights) fg.cost(e, -g.weight(e));
    }
    for (let u = g.firstInput(); u; u = g.nextInput(u)) {
        let e = fg.join(fg.source,u); fg.cost(e, 0);
        fg.cap(e, 1);
    }
    for (let u = g.firstOutput(); u; u = g.nextOutput(u)) {
        let e = fg.join(u,fg.sink); fg.cost(e, 0);
        fg.cap(e, 1);
    }

    (g.hasWeights ? mcflowJEK(fg) : maxflowD(fg));

    // construct matching from flow
    let match = new Matching(g);
    for (let e = g.first(); e; e = g.next(e)) {
        if (fg.f(e)) match.add(e);
    }
    return [match ..];
}
</textarea> <p>
For weighted graphs, a maximum weight matching is returned.
Note that since the flow graph is acyclic, there are no unsaturated negative
cycles at the start.
Consequently, the minimum cost flow can be computed $O((m+n\log n)n)$ time,
if one uses Jewell's minimum cost flow algorithm, with the
Edmonds/Karp edge cost transform
(because the graph is acyclic, the initial edge cost
computation takes $O(m)$ time, not $O(mn)$).
<p>
The flow-based algorithm for maximum weight matchings was preceded by
one that operates directly on the original graph, finding a series
of maximum weight augmenting paths to extend the matching.
This algorithm was first described by Kuhn who called it
the Hungarian algorithm in honor of earlier work on matching
by Konig and Egervary [Kuhn55].
An abridged <i>Javascript</i> implementation
is shown below.
<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
let g;            // shared copy of graph
let weight;       // copy of edge weights
let match;        // match is a Matching object
let lab;          // lab[u] is vertex label at u
let link;         // link[u] is edge to parent of u in shortest path forest
let free;         // list containing free vertices in first subset
let border;       // heap containing border in forest
let cost;         // cost[u]=cost of shortest path to u in forest

export default function wbimatchH(G ..) {
    g = G; match = new Matching(g);

    link = new Int32Array(g.n+1);
    lab = new Float32Array(g.n+1);
    free = new List(g.n); free.hasReverse = true
    border = new ArrayHeap(g.n,4);
    cost = new Float32Array(g.n+1);

    // add unmatched vertices from first subset to free
    for (let u = g.firstInput(); u; u = g.nextInput(u)) {
        if (g.firstAt(u)) free.enq(u);
    }

    // initialize vertex labels
    initLabels();

    // augment the matching until no augmenting path remains
    let u = findpath();
    while (u) {
        augment(u); u = findpath();
    }
    return [match ..];
}


/** Compute values for labels that give non-negative transformed costs.
 *  The labels are the least cost path distances from an imaginary
 *  vertex with a length 0 edge to every input vertex in g.
 *  Edges are treated as directed from inputs to outputs.
 */
function initLabels() {
    lab.fill(0);
    for (let u = g.firstInput(); u; u = g.nextInput(u)) {
        for (let e = g.firstAt(u); e; e = g.nextAt(u,e)) {
            let v = g.mate(u,e);
            if (lab[v] > lab[u] - weight[e]) {
                lab[v] = lab[u] - weight[e];
            }
        }
    }
}

/** Find a least cost augmenting path.
 *  Unmatched edges are "directed" from io's set1 to its set2.
 *  Matched edges are "directed" from set2 to set1.
 *  The cost of a path is the weight of its matched edges minus the
 *  weight of its unmatched edges.
 *  @returns the sink vertex of the path found, or 0 if no such path
 */
function findpath() {
    link.fill(0); cost.fill(Infinity); border.clear(); border.clearStats();
    for (let u = free.first(); u; u = free.next(u)) {
        cost[u] = 0; border.insert(u,0); steps++;
    }

    let bestSink = 0; let bestPathCost = Infinity;
    let maxcost = -Infinity;
    while (!border.empty()) {
        let u = border.deletemin(); // u is in set1
        maxcost = Math.max(maxcost, cost[u]);
        for (let e = g.firstAt(u); e; e = g.nextAt(u,e)) {
            if (e == match.at(u)) continue;
            let v = g.mate(u,e);
            if (cost[v] > cost[u] + (-weight[e] + (lab[u]-lab[v]))) {
                link[v] = e;
                cost[v] = cost[u] + (-weight[e] + (lab[u]-lab[v]));
                let ee = match.at(v);
                if (ee == 0) {
                    // select best sink based on "true path cost"
                    if (cost[v] + lab[v] < bestPathCost) {
                        bestSink = v; bestPathCost = cost[v] + lab[v] ;
                    }
                    continue;
                }
                let x = g.mate(v,ee);
                link[x] = ee;
                cost[x] = cost[v] + weight[ee] + (lab[v]-lab[x]);
                if (!border.contains(x)) border.insert(x,cost[x]);
                else border.changekey(x,cost[x]);
            }
        }
    }
    // update labels for next round
    for (let u = 1; u <= g.n; u++) {
        lab[u] += (cost[u] == Infinity ? maxcost : cost[u]);
    }

    if (bestSink == 0 || bestPathCost >= 0) return 0;

    return bestSink;
}

/** Flip the edges along an augmenting path
 *  @param[in] u is an endpoint of an augmenting path; the edges in
 *  the path can be found using the link pointers
 */
function augment(u) {
    let e = link[u];
    while (e) {
        match.add(e);  u = g.mate(u,e); e = link[u];
        if (!e) break;
        match.drop(e); u = g.mate(u,e); e = link[u];
    }
    free.delete(u);
}
</textarea> <p>
This implementation mimic's Jewell's minimum cost flow algorithm, with
the edge transform of Edmonds and Karp.
However, it is simpler in one particular.
Because the flow graph is acyclic, it has no unsaturated
negative cycles at the start, so there is no need to eliminate negative cycles.
Also, because the graph is acyclic, the initial computation of the labels
can be simplified.
<p>
The following script can be used to demonstrate the Hungarian algorithm.
<p> <pre style="padding-left:5%">
let g = randomBigraph(5,3); g.randomWeights(randomInteger, 0, 9);
let [match,ts] = wbimatchH(g,0,1);
log(ts);
</pre> <p>
Sample output appears below.
<p> <pre style="padding-left:5%">
{
a[g:8 h:5 k:5 l:6] b[i:7 j:7] c[j:3 k:8 l:4] d[i:2 k:7]
e[i:3 j:2 k:9 l:7] f[g:2 h l:4] g[a:8 f:2] h[a:5 f]
i[b:7 d:2 e:3] j[b:7 c:3 e:2] k[a:5 c:8 d:7 e:9] l[a:6 c:4 e:7 f:4]
}

augmenting path, path weight
[ek] 9
[ag] 8
[bi] 7
[ck ek el] 6
[di bi bj] 2

matching: [ag:8 el:7 ck:8 bj:7 di:2] 5 32
</pre> <p>
The following script can be used to evaluate the performance of the
Hungarian algorithm and compare it to the similar algorithm
using minimum cost flows.
<p> <pre style="padding-left:5%">
let n=100; let d=3;
let g = randomBigraph(n,d); 
g.randomWeights(randomInteger, 0, 999);
let t0; let t1; let stats;
t0 = Date.now(); [,,stats] = wbimatchH(g); t1 = Date.now();
let s = `${n} ${d} H ${stats.paths} ${(stats.steps/g.m).toFixed()} ${t1-t0}ms`;
t0 = Date.now(); [,,stats] = bimatchF(g); t1 = Date.now();
log(`${s}    F ${stats.paths} ${(stats.steps/g.m).toFixed()} ${t1-t0}ms `);
</pre> <p>
Some sample results appear below.
Note that the for both algorithms, the number of path searches grows
directly with the number of vertices and the number
of steps per edge grows directly slightly faster than the number of vertices.
<p> <pre style="padding-left:5%">
100 3 H  81  293   4ms    F  81  820   9ms  
200 3 H 168  657  14ms    F 168 1758  32ms  
400 3 H 328 1387  41ms    F 328 3723 112ms  
800 3 H 655 2781 161ms    F 655 7884 463ms  
</pre>

<h2>Maximum Size Matchings in General Graphs</h2>
Finding a matching in a general graph is more complicated
than for bipartite graphs, due to the presence of odd length cycles.
Such cycles complicate the usual search procedure,
which explores the graph, while constructing a subtree
on the vertices visited so far.
When odd length cycles are present, it can be hard to determine
how the tree should be structured.
This situation is illustrated below. Imagine an augmenting path search
that starts from vertex $a$.
<p>
<div  style="text-align:center;">
<img width="24%" src="figs/oddCycleIssue.png"><br>
</div>
<p>
When the path search reaches the cycle $[u,v,x,y,z]$, there
are two options for how to proceed beyond the cycle.
If one chooses to continue towards $h$, an alternating
path back to $a$ must pass through vertices $z,y,x,v,u$.
If one chooses to continue towards $d$, an alternating
path back to $a$ must pass through $v,x,y,z,u$.
Because these two path segments traverse the cycle
edges in opposite directions, it's not immediately obvious how to
structure the search tree while allowing for both possibilities.
The solution to this puzzle is to defer the decision by collapsing
the cycle into a single <i>pseudo-vertex</i> and continuing the
search on the resulting graph. If this search leads to an augmenting
path that includes the cycle, that path can be extended to include
the appropriate subset of cycle edges, and at that point there is
no ambiguity about which way to proceed.
This idea is at the heart of Edmond's algorithm.

<h3>Edmond's Algorithm</h3>
Most algorithms for finding maximum matchings can be viewed as
refinements of Edmond's algorithm, first described in [Edmonds65].
Edmond's algorithm finds an augmenting path by constructing a
collection of trees rooted at unmatched vertices and with
leaf-to-root paths that are alternating with respect to the
current matching.
The trees can be represented by a mapping from each tree vertex $u$ to
an edge $\link(u)$ that joins $u$ to its parent in the tree.
Tree vertices are classified as <i>even</i> or <i>odd</i> depending
on the length of their path to the root in the tree and leaves
are always even, implying that for a leaf $u$, $\link(u)$ is a
matching edge. Vertices that are not part of any tree are
classified as <i>unbound</i>. An example of such a collection of trees
is illustrated below, along with some non-tree edges (dashed)
and with the even and odd vertices indicated
by plus or minus symbols, respectively.
<p>
<div  style="text-align:center;">
<img width="35%" src="figs/altPathTrees.png"><br>
</div>
<p>
Notice that edge $\{i,m\}$ joins two even vertices in different
trees and that the path from $i$ to the root of its tree can be
linked to the path from $m$ to the root of its tree through $\{i,m\}$
yielding an augmenting path from $a$ to $j$.
<p>
It's helpful to introduce the algorithm by considering the bipartite case first.
The algorithm starts by making every vertex a
tree root and placing every edge in a queue of <i>pending edges</i>.
In general, the pending queue contains edges with at least
one even endpoint.
The algorithm then repeats the
following step until no more augmenting paths can be found.
<p style="padding-left:5%">
Remove an edge $e=\{u,v\}$ with $u$ even from the pending queue and then
apply the appropriate case from the list below.
<ul style="padding-left:8%">
<li> If $v$ is odd, just ignore $e$.
<li> If $v$ is not yet in any tree, add $e$ and $v$ to $u$'s tree;
     if $f=\{v,w\}$ is the matching edge at $v$, add $f$ and $w$ to the tree.
     Add all edges incident to $w$ to the queue of pending edges.
<li> If $v$ is an even vertex, then it must be in a different tree than $u$.
     So, an augmenting
     path can be formed by linking the tree path from $u$ to the root
     of its tree to the tree path from $v$ to the root of its tree
     through $e$. Augment the matching, then discard the current set of
     trees and the contents of the pending queue. Make every unmatched
     vertex a tree root and add every edge incident to an unmatched vertex
     to the pending queue.
</ul>
Note that in the second case, $v$ must be matched since unmatched vertices
are tree roots. In the third case, there can be no
edge $e$ joining vertices in the same tree, since this would imply the
presence of an odd length cycle.
<p>
The time between successive augmentations is referred to as a <i>phase</i>
and since each successful phase adds an edge to the matching there can be
at most $n/2+1$ phases.
The number of steps within a phase that add a branch to a tree is at most $n/2$
and edges are added to the pending queue at most $m$ times per phase.
Consequently, for bipartite graphs, the algorithm finds a maximum matching
in $O(mn)$ time.
<p>
To handle general graphs, Edmond's algorithm must be generalized to deal
with odd cycles.
In particular, when odd cycles are present, the algorithm may sometimes 
have to process an edge that joins vertices in the same tree.
When this happens, the edge together with the tree path segments
joining its endpoints to their nearest common ancestor in the tree
is an odd-length cycle.
The algorithm handles the cycle by
<i>shrinking</i> it to form a new vertex called a <i>blossom</i>.
This can lead to a situation like the one shown below, in which the
odd length cycle $[u,v,w,x,y]$ has been shrunk to form the blossom $B$.
The resulting graph has two augmenting paths, $[a,c,B,d,e,f]$ and
$[a,c,B,h,g,f]$.
<p>
<div  style="text-align:center;">
<img width="40%" src="figs/blossomPath.png"><br>
</div>
<p>
In the first case, the augmenting path can be expanded
into an augmenting path in the original graph by inserting the
path segment $[u,y,x,w,v]$ in place of $B$. In the second case,
it can be expanded by inserting the segment $[u,v,w]$.
In general, the path is expanded by inserting the even-length portion
of the blossom cycle connecting the endpoints of the path edges that
are incident to the cycle.
<p>
Note that whenever a cycle is shrunken into a blossom,
$k$ of the $2k+1$ edges on the cycle are in the matching.
One vertex on the cycle has no matching edges in the cycle
and is referred to as the <i>base</i> of the blossom.
The base may have a matching edge for which the other
endpoint is external to the blossom and it is the only vertex
in the blossom for which this is true.
This means that at the time a blossom is formed, the base is even;
consequently, the blossom is also considered even.
Also notice that for an even vertex on a blossom cycle,
the even length path to the base of the blossom goes up the tree,
but for an odd vertex on a blossom cycle, the even length path must pass through
the edge that led to the formation of the blossom.
This edge is referred to as the <i>bridge</i> of the blossom.
<p>
Vertices contained in blossoms are called <i>inner</i> vertices; those that
are not in any blossom are called <i>outer</i> vertices.
Since blossoms may be contained in other blossoms, they can be similarly
classified as inner or outer.
Edges joining vertices within a common outer blossom are called inner,
while the others are called outer.
The graph consisting of outer vertices, blossoms and edges is
referred to as the outer graph.
As the algorithm proceeds, new blossoms may be formed,
leading to an ongoing evolution of the outer graph.
When an augmenting path in the outer graph is discovered,
it is expanded into an augmenting path on the original graph.
This may require expanding nested blossoms as well.
<p>
The general version of Edmond's algorithm is similar to the bipartite version.
To simplify the presentation, let $U$ denote the outer blossom
containing a vertex $u$ (if $u$ is an outer vertex, let $U=u$).
The algorithm builds alternating trees as before and an outer blossom
in a tree is considered even or odd based on the length of the tree path to
its tree root.
As before, the algorithm starts by making every vertex a tree root
and placing every edge in the pending queue.
It then repeats the following step until no more augmenting paths
can be found.
<p style="padding-left:5%">
Remove an edge $e=\{u,v\}$ from the pending queue (assume $U$ is even)
and then apply the appropriate case from the list below.
<ul style="padding-left:8%">
<li> If $U=V$ or $V$ is odd, just ignore $e$.
<li> If $V$ is not yet in any tree, add $e$ and $v$ to $U$'s tree;
     if $f=\{v,w\}$ is the matching edge incident to $v$,
     add $f$ and $w$ to the tree.
     Add all edges incident to $w$ to the queue of pending edges.
<li> If $V$ is even and in the same tree as $U$, then $e$
     together with the tree path from $U$ and $V$ to their nearest
     common ancestor in the outer graph is an odd cycle.
     Shrink the cycle to form a new blossom, while adding external edges
     incident to the formerly odd vertices on the cycle to the
     pending queue.
<li> If $V$ is even and in some other tree, then an augmenting
     path can be formed by linking the tree path from $U$ to the root
     of its tree to the tree path from $V$ to the root of its tree
     through $e$. Augment the matching, then discard the current set of
     trees, the contents of the pending queue and the blossoms.
     Now, make every unmatched vertex a tree root and add every edge
     incident to an unmatched vertex to the pending queue.
</ul>
The diagram below illustrates several steps in an execution
of Edmond's algorithm, starting from a partial matching that
leaves just two vertices unmatched.
<p>
<div  style="text-align:center;">
<img width="70%" src="figs/edmonds1.png"><br>
</div>
<p>
At the start of the augmenting path search, only vertices
$a$ and $k$ are unmatched, so they form tree roots.
The first three steps expand these trees, while the third one
forms a new blossom containing vertices $i$, $k$ and $l$.
The diagram uses plus symbols to identify even vertices, odd symbols
to identify odd vertices and arrows to identify tree edges.
This information is preserved for internal vertices and edges,
and odd vertices within blossoms are also labeled with the edge that
led to the discovery of the blossom.
The next diagram shows the result of three more steps, leading to
the formation of two new blossoms.
<p>
<div  style="text-align:center;">
<img width="35%" src="figs/edmonds2.png"><br>
</div>
<p>
Two more steps lead to the formation of a fourth blossom,
and one more leads to the discovery of an augmenting path.
<p>
<div  style="text-align:center;">
<img width="35%" src="figs/edmonds3.png"><br>
</div>
<p>
The augmenting path is shown with parentheses surrounding
the vertices in different blossoms and sub-blossoms.
<p>
Observe that the algorithm maintains the following two properties
<ol>
<li> If a vertex $v$ is in a tree and matched, $\mate(v)$ is in the same
    tree.
<li> If $v$ is an unmatched vertex contained in an outer blossom $B$,
    then $B$ is unmatched (in the outer graph).
</ol>
Also, if the algorithm fails to discover an augmenting path,
then any two adjacent vertices that are both even (or inner) are contained in
the same outer blossom when the algorithm halts.
If this were not true, either that edge could have been used to form a new
blossom or an alternating path.
<p>
To establish the correctness of the algorithm, suppose that it fails to
find an augmenting path, when there is one.
Let $p=[x_0,\ldots,x_{2r+1}]$ be such a path and consider the situation just
after the algorithm halts.
Note that because $x_0$ and $x_{2r+1}$ are endpoints of the augmenting path,
they are roots of different trees; let $k$ be any integer
such that $x_k$ is not in the same tree as $x_{k+1}$, and at most one
of the pair is unbound.
Because they are not in the same tree $x_k$ and $x_{k+1}$ cannot both be
even.
Also, if one is even, the other must be odd.
Consequently, at least one of them must be odd.
<p>
Assume that $x_k$ is odd.
Since the edge $\{x_{k-1},x_k\}$ is in the matching, $k$ must be even.
Let $j$ be the smallest integer such that $j$ is even, while $x_j$ is odd.
If all vertices $\{x_0,\ldots,x_{j-1}\}$ are even, they must
be contained in a common blossom and since $x_0$ is unmatched, this
blossom must be unmatched.
However, since $j$ is even, the edge $\{x_{j-1},x_j\}$ is a matching
edge, with one end in the blossom. Consequently, some vertex in
$\{x_0,\ldots,x_{j-1}\}$ must be odd.
Let $i\lt j$ be the largest integer for which $x_i$ is odd
and note the $i$ must be odd, implying that $\{x_i,x_{i+1}\}$ is a
matching edge.
This implies that $\{x_{i+1},\ldots,x_{j-1}\}$ are contained in a blossom
with two incident matching edges.
This contradicts the assumption that the algorithm fails to find an augmenting
path.
The diagram below summarizes this argument.
<p>
<div  style="text-align:center;">
<img width="85%" src="figs/edmonds4.png"><br>
</div>
<h3>Gabow's Implementation of Edmond's Algorithm</h3>
To implement Edmond's algorithm, one requires some representation of the
outer graph.
Gabow [Gabow76] devised an implementation that maintains just enough
information about the structure of the outer
graph to allow it carry out the augmenting path search.
This includes a mapping from each tree vertex or blossom to
the edge linking to its parent in the tree,
a mapping $\state$ from each vertex or blossom to one of
<i>even</i>, <i>odd</i> or <i>unbound</i> and a <i>merge sets</i>
data structure that partitions the vertices according to which
outer blossom they are in.
So in the example below (reproduced from the previous section),
the subsets would be $\{a,b,c,d,e,f,g\}$, $\{i,j,k,l,m\}$, $\{h\}$ and $\{n\}$.
<p>
<div  style="text-align:center;">
<img width="35%" src="figs/edmonds3.png"><br>
</div>
<p>
Using this data structure, one can determine if two vertices are in the same
blossom by determining if they are in the same subset. This is useful for
distinguishing between inner and outer edges.
<p>
The algorithm also maintains a mapping from each subset
to the base of the blossom it represents.
The base is used as the identifier of the blossom and
the vertices contained within it.
In the example above, the first subset is mapped to $a$ and
the second to $k$.
One can use this information to traverse a tree path in the current outer
graph efficiently. Specifically, if $u$ is a vertex in a blossom $U$
on a tree path, the mapping can be used to obtain the base of $U$,
and the base's link leads to the next vertex or blossom up the tree.
<p>
Finally, the algorithm maintains a mapping from odd vertices on a
blossom cycle to the blossom's bridge.
Specifically, if $e=\{u,v\}$ is the bridge of a blossom and
$x$ is an odd vertex that is an ancestor
of $u$, then $\bridge(x)$ is assigned the value $[e,u]$.
Similarly if $y$ is an odd vertex that is an ancestor of $v$,
$\bridge(y)=[e,v]$.
For an even vertex on a blossom cycle, the even length path to the base
of the blossom goes up the tree, while for an odd vertex,
the even length path passes through the bridge.
So one can extend an augmenting path that enters a blossom at an odd vertex
$x$ by combining the bridge with the tree path segments connecting it
to $x$ and the base of the blossom.
So in the example above, when edge $\{e,j\}$ is processed the
path segment $[c,d,f,g,e]$ is constructed with the assistance
of $\bridge(e)=[\{f,g\},g]$. (Note, the diagram shows the bridge in
an abbreviated form.)
<p>
To analyze the performance, first note that edges must be added to the queue
of pending edges not only when a new branch is added,
but also when a blossom is formed. Specifically, the edges incident to
odd vertices on the blossom cycle must be added, since after the blossom
is shrunk, these vertices are contained in a blossom and all blossoms are even.
This implies that edges may be added to the queue more than once per phase
(but not more than twice).
So, the number of steps per phase is now bounded by $2m$.
Also note that when an edge is removed from the queue, one must verify that
the edge is outer by comparing its endpoints to determine if
they are in same subset.
This requires a pair of $\find$ operations on the merge sets data structure.
<p>
The time spent adding branches to trees is $O(n)$ per phase,
excluding the time for adding edges to the queue.
To determine if the endpoints of an edge are in the same tree,
the algorithm computes their nearest common ancestor in the current outer
graph. This can be done by proceeding up the tree from both
endpoints in parallel, using mark bits to detect when the two
paths converge. This makes the number of steps proportional to the number of
external edges on the blossom cycle. 
The sum of the lengths of all the blossom cycles formed during an augmenting
path search is $\leq 3n/2$, so the total number of steps involved in 
finding nearest common ancestors is $O(n)$ per phase.
Note that each step involves a $\find$ operation on the merge sets data structure,
so the time required is bounded by the time required for the $\find s$.
Similarly, the number of steps required to shrink all the blossoms is $O(n)$
per phase and since each step requires a $\find$ operation,
the time is bounded by the $\find s$.
<p>
The total number of $\find$ operations performed during a phase
is $O(m+n)$, so the time contributed by these operations is
$O(m \alpha(m,n))$. This also bounds the time for the complete phase,
yielding a bound of $O(mn\alpha(m,n))$ on the time to compute the matching.
<p>
An abridged <i>Javascript</i> implementation of Gabow's version of Edmond's algorithm
appears below.
<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
let g;            // shared copy of graph
let match;        // match is a Matching object
let link;         // link[u] is parent edge of u in matching forest
let q;            // q is list of pending edges
let outer;        // MergeSets object partitioning graph into blossoms
let apath;        // ReverseLists object used to build augmenting paths
let base;         // base[b] is the base of an outermost blossom b
let bridge;       // bridge[x] is pair [e,u] where e is bridge in x's blossom
                  // and u is the endpoint of e that is a descendant of x
let state;        // state[u] is 0 if u is unbound, +1 if even, -1 if odd
let mark;         // mark[u] is a flag used when computing nca

export default function matchEG(G) {
    g = G;
    match = new Matching(g);
    link = new Int32Array(g.n+1);
    q = new List(g.edgeRange);
    outer = new MergeSets(g.n);
    apath = new ReverseLists(g.edgeRange);
    base = new Int32Array(g.n+1);
    bridge = new Array(g.n);
    state = new Int8Array(g.n+1);
    mark = new Int8Array(g.n+1);

    // add edges to match, yielding maximal (not maximum) matching
    for (let u = 1; u <= g.n; u++) {
        if (match.at(u)) continue;
        for (let e = g.firstAt(u); e != 0; e = g.nextAt(u,e)) {
            if (!match.at(g.mate(u,e))) { match.add(e); break; }
        }
    }

    newPhase();
    while (!q.empty()) {
        let e = q.deq(); let u = g.left(e); let U = bid(u);
        if (state[U] != +1) { u = g.right(e); U = bid(u); }
        let v = g.mate(u,e); let V = bid(v);
        if (U == V || state[V] < 0) continue;
            // skip edges internal to a blossom and edges to odd vertices

        if (state[V] == 0) {
            addBranch(u,e);
        } else {
            // U and V are both even
            let A = nca(U,V);
            if (A != 0) {
                addBlossom(e, A);
            } else {
                // U, V are in different trees - augment and start new phase
                let r1 = root(U); let r2 = root(V);
                let ee = apath.join(apath.reverse(path(u, r1)), e);
                augment(apath.join(ee, path(v, r2)));
                newPhase();
            }
        }
    }
}

/** Prepare for a new phase */
function newPhase() {
    outer.clear(); q.clear(); link.fill(0); state.fill(0);
    for (let u = 1; u <= g.n; u++) {
        base[u] = u;
        if (!match.at(u)) {
            state[u] = 1; add2q(u);
        }
    }
}

/** Extend tree at an even vertex.
 *  @param u is an even matched vertex that is not in a blossom.
 *  @param e is an edge connecting u to an unbound vertex v
 */
function addBranch(u, e) {
    let v = g.mate(u,e);  state[v] = -1; link[v] = e;
    let ee = match.at(v);
    let w = g.mate(v,ee); state[w] = +1; link[w] = ee;
    add2q(w);
    return;
}

/** Add edges incident to a new even vertex to q.
 *  @param u is a vertex that just became even
 */
function add2q(u) {
    for (let e = g.firstAt(u); e; e = g.nextAt(u,e)) {
        if (!match.contains(e) && !q.contains(e)) q.enq(e);
    }
}

/** Add new blossom defined by edge.
 *  @param e is an edge joining two even vertices in same tree
 *  @param A is the nearest common ancestor of e's endpoints
 */
function addBlossom(e, A) {
    let u = g.left(e);  let U = bid(u);
    let v = g.right(e); let V = bid(v);
    let x = U; let s = '';
    while (x != A) {
        base[outer.merge(outer.find(x), outer.find(A))] = A;
        x = g.mate(x,link[x]); // x now odd
        base[outer.merge(x, outer.find(A))] = A;
        bridge[x] = [e,u];
        add2q(x);
        x = bid(g.mate(x,link[x]));
    }
    x = V;
    while (x != A) {
        base[outer.merge(outer.find(x), outer.find(A))] = A;
        x = g.mate(x,link[x]); // x now odd
        base[outer.merge(x,outer.find(A))] = A;
        bridge[x] = [e,v];
        add2q(x);
        x = bid(g.mate(x,link[x]));
    }
}

/** Augment the matching.
 *  @param e is the first edge in the path.
 */
function augment(e) {
    while (true) {
        match.add(e);
        if (apath.isLast(e)) break;
        e = apath.pop(e); match.drop(e);
        e = apath.pop(e);
    }
}

/** Get identifier of outer blossom of a vertex.
 *  @param u is some vertex
 *  @return u's the identifier of the outer blossom containing u;
 *  specifically, the base of the outer blossom u (or u, if u is outer).
 */
function bid(u) {
    return base[outer.find(u)];
}

/** Find the root of a tree.
 *  @param rv is the id for a vertex in current graph
 *  @return the root of the tree containing rv
 */
function root(rv) {
    while (link[rv] != 0) {
        rv = bid(g.mate(rv,link[rv]));
    }
    return rv;
}

/** Find the nearest common ancestor of two vertices in
 *  the current outer graph.
 *  To avoid excessive search time, search upwards from both vertices in
 *  parallel, using mark bits to identify the nca. Before returning,
 *  clear the mark bits by traversing the paths a second time.
 *  @param u is an external vertex or the base of a blossom
 *  @param v is another external vertex or the base of a blossom
 *  @returns the nearest common ancestor of u and v or 0 if none
 */
function nca(u, v) {
    let result;

    // first pass to find the nca
    let x = u; let y = v;
    while (true) {
        if (x == y) { result = x; break; }
        if (mark[x]) { result = x; break; }
        if (mark[y]) { result = y; break; }
        if (link[x] == 0 && link[y] == 0) { result = 0; break; }
        if (link[x] != 0) {
            mark[x] = true;
            x = g.mate(x,link[x]);
            x = bid(g.mate(x,link[x]));
        }
        if (link[y] != 0) {
            mark[y] = true;
            y = g.mate(y,link[y]);
            y = bid(g.mate(y,link[y]));
        }
    }
    // second pass to clear mark bits
    x = u;
    while (mark[x]) {
        mark[x] = false; x = g.mate(x,link[x]); x = bid(g.mate(x,link[x]));
    }
    y = v;
    while (mark[y]) {
        mark[y] = false; y = g.mate(y,link[y]); y = bid(g.mate(y,link[y]));
    }
    return result;
}

/** Find path joining two vertices in the same tree.
 *  @param a is a matched vertex in some tree defined by parent
 *  pointers
 *  @param b is an ancestor of a
 *  @return the ab-path that starts with the matching edge incident to a;
 *  specifically, return the index of the id of the list of vertices in
 *  the path using the apath object
 */
function path(a, b) {
    if (a == b) return 0;
    if (state[a] > 0) { // a is even
        let e1 = link[a];  let pa = g.mate(a,e1);
        if (pa == b) return e1;
        let e2 = link[pa]; let p2a = g.mate(pa,e2);
        let e = apath.join(e1,e2);
        if (p2a == b) return e;
        return apath.join(e, path(p2a,b));
    } else {
        let [e,v] = bridge[a]; let w = g.mate(v,e);
        e = apath.join(apath.reverse(path(v,a)), e);
        e = apath.join(e, path(w, b));
        return e;
    }
}
</textarea> <p>
Note that the program first computes an initial matching by sequentially
adding non-conflicting edges. This eliminates many augmenting path searches,
and often has a large impact on the running time, since the initialization
overhead for each path search is significant.
Also note that the program uses a <i>ReverseLists</i> object
and a recursive path construction method to
construct the list of edges that form each augmenting path.
<p>
The following script can be used to demonstrate the program.
<p> <pre style="padding-left:5%">
let g = randomGraph(20,3);
let [,ts] = matchEG(g,0,1);
log(ts);
</pre> <p>
A sample of the trace output appears below.
<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
matchEG(small random (20,3))
{
a[b l] b[a k] c[e k p t] d[f g j] e[c i m]
f[d h n] g[d q] h[f j o] i[e j m p q] j[d h i s]
k[b c] l[a r] m[e i o t] n[f r] o[h m r t]
p[c i] q[g i] r[l n o s t] s[j r] t[c m o r]
}
initial matching: [ab ce df gq hj im lr ot]

branch: k b a
branch: k c e
branch: n f d
branch: n r l
branch: p i m
branch: s j h
augment: [k b a--l r n]
    [ce df gq hj im ot bk al nr]

branch: p c e
branch: p i m
branch: s j h
branch: s r n
blossom: {e,m} p [p c e--m i]
branch: Pm o t
blossom: {m,t} p [mPm--t o]
branch: h f d
augment: [s j h--o t m i p]
    [ce df gq bk al nr js ho mt ip]

final matching: [ce df gq bk al nr js ho mt ip]
</textarea> <p>
For each new blossom, the trace shows the edge that triggered the formation
of the blossom, the base of the blossom, and a list of the outer vertices
or blossoms that were combined to form the new blossom, with the location
of the bridge highlighted. When a blossom appears on a new blossom cycle,
it is identified by a capital letter, corresponding to the blossom's base
and is decorated with the endpoints of its edges on the blossom cycle.
(So, in the second blossom above, both of the edges on the cycle that
are incident to blossom $P$ are incident to vertex $m$.)
Augmenting paths are shown as a list of the vertices on the path,
with the location of the edge joining the two trees highlighted by
a double-dash.
<p>
The following script can be used to observe the performance
on random graphs.
<p> <pre style="padding-left:5%">
let n = 2000; let d = 5;
let g = randomGraph(n,d);
let t = Date.now(); let [match,,stats] = matchEG(g); t = Date.now() - t;
log(`n=${n} d=${d} size=${match.size()} paths=${stats.paths} ` +
    `blossoms=${stats.blossoms} steps=${stats.steps} time=${t}ms`);
</pre> <p>
Some sample output is shown below, with repeated runs used to give a
sense of the wide variability in the number of blossoms.
<p> <pre style="padding-left:5%">
n=2000 d=5 size=973 paths=60 blossoms=  7 steps=207610 time=10ms 
n=2000 d=5 size=976 paths=60 blossoms= 21 steps=207969 time=10ms 
n=2000 d=5 size=973 paths=60 blossoms=582 steps=237925 time=11ms 
n=2000 d=5 size=972 paths=64 blossoms=614 steps=257419 time=13ms 

n=4000 d=5 size=1950 paths=124 blossoms=   3 steps=786165 time=29ms 
n=4000 d=5 size=1961 paths=138 blossoms=  21 steps=906813 time=35ms 
n=4000 d=5 size=1959 paths=135 blossoms=1141 steps=950889 time=36ms 
n=4000 d=5 size=1939 paths=127 blossoms=  33 steps=854823 time=34ms 

n=8000 d=5 size=3887 paths=260 blossoms=2285 steps=3466012 time=133ms 
n=8000 d=5 size=3893 paths=255 blossoms=2367 steps=3377792 time=130ms 
n=8000 d=5 size=3884 paths=242 blossoms= 111 steps=3097994 time=117ms 
n=8000 d=5 size=3896 paths=244 blossoms=2297 steps=3195106 time=121ms 
</pre> <p>
Notice that augmenting path searches are required for only a small fraction
of the matching edges. As the number of vertices is doubled, so is the
number of edges and the number of path searches. The number of steps and
time required grows by roughly a factor of four when the number of vertices
doubles, which is consistent with the worst-case analysis. It's
interesting to note that even when the number of blossoms is relatively large,
there is little discernible impact on the computation time.
<p>
Varying the average vertex degree, while holding the number of vertices
constant affects the performance in a more subtle way than the
worst-case analysis suggests.
In particular, when the degree gets larger, the number of augmenting
path searches drops off quickly, as it becomes easy to find
non-conflicting matching edges.
When the degree gets smaller, the matching size drops and degree one edges
become common, both of which reduce the number of path searches.

<h2>Maximum Weight Matchings in General Graphs</h2>
The maximum weight matching problem on general graphs is considerably
more complicated than the maximum size problem. However, it can be
solved by finding a sequence of <i>maximum weight
augmenting paths</i>, where the weight of an augmenting path
with respect to a matching $M$, is defined as the total weight of
the unmatched edges minus the weight of the matched edges.
This ensures that each intermediate matching has maximum weight among
all matchings of the same size, and that when there is no augmenting
path of positive weight, the current matching has maximum weight
among all matchings.
This is the approach used by Edmond's weighted matching algorithm.

<h3>Edmond's Algorithm</h3>
The maximum weight matching problem can be formulated as an
integer linear program by defining a vector $X=[x_e]$ of 0-1
<i>edge selection variables</i> and an objective function $\sum_e x_e w_e$,
where $w_e$ denotes the weight of edge $e$.
To ensure that the selection variables define a matching, the sum of
the selection variables for edges incident to a common vertex is constrained
to be no more than 1;
that is, $\sum_{e=\{u,v\}} x_e \leq 1$ for all vertices $u$.
Consider the example graph below.
<p>
<div  style="text-align:center;">
<img width="23%" src="figs/wmatch1.png">
</div>
<p>
In this case the ILP is
\begin{eqnarray*}
\textrm{maximize}\;\; 
2x_{ab} + x_{ac}+ 3x_{bc}+ 4x_{bd}+ 5x_{cd} + 3x_{ce}&+& 2x_{de} \\\\
x_{ab} + x_{ac} \leq 1&& \\
x_{ab} + x_{bc} + x_{bd} \leq 1&& \\
\textrm{subject to} \qquad
x_{ac} + x_{bc} + x_{cd} + x_{ce} \leq 1&& \\
x_{bd} + x_{cd} + x_{de} \leq 1&& \\
x_{ce} + x_{de} \leq 1&& \\
\end{eqnarray*}
and one optimal assignment sets $x_{ab}=x_{cd}=1$ and the remaining variables
to zero, giving a maximum objective function value of 7.
Notice that if the $x_e$ are allowed to be continuous variables,
there are assignments for which the objective function exceeds 7.
So for this problem, an optimal solution of the ILP is not an
optimal solution of its LP relaxation.
However, for bipartite graphs, the coefficient matrix of the ILP
is totally unimodular, so in that case the optimal ILP solutions
do match the LP relaxation.

<h4>Bipartite Case</h4>
Before proceeding to the general case, let's see how linear
programming can be used to derive an algorithm for the bipartite case.
In general, the primal LP takes the form
$$
\textrm{maximize}\;W\!\cdot\! X \;\textrm{subject to}\;
AX \leq [1]
$$
where $W$ is a vector of edge weights and $A[u,e]=1$ whenever
vertex $u$ is an endpoint of edge $e$ and zero otherwise.
Note that for bipartite graphs, the rows of $A$ can be divided into
two subsets using the bipartition, so that no column contains two
ones in the same subset. Hence $A$ is totally unimodular.
The dual can be written
$$
\textrm{minimize}\;[1]\!\cdot\! Z \;\textrm{subject to}\;
A^T Z \geq W
$$
where $Z$ is a vector of <i>dual variables</i> $z_u$,
which for brevity are also referred to as <i>labels</i>.
The dual constraints can be written $z_u + z_v \geq w_e$ for all edges $e$.
If the slack of the dual constraint for $e$ is zero, then $e$ is referred
to as a tight edge. Augmenting paths consisting of tight edges have
a useful property. In particular, the weight of such an augmenting path
$[u_0,\ldots,u_{2k+1}]$ is
$$
\sum_{0\leq i \leq k} w_{\{u_{2i},u_{2i+1}\}} -
\sum_{0< i \leq k} w_{\{u_{2i-1},u_{2i}\}}
=
\sum_{0\leq i \leq k} z_{u_{2i}} + z_{u_{2i+1}} -
\sum_{0< i \leq k} z_{u_{2i-1}} + z_{u_{2i}}
= z_{u_0} + z_{u_{2k+1}}
$$
That is, the weight of a tight augmenting path is equal to the
sum of its endpoint's labels.
If all the unmatched vertices have equal labels, then
all tight augmenting paths have the same weight.
If all matching edges are tight, then the tight augmenting paths
have larger weight than any path that is not tight.
<p>
This property of augmenting paths enables Edmond's algorithm
to find max weight paths by searching for augmenting paths using
only tight edges.
Whenever one is found, it augments the matching in the usual way and
since all edges on the path were tight, the new matching edges are
guaranteed to be tight.
From time-to-time the algorithm adusts the labels in order to increase
the number of tight edges. It does this in a way that ensures all
unmatched vertices have equal labels and so that all tight edges
remain tight.
<p>
From the perspective of linear programming, Edmond's algorithm is
a primal/dual algorithm in which solutions to the primal and dual
problems are constructed simultaneously.
Note that for optimal assignments to $X$ and $Z$,
the complementary slackness conditions (or CS conditions for short) require that
for every $x_e>0$ the dual constraint for $e$ must be tight
and for every loose primal constraint at a vertex $u$, $z_u$ must be zero.
If the $x_e$ are all 0 or 1 (that is, they define a matching),
this can be restated as: every matching edge is tight and
and every unmatched vertex has a zero label.
<p>
The first step in the algorithm is to construct feasible solutions to the
primal and dual problems by starting with an empty matching
($x_e=0$ for all $e$) and setting $z_u$ to half of the largest edge weight,
for all $u$.
This assignment trivially satisfies the first CS condition for all edges.
The iitialization also places all tight edges in a queue of
&ldquo;pending edges&rdquo;.
It then constructs a maximum weight matching by repeating the following step
until all unmatched vertices have zero labels (or equivalently,
there is no augmenting path with positive weight).
<p style="padding-left:5%">
Remove a tight edge $e=\{u,v\}$ from the pending queue with $u$ even.
If $v$ is odd, discard $e$ and proceed to the next pending edge.
Process $e$ using the appropriate case below:
<ul style="padding-left:8%">
<li> If $v$ is an even vertex, then a tight augmenting
     path can be formed by linking the tree path from $u$ to the root
     of its tree to the tree path from $v$ to the root of its tree
     through $e$. Augment the matching, then discard pending edges and
     the current set of trees. Then, make all the unmatched vertices
     tree roots and add all tight edges incident to these vertices to the
     pending queue.
<li> If $v$ is not yet in any tree, add $e$ and $v$ to $u$'s tree.
     If $f=\{v,w\}$ is the matching edge at $v$, add $f$ and $w$ to the tree
     and add all tight edges incident to $w$ to the pending queue.
</ul>
<p style="padding-left:5%">
If there is no edge remaining in the pending queue,
modify the vertex labels by subtracting $\delta$ from the labels of
all even vertices and adding $\delta$ to the labels of all odd vertices.
If the value of $\delta$ is chosen as described below,
this either makes $z_u=0$ for all unmatched vertices
(leading the algorithm to terminate) or makes one or more
new edges tight.
<p>
Notice that while trees are discarded after an augmenting path
is found, the values of the labels are retained.
The value of $\delta$ is chosen so that the labels remain
non-negative and no dual constraints are violated.
This can be accomplished by making
$\delta=\min\{\delta_1,\delta_2, \delta_3\}$, where
$\delta_1$ is the smallest $z_u$ value for an even vertex $u$,
$\delta_2$ is the smallest slack in a dual constraint involving an edge with
one even endpoint and one unbound endpoint and
$\delta_3$ is one half the smallest slack in a dual constraint involving
an edge with two even endpoints.
Note that if $\delta=\delta_1$ then after the relabeling,
<i>all</i> unmatched vertices have $z_u=0$,
since every relabeling step reduces the label for every unmatched vertex
and no vertex becomes unmatched, once it is matched.
So in this case, the solution satisfies all the CS conditions
meaning that the current matching is optimal.
If $\delta>\delta_1$ then at least one new edge
with an even endpoint becomes tight, allowing the
search for augmenting paths to resume.
<p>
This algorithm can be implemented to run in $O(mn)$ time if one excludes
the relabeling steps. Let the period between two augmentations of the
matching be called a <i>phase</i>.
The number of relabeling steps that can occur within a phase
is $O(n)$, since each relabeling step that makes an edge tight
ensures that a new branch will be created
before the next relabeling operation in the phase and
the number of branches created per phase is at most $n/2$.
Hence, the total number of relabeling steps is $O(n^2)$.
The relabeling steps can be done in $O(m)$ time,
leading to an overall running time of $O(mn^2)$.

<h4>General Case</h4>
The essential idea remains the same in the general case.
That is, the algorithm finds a series of maximum weight augmenting paths
by restricting the path search to tight edges.
The algorithm is complicated by the need to handle blossoms.
The key to making it work is to ensure that
a max weight augmenting path in the outer graph can be extended
to a max weight augmenting path passing through all the blossoms on the path.
Since whenever a blossom is formed, the edges on the blossom cycle are tight,
it suffices to ensure that they remain tight.
<p>
As previously noted, the optimal solutions for the matching ILP do not
match its LP-relaxation for general graphs.
However, Edmonds showed that if suitable constraints are added to the ILP,
the optimal solutions for this new ILP do match its LP-relaxation.
This enables a similar primal-dual algorithm.
For each odd vertex subset $S$ with three or more vertices,
there is a new constraint
$$\sum_{e\subset S} x_e \leq (|S|-1)/2$$
Note that if the $x_e$ values are 0-1 and define a matching,
these constraints are automatically satisfied.
So if the algorithm limits itself to primal solutions that define
matchings it will automatically satisfy all the new primal constraints.
For each odd subset $S$, there is also a new dual variable $z_S$
and the dual objective function becomes
$$\sum_u z_u + \sum_S z_S (|S|-1)/2$$
where the first sum is over the vertices and the second is over
the odd subsets.
For each edge $e=\{u,v\}$, the dual constraint becomes
$$ z_u + z_v + \sum_{S\supset e} z_S \geq w_e$$
where the sum is over all odd subsets that include both
endpoints of $e$. Now one might reasonably object to the
introduction of an exponential number of constraints and dual variables,
but the algorithm addresses this issue by limiting the
primal and dual solutions that it considers.
Specifically, it limits itself to primal solutions corresponding to matchings,
and to dual solutions in which $z_S$ is non-zero
only if $S$ corresponds to a blossom.
Consequently, the primal constraints are satisfied automatically and
dual variables need only be maintained for blossoms.
It also means that the complementary slackness
condition for the new primal constraints are automatically satisfied,
since every vertex subset $S$ with a non-zero dual variable
contains exactly $(|S|-1)/2$ matching edges.
The complementary slackness conditions for the original primal constraints
are not automatically satisfied, so the algorithm must still terminate
with $z_u=0$ for each unmatched vertex $u$
(equivalently, when there remains no augmenting path of positive weight).
<p>
Complementary slackness also requires that for every edge $e$,
either $e$ is unmatched or it is tight.
The algorithm ensures this by adding only tight edges to its collection of
trees and maintaining the tightness of all matching edges
as the algorithm proceeds.
<p>
The handling of blossoms is more complicated in this case
than for unweighted matchings, because blossoms may have to be expanded
during a phase in order to make more edges tight.
Also, blossoms with non-zero dual variables have to be
retained from one phase to the next, which creates the
possibility of odd blossoms in subsequent phases.
This means that the algorithm must keep track of all the blossoms
and their sub-blossoms across multiple phases.
<p>
A vertex $u$ within a blossom is considered even (odd) if the
outer blossom containing $u$ is even (odd).
Individual vertices are considered
to be (trivial) blossoms and for a vertex $u$, its outermost blossom
is denoted by $U$ (for outer vertices, $U=u$).
A blossom that is not part of any tree is considered <i>unbound</i>.
<p>
As in the bipartite case, the algorithm starts with an empty matching
and makes every vertex a tree root. The vertex labels $z_u$, are all set
to one-half the maximum edge weight and the labels for the odd subsets
are all implicitly zero.
It also places every maximum weight edge in a pending queue.
In general, outer edges are added to the queue when they are tight
and have at least one even endpoint.
The algorithm then constructs a matching by repeating the following step.
<p style="padding-left:5%">
Remove the next tight edge $e=\{u,v\}$ with $U$ even from the pending queue.
If $U=V$ or $V$ is odd, discard $e$ and proceed to the next edge.
Process $e$ using the appropriate case below:
<ul style="padding-left:8%">
<li> If $V$ is an even vertex and $U$ and $V$ are in different trees,
     then an augmenting path can be formed by linking the tree path
     in the external graph from $U$ to the root of its tree and the
     path from $V$ to the root of its tree through $e$.
     Extend this augmenting path through the blossoms along the path
     but do not expand them.
     Augment the matching, then discard the current set of
     trees, leaving just the unmatched vertices as tree roots.
     Expand all blossoms $B$ with $z_B=0$ and make all unmatched blossoms
     tree roots. Add all edges incident to unmatched blossoms to the queue.
<li> If $V$ is even and $U$ and $V$ are in the same tree, then the edge
     $e$ together with the paths in the outer graph from $U$ and $V$ to their
     nearest common ancestor forms an odd cycle.
     Shrink the cycle to create a new blossom $B$ and initialize $z_B=0$.
     Add all edges incident to previously odd blossoms on the cycle
     to the pending queue.
<li> If $V$ is not yet in any tree, add $e$ and $V$ to $U$'s tree.
     If $f=\{x,w\}$ is the outer matching edge incident to $V$ with $x$ in $V$,
     add $f$ and $W$ to the tree.
     Add outer edges incident to $W$ to the pending queue.
</ul>
<p style="padding-left:5%">
If there is no queued edge, modify the vertex labels by
subtracting $\delta$ from the labels of all even vertices and
adding $\delta$ to the labels of all odd vertices;
also add $2\delta$ to the labels of all non-trivial even outer blossoms
and subtract $2\delta$ from the labels of all non-trivial odd outer blossoms.
Observe that this relabeling ensures that tight edges in trees
in the outer graph remain tight and all tight edges internal to
blossoms remain tight.
If the relabeling makes $z_u=0$ for all unmatched vertices $u$,
the algorithm can terminate.
If the relabeling makes any edges with an even endpoint tight,
add them to the queue.
If the relabeling makes $z_B=0$ for some odd outer blossom $B$, expand $B$.
<p>
Notice that edges are checked to confirm that they are <i>eligible</i>
for use before they are processed. Ineligible edges are discarded.
In order for an edge to be eligible, it must be tight, outer, have an even
endpoint, but no odd endpoint.
Edges on the queue need not be checked for tightness or having an
even endpoint, since they are only placed on the queue if they have
these properties and they retain these properties while on the queue.
Edges must be checked for the other two properties because they can
become inner or acquire an odd endpoint while they are in the queue.
<p>
The value of $\delta$ is chosen so that the dual variables remain
non-negative and no dual constraints are violated.
This can be accomplished by making
$\delta=\min\{\delta_1,\delta_2, \delta_3,\delta_4\}$, where
$\delta_1$ is the smallest label for an even vertex,
$\delta_2$ is the smallest slack in a dual constraint involving an
outer edge with one even and one unbound endpoint and
$\delta_3$ is one half of the smallest slack in a dual constraint involving
an outer edge with two even endpoints.
$\delta_4$ is one half of the smallest odd outer blossom label.
Notice that the only edges for which the slack in the dual constraint
decreases are the outer edges with an even endpoint and no unbound endpoint.
For other outer edges, the slack either doesn't change or gets larger.
For inner edges, the slack does not change.
The choice of $\delta_2$ and $\delta_3$
ensures that no dual constraint is violated and the choice of $\delta_4$
ensures that no blossom label becomes negative.
<p>
When an odd blossom $B$ is expanded, the sub-blossoms on its blossom
cycle must either be incorporated into their outer tree or
re-classified as unbound. Specifically, if $u$ is the vertex
in $B$ connecting to its outer matching edge and $v$ is the
vertex in $B$ that is linked to its tree parent, then the
sub-blossoms on the even-length segment of the cycle from
$U$ to $V$ are incorporated into the tree.
The remaining sub-blossoms are re-classified as unbound.
This does not change any matching edges.
<p>
Note that if $\delta=\delta_1$, all unmatched vertices have $z_u=0$,
since every relabeling step reduces the label for every unmatched vertex
and no vertex becomes unmatched, once it is matched.
So in this case, the solution satisfies all the CS conditions
meaning that the current matching has maximum weight.
If $\delta=\delta_4$ an odd blossom is expanded.
Otherwise, at least one edge with an even endpoint becomes tight,
allowing the search for augmenting paths to resume.
<p>
As in the bipartite case, the augmenting paths chosen by Edmond's algorithm
have maximum weight. To understand why, note that edges on these paths are
tight (with respect to the modified dual constraints),
pass through the base of each blossom $B$ that intersects the path
and have an even number of edges within $B$. Consequently $z_B$ makes a net
contribution of zero to the path's weight.
No other augmenting paths can have larger weight.

<h4>Analysis of Algorithm for General Case</h4>
A vertex or blossom can alternate between unbound and odd
several times during the course of a phase, but once it becomes even,
it stays even.
Only an outer unbound blossom can become odd, since
expanding a blossom does not create any new odd blossoms,
it just converts inner odd blossoms into outer odd blossoms.
Consequently, all odd blossoms expanded during a phase existed at the
start of the phase, so at most $n/2$ odd blossoms are expanded
during a phase and no vertex becomes odd or unbound more than
$n/2$ times.
<p>
The algorithm must keep track of the structure of the blossoms.
This can be done using a <i>blossom structure tree</i> for every outer
blossom, in which the leaves are the vertices in the blossom and
the non-leaf vertices are non-trivial blossoms.
The parent of each blossom in the tree is the next larger blossom
that contains it with the outer blossom appearing at the root.
This structure is easy to maintain as new blossoms are formed or
existing blossoms expanded and enables efficient processing
of the vertices in a blossom or the edges incident to its vertices.
<p>
The algorithm also needs a way to determine the outer blossom
containing a given vertex or inner blossom. One way to do this is
to simply go up blossom structure tree. It's more efficient to
maintain a separate mapping from a vertex or blossom to the outer
blossom that contains it. For now, let's defer the question of how
this mapping is maintained and proceed to analyze the algorithm's
performance, assuming that outer blossom computations take constant time.
The analysis can be broken down into several pieces.
<ul>
<li>    The number of steps that extend a tree is at most $n/2$ per phase.
        The total time spent on such steps is $O(n^2)$, excluding the
        time for adding edges to the pending queue.
<li>    The number of steps that form a new blossom is at most
        $n/2$ per phase, since the total number of blossoms can never
        exceed $n/2$ and no newly formed blossom is expanded before
        the end of the phase.
        Since the total length of all the blossom cycles formed in a
        phase is at most $3n/2$,
        the total time spent on these steps is $O(n^2)$, excluding the time
        for adding edges to the pending queue.
<li>    Edges added to the pending queue in steps that extend a tree or
        form a new blossom are incident to outer blossoms at the time those
        blossoms become even. The blossoms that become even during a cycle are
        vertex disjoint, meaning that their incident edges can all be
        processed in $O(m)$ time per phase, so the total time spent adding
        edges to the pending queue during steps that extend a tree
        or form a new blossom is $O(mn)$.
<li>    The number of relabeling steps that add an edge to the pending queue
        is $O(n)$ per phase since each of these steps provides an edge
        that can be used to extend the tree, or to form a new blossom
        or to complete an augmenting path before the next relabeling step.
<li>    The number of relabeling steps that expand an odd blossom is
        at most $n/2$ since only blossoms that were present at the start
        of the phase can become odd.
<li>    Each relabeling step can be done in $O(m)$ time so the total time
        for all relabeling steps is $O(mn^2)$.
</ul>
All that remains is to determine the time needed to maintain the mapping
from vertices and inner blossoms to outer blossoms. This changes every
time a new blossom is formed or a blossom is expanded. So the number of
times the mapping changes is $O(n^2)$. Since the mapping can be computed
in $O(n)$ time, the total time spent maintaining the mapping is $O(n^3)$. 
It follows that Edmond's algorithm can be implemented to run in
$O(mn^2)$ time for general graphs.
It's worth noting that the worst-case number of relabel operations
and the worst-case number of blossoms have a big impact on the
worst-case running time. If the number of relabels and blossoms grows
more slowly than $O(n^2)$, then the running time will also grow
more slowly. In random graphs, for example, both the number of
relabels and blossoms grows at a rate closer to $O(n)$ yielding
a running time of $O(mn)$ rather than $O(mn^2)$.
<p>
There is one last issue to consider. As described above, whenever the
matching is augmented, the algorithm reverses the matching status of
the path edges in the outer graph and within the blossoms that lie along
the path. One can simplify the implementation by deferring the
processing of the internal edges.
This maintains the consistency of the outer matching edges
but may create conflicts between inner matching edges and
outer matching edges.
Whenever a blossom is $B$ is expanded, the matching status of the edges
on its cycle can be updated to eliminate any conflict that might be present.
This is accomplished by reversing the edges on
the even length part of the cycle connecting the sub-blossom incident to
the external matching edge to the sub-blossom with
no matching edge on the cycle.

<h4>Implementation of the General Algorithm</h4>
The implementation of Edmond's algorithm uses a special-purpose
<i>Blossoms</i> data structure to keep track of the structure
of the alternating path trees and the blossoms.
It supports the following methods.
<ul>
<li>    $\base(b)$ returns the base of blossom $b$
        in the context of the outer graph (if the blossom is matched,
        this is the endpoint of the matching edge incident to the blossom);
        this method can also be used to change the base.
<li>    $\state(b)$ returns the state of outer blossom
        $b$ ($-1$ for odd, 0 for unbound, $+1$ for even);
        can also be used to change the state.
<li>    $\link(b)$ returns a pair $[v,e]$ where
        $v$ is a vertex in blossom $b$ and
        $e$ is an edge joining $v$ to a vertex
        not in $b$; if $b$ is an outer blossom,
        $e$ is an edge to its parent in the alternating
        path tree; if $b$ is an inner blossom,
        $e$ is an edge to the next blossom in the blossom
        cycle of $b$'s parent;
        this method can also be used to set the link.
<li>    $\parent(b)$ returns the innermost blossom that contains
        $b$ as a sub-blossom.
<li>    $\outer(b)$ returns the outer blossom containing blossom
        $b$.
<li>    $\firstOuter()$ returns the first outer blossom.
<li>    $\nextOuter(b)$ returns the next outer blossom after
        $b$.
<li>    $\firstIn(b)$ returns the first vertex in
        blossom $b$.
<li>    $\nextIn(b,u)$ returns the next vertex in
        blossom $b$ following $u$.
<li>    $\addBranch(e,v)$ adds a branch to an alternating path
        tree; $e$ is a edge incident to an even vertex
        or blossom and $v$ is an endpoint of $e$ in an
        unbound blossom $V$; $V$ and the blossom
        $W$ at the other endpoint of its incident matching edge
        are added to the tree.
<li>    $\addBlossom(e,A)$ forms a new blossom from the outer blossoms
        on the cycle connecting the endpoints of $e$ to their nearest
        common ancester $A$ in the outer graph.
<li>    $\expand(B)$ expands an outer blossom $B$ and
        propagates the matching in the outer graph to $B$'s
        blossom cycle; also makes the unmatched sub-blossoms tree roots and the
        matched blossoms unbound.
<li>    $\expandOdd(B)$ expands an odd outer blossom $B$.
        If $u$ and $v$ are the endpoints of the two outer edges incident to
        $B$ that are in its tree, then after the blossom is expanded,
        $U$ and $V$ are the new outer-blossoms containing $u$ and $v$.
        The new outer blossoms along the even length segment of 
        of the blossom cycle connecting $U$ and $V$ are integrated into
        the tree, while the remaining new outer blossoms become unbound.
</ul>
There are also several other helper methods that assist with details
of the methods described above.
The structure of the blossoms and sub-blossoms is maintained as a
$\Forest$ object, with the parent of a blossom $b$ in
its tree being the smallest blossom that contains $b$.
<p>
The $\Blossoms$ object can be configured to use one of
three different methods to compute $outer(b)$.
The simplest simply goes up the blossom structure tree to its root.
The next simplest maintains an array mapping
a vertex or blossom to its outer blossom.
This array is modified whenever
a new blossom is formed or an existing blossom is expanded.
The third method is discussed in the next section.
<p>
The (abridged) <i>Javascript</i> implementation of the $\Blossoms$
data structure appears below.
<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
export default class Blossoms extends Top {
    g;              // reference to client's graph
    match;          // reference to client's matching

    bsf;            // blossom structure forest defines hierarchy of
                    // blossoms and sub-blossoms

    State;          // State[b] is state of outer blossom b; for inner
                    // blossoms, State[b] is undefined
    Base;            // if b is an outer blossom, Base[b] identifies the unique
                    // vertex in b that has no matching edge with both
                    // endpoints in b's vertex set
    Link;           // Link[b] is pair [v,e] where e is an edge incident to
                    // b and v is the endpoint of e in b; if undefined: [0,0];
                    // for an external blossom b, e is the edge to b's
                    // tree parent; for an internal blossom, e is an edge to
                    // next internal blossom in the blossom cycle of b's parent

    outerMethod;    // method used to compute outer: 0 for find root of bsf,
                    // 1 for static map method, 2 find root of balanced forest
                    // where each tree enumerates vertices in an outer blossom
    Outer;          // reference to data structure used to compute outer

    ids;            // list of available blossom ids (reduced by n)
    blist;          // temporary list used when forming new blossom

    /** Constructor for Blossoms object.
     *  @param g is the client's graph on which matching is computed
     *  @param match is the client's matching object
     */
    constructor(g, match, outerMethod=0) {
        super(g.n + ~~(g.n/2));
        this.g = g;
        this.match = match;
        this.outerMethod = outerMethod;

        this.bsf = new Forest(this.n);

        this.Base = new Int32Array(this.n+1);
        for (let i = 0; i <= this.g.n; i++) this.Base[i] = i;
        this.State = new Int32Array(this.n+1).fill(1);
        for (let e = match.first(); e != 0; e = match.next(e)) {
            this.State[this.g.left(e)] = 0;
            this.State[this.g.right(e)] = 0;
        }
        this.Link = new Array(this.n+1);
        for (let b = 1; b <= this.n; b++) this.Link[b] = [0,0];

        this.ids = new List(this.n - this.g.n);
        for (let b = g.n+1; b <= this.n; b++) this.ids.enq(b-this.g.n);
        this.blist = new List(this.n);

        if (this.outerMethod == 1) {
            this.Outer = new Int32Array(this.n+1);
            for (let b = 1; b <= this.n; b++) this.Outer[b] = b;
        } else if (this.outerMethod == 2) {
            this.Outer = {
                bf : new BalancedForest(this.n),
                bid : new Int32Array(this.n+1),
                root : new Int32Array(this.n+1)
            }
            for (let b = 1; b <= this.n; b++)
                this.Outer.bid[b] = this.Outer.root[b] = b;
        }
    }

    validBid(b) { return 0 <= b && b <= this.n &&
                  !this.ids.contains(b-this.g.n); }

    base(b,u=-1) { if (u!=-1) this.Base[b] = u; return this.Base[b]; }

    state(b,s=-3) { if (s!=-3) this.State[b] = s; return this.State[b]; }

    link(b,p=-1) { if (p!=-1) this.Link[b] = p; return this.Link[b]; }

    firstOuter() {
        for (let b = 1; b <= this.n; b++) {
            if (this.parent(b)) continue;
            if (b <= this.g.n || !this.ids.contains(b-this.g.n))
                return b;
        }
        return 0; 
    }

    nextOuter(b) {
        for (b++; b <= this.n; b++) {
            if (this.parent(b)) continue;
            if (b <= this.g.n || !this.ids.contains(b-this.g.n))
                return b;
        }
        return 0; 
    }

    outer(b) {
        if (this.outerMethod == 0)
            return this.bsf.root(b);
        else if (this.outerMethod == 1)
            return this.Outer[b];
        else
            return this.Outer.bid[this.Outer.bf.root(b)];
    }

    refreshOuter(b) {
        for (let sb = this.bsf.first(b); sb; sb = this.bsf.next(sb)) {
            this.Outer[sb] = b;
        }
    }

    firstIn(b) { return this.bsf.firstLeaf(b); }

    lastIn(b) { return this.bsf.lastLeaf(b); }

    nextIn(b,u) { return this.bsf.nextLeaf(u,b); }

    parent(b) { return this.bsf.p(b); }

    firstSub(b) { 
        if (this.ids.contains(b-this.g.n)) return 0;
        return this.bsf.firstChild(b);
    }

    nextSub(s) { return this.bsf.nextSibling(s); }

    addBranch(e,v,V=this.outer(v)) {
        this.state(V,-1); this.link(V,[v,e]);
        let bV = this.base(V);
        let me = this.match.at(bV);
        let w = this.g.mate(bV,me); let W = this.outer(w);
        this.state(W,+1); this.link(W,[w,me]);
        return W;
    }
    
    addBlossom(e, A) {
        // initialize
        let u = this.g.left(e);  let U = this.outer(u);
        let v = this.g.right(e); let V = this.outer(v);
        let Alink = this.link(A); // save for later use

        // first, create ordered list of sub-blossoms of new blossom
        // using link values
        let subs = this.blist; subs.clear();
        let B = U;
        while (B != A) {
            subs.push(B);        // adds B to front of subs
            let [x,ee] = this.link(B);
            B = this.outer(this.g.mate(x,ee));
        }
        subs.push(A);
        B = V;
        while (B != A) {
            subs.enq(B);        // adds B to end of subs
            let [x,ee] = this.link(B);
            B = this.outer(this.g.mate(x,ee));
        }

        // now, re-direct the links for sub-blossoms on the "left sub-cycle";
        // undefine sub-blossom state values while we're at it
        let firstPart = true;
        for (let B = subs.first(); B; B = subs.next(B)) {
            if (B == U) {
                this.link(B,[u,e]); firstPart = false;
            } else if (firstPart) {
                // reverse direction of links in first part of cycle
                let nextB = subs.next(B);
                let [x,ee] = this.link(nextB);
                this.link(B,[this.g.mate(x,ee),ee]);
            }
            this.state(B,-2);    // -2 means undefined
        }
    
        // finally, use the list of sub-blossoms to construct blossom
        B = this.construct(subs);
        this.state(B, +1);
        this.link(B, Alink);
        return [B, subs, U];
    }

    construct(subs) {
        let B = this.g.n + this.ids.deq();
        this.base(B, this.base(subs.first()));
        for (let b = subs.first(); b; b = subs.next(b)) {
            this.bsf.link(b,B);
        }

        if (this.outerMethod == 1) {
            this.refreshOuter(B);
        } else if (this.outerMethod == 2) {
            let bf = this.Outer.bf;
            let bid = this.Outer.bid;
            let root = this.Outer.root;

            bid[B] = root[B] = B;
            for (let b = subs.first(); b; b = subs.next(b)) {
                root[B] = bf.append(root[B],root[b]);
                bid[root[B]] = B;
            }
        }
        return B;
    }

    expand(B) {
        let [subs,bBsub] = this.deconstruct(B);
        // now set sub-blossom links to [0,0] and states to 0 or +1
        for (let b = subs.first(); b; b = subs.next(b)) {
            this.state(b, this.match.at(this.base(b)) ?  0 : +1);
            this.link(b,[0,0]);
        }
        return subs;
    }

    expandOdd(B) {
        let [subs,bBsub] = this.deconstruct(B);
            // bBsub is sub-blossom in subs that contained base(B) before B
            // B was deconstructed

        let [v] = this.link(B); let V = this.outer(v);
            // V is outer blossom incident to edge linking
            // to B's former tree parent

        // set states on even-length cycle segment from V to B and redirect
        // links if necessary; make other sub-blossoms unbound
        if (this.match.contains(this.link(V)[1])) {
            // reverse links on path from bBsub back to V
            let b = bBsub; let sb = -1;
            while (b != V) { // even length segment
                let pb = subs.prev(b) ? subs.prev(b) : subs.last();
                let [x,e] = this.link(pb);
                this.link(b,[this.g.mate(x,e),e]);
                this.state(b,sb); sb = -sb;
                b = pb;
            }
            b = subs.prev(b) ? subs.prev(b) : subs.last();
            while (b != bBsub) { // odd length segment
                this.link(b,[0,0]); this.state(b,0);
                b = subs.prev(b) ? subs.prev(b) : subs.last();
            }
        } else {
            let b = bBsub; let sb = -1;
            while (b != V) { // even length segment
                this.state(b,sb); sb = -sb;
                b = subs.next(b) ? subs.next(b) : subs.first(b);
            }
            b = subs.next(b) ? subs.next(b) : subs.first(b);
            while (b != bBsub) { // odd length segment
                this.link(b,[0,0]); this.state(b,0);
                b = subs.next(b) ? subs.next(b) : subs.first(b);
            }
        }
        this.link(V,this.link(B)); this.state(V,-1);
        return subs;
    }

    deconstruct(B) {
        let bB = this.base(B);
        let subs = this.blist; subs.clear();

        let b0 = this.firstSub(B);
        while (b0) {
            subs.enq(b0);
            this.bsf.cut(b0);        // remove b0 from B's list of sub-blossoms
            b0 = this.firstSub(B);
        }
        this.ids.enq(B-this.g.n); // return B to list of available ids

        if (this.outerMethod == 1) {
            for (let b = subs.first(); b; b = subs.next(b)) {
                this.refreshOuter(b);
            }
        } else if (this.outerMethod == 2) {
            let bf = this.Outer.bf;
            let bid = this.Outer.bid;
            let root = this.Outer.root;

            let next;
            for (let b = subs.first(); b; b = next) {
                next = subs.next(b);
                if (next) {
                    let [t1,t2] = bf.split(next);            
                    root[b] = t1; bid[t1] = b;
                    bf.join(0,next,t2);
                } else {
                    root[b] = bf.root(b); bid[root[b]] = b;
                }
            }
        }

        let bBsub = this.outer(bB); this.base(bBsub, bB);
        this.extendMatching(subs, bBsub);

        return [subs, bBsub];
    }

    /** Extend a matching to new outer blossoms formed when a
     *  blossom is expanded (helper method).
     *  @param subs is a List of new outer blossoms formed when a former
     *  blossom B is expanded
     *  @param bBsub is the former sub-blossom of B that contains its base;
     *  the matching status of tree edges incident to blossoms in subs is
     *  determined by starting at bBsub and alternating around the former
     *  blossom cycle; the base values of the blossoms in subs are also
     *  updated to be consistent with the new matching edges.
     */
    extendMatching(subs, bBsub) {
        let sub = bBsub; let even = true; let first = true;
        while (first || sub != bBsub) {
            if (first) first = false;
            let next = (subs.next(sub) ? subs.next(sub) :
                                         subs.first());
            let [v,e] = this.link(sub);
            if (this.match.contains(e))
                this.match.drop(e);
            if (!even) {
                this.match.add(e);
                this.base(sub, v);
                this.base(next, this.g.mate(v,e));
            }
            even = !even; sub = next;
        }
    }
}
</textarea> <p>
The following script can be used to demonstrate the $\Blossoms$
data structure.
<pre style="padding-left:5%">
let g = new Graph();
g.fromString('{a[b j k] b[a c] c[b d j h] d[c e f] e[d f] f[d e g i] g[f i] ' +
             'h[i n] i[f g h] j[a c k] k[a j m] l[m n] m[k l n] n[h l m]}');
let match = new Matching(g);
match.fromString('[bc fg hi jk mn]');
let bloss = new Blossoms(g,match);
bloss.fromString('{{[A(d e f!)]} ' +
                 '{[a(b(c(A{d,c}(g{g,f}))) j(k))] [l(m(n))]}}');
log(bloss.toString();
log(bloss.outerGraph2string());
</pre>
It produces the output below.
<pre style="padding-left:5%">
{{[A(d e f!)]} {[a(b(c(A{d,c}(g{g,f}))) j(k))] [l(m(n))]}} 
{
a[b j k] b[a c] c[b A j h] g[A i]
h[c i n] i[A g h] j[a c k] k[a j m]
l[m n] m[k l n] n[h l m] A[c g i]
} 
</pre>
Notice that the $\Blossoms$ object is defined relative to
a previously defined graph and matching.
One need only specify the matching edges in the outer graph,
as the matching edges within a blossom can be inferred.
The new object $\bloss$ is defined to include one non-trivial
blossom and two alternating path trees.
In the $\fromString$ method, the first component of
the string shows the blossom, designated by an upper-case $A$.
The exclamation point identifies the base of $A$
(from the perspective of the outer graph).
This is optional, as the base of the blossom can be inferred by the
matching edge incident to the blossom (when there is no such matching
edge, the base is the first sub-blossom in the list of sub-blossoms).
The second component of the string shows the two trees,
the first containing the blossom.
The edges $\{d,c\}$ and $\{g,f\}$ joining
$A$ to its neighbors in its tree are shown explicitly.
The first line of the output matches the argument to the
$\fromString()$ method and the second line shows
the outer graph.
The diagram below shows the outer graph,
and highlights the outer matching edges, the blossom and tree edges.
<p>
<div  style="text-align:center;">
<img width="40%" src="figs/Blossoms1.png">
</div>
<p>
Note that the base of blossom $A$ is highlighted.
Replacing the last line of the script with the following four lines adds a
branch and a blossom to the graph.
<pre style="padding-left:5%">
bloss.addBranch(g.findEdge(3,8),8);       // {c,h} h
bloss.addBlossom(g.findEdge(1,11),1);     // {a,k} 1
log(bloss.blossoms2string());
log(bloss.trees2string());
</pre>
and produces the following additional output.
<pre style="padding-left:5%">
{[A(d e f!)] [B(a k j)]} 
{[l(m(n))] [B(b{b,a}(c(h(i) A{d,c}(g{g,f}))))]} 
</pre><p>
This is illustrated below.
<p>
<div  style="text-align:center;">
<img width="40%" src="figs/Blossoms2.png">
</div>
<p>
Let's go another step by inserting the following two lines before
the final calls to $\textit{log}()$.
<pre style="padding-left:5%">
bloss.expandOdd(g.n+1);                   // blossom A
bloss.addBlossom(g.findEdge(12,14),12);   // {l,n} l
</pre>
This produces the result below.
<pre style="padding-left:5%">
{[B(a k j)] [C(l n m)]} 
{[B(b{b,a}(c(d(e(f(g))) h(i))))]}
</pre>
<p>
<div  style="text-align:center;">
<img width="40%" src="figs/Blossoms3.png">
</div>
<p>
Notice how the edge $\{d,e\}$ within former blossom
$A$ has been added to the matching and the path from
$d$ to $f$ has been inserted into the first tree.
Also, the vertices of the second tree have been collapsed into
the new blossom $C$.
Adding one more blossom using $\bloss.\addBlossom(g.\findEdge(7,9),3)$
produces
<pre style="padding-left:5%">
{[B(a k j)] [C(l n m)] [D(c d e f g i h)]} 
{[B(b{b,a}(D{c,b}))]} 
</pre>
<p>
<div  style="text-align:center;">
<img width="40%" src="figs/Blossoms4.png">
</div>
<p>
As a final observation, note that edge $\{k,m\}$ can be used to
form augmenting path $[a,j,k,m,n,l]$, or alternatively,
edge $\{h,n\}$ can be used to form the augmenting path
$[a,b,c,d,e,f,g,i,h,n,m,l]$.
<p>
An abridged <i>Javascript</i> implementation of Edmond's algorithm is
shown below.
<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
let g             // shared copy of graph
let match;        // Matching object representing matching for graph
let bloss;        // Blossoms object representing blossoms and alternating trees

let z;            // z[b] is dual variable for blossom (or vertex) b
let slack;        // slack[e] is slack of dual constraint for edge e
let q;            // list of tight edges with an even endpoint
let blist;        // temporary list of blossoms
let mark;         // temporary array of flags

export default function wmatchE(G ..) {
    g = G;
    match = new Matching(g);
    bloss = new Blossoms(g, match, 1);
    z = new Float32Array(bloss.n+1);
    slack = new Float32Array(g.edgeRange+1);
    q = new List(g.edgeRange);
    blist = new List(bloss.n);
    mark = new Int8Array(bloss.n+1).fill(false);

    let maxwt = 0;
    for (let e = g.first(); e; e = g.next(e)) {
        maxwt = Math.max(g.weight(e),maxwt);
    }
    if (maxwt == 0)
        return [match, trace ? 'no positive weights' : '', {'weight':0}];
    z.fill(maxwt/2.0,1,g.n+1);

    for (let e = g.first(); e; e = g.next(e)) {
        slack[e] = maxwt - g.weight(e);
        if (slack[e] == 0) q.enq(e);
    }

    while (true) {
        while (!q.empty()) {
            let e = q.deq();
            let [u,v] = [g.left(e),g.right(e)];
            let [U,V] = [bloss.outer(u),bloss.outer(v)];
            let [sU,sV] = [bloss.state(U),bloss.state(V)];
            if (U == V || sU + sV <= 0 || slack[e] > 0) continue;
            // at least one even endpoint
            if (sU + sV == 1 && sU == 0) {
                [u,v] = [v,u]; [U,V] = [V,U]; [sU,sV] = [sV,sU];
            }

            // now e is tight, U is even and V is even or unbound
            if (sV == +1) {
                let A = nca(U,V);
                if (A == 0) {
                    augment(e); newPhase();
                } else {
                    let [B,subs,sb] = bloss.addBlossom(e,A); z[B] = 0;
                    let state = +1;
                    for (let b = subs.first(); b; b = subs.next(b)) {
                        if (state == -1) add2q(b);
                        state = (b == sb ? +1 : -state);
                    }
                }
            } else if (sV == 0) {
                let W = bloss.addBranch(e,v,V); add2q(W);
            }
        }
        if (relabel()) break;
    }

    bloss.rematchAll();
        // extend matching in outer graph to all blossoms and sub-blossoms

    return [match ..];
}

function augment(e) {
    // trace paths up to tree roots and update matching
    match.add(e);
    let x = g.left(e); let X = bloss.outer(x);
    let [y,ee] = bloss.link(X);
    while (y) {
        if (match.contains(ee)) {
            match.drop(ee); bloss.base(X,x);
        } else {
            match.add(ee); bloss.base(X,y);
        }
        x = g.mate(y,ee); X = bloss.outer(x); [y,ee] = bloss.link(X);
    }
    bloss.base(X,x);

    x = g.right(e); X = bloss.outer(x); [y,ee] = bloss.link(X);
    while (y) {
        if (match.contains(ee)) {
            match.drop(ee); bloss.base(X,x);
        } else {
            match.add(ee); bloss.base(X,y);
        }
        x = g.mate(y,ee); X = bloss.outer(x); [y,ee] = bloss.link(X);
    }
    bloss.base(X,x);
}

/** Prepare for next phase, following an augmentation.
 *  Expand all outer blossoms with z==0, set states of remaining outer
 *  blossoms to unbound or even and their links to null.
 *  Put all vertices in even blossoms into queue of even vertices.
 */
function newPhase() {
    // expand non-trivial outer blossoms with z == 0
    q.clear(); blist.clear();
    for (let b = bloss.firstOuter(); b; b = bloss.nextOuter(b)) {
        if (z[b] == 0 && b > g.n) blist.enq(b);
    }
    while (!blist.empty()) {
        let b = blist.deq();
        let subs = bloss.expand(b); 
        for (let sb = subs.first(); sb; sb = subs.next(sb)) {
            if (z[sb] == 0 && sb > g.n) blist.enq(sb);
        }
    }

    // set state and link for remaining blossoms 
    for (let b = bloss.firstOuter(); b; b = bloss.nextOuter(b)) {
        bloss.state(b, match.at(bloss.base(b)) ? 0 : +1);
        bloss.link(b,[0,0])
    }
    // and add eligible edges to q
    for (let b = bloss.firstOuter(); b; b = bloss.nextOuter(b))
        if (bloss.state(b) == +1) add2q(b);
}

function relabel() {
    let d1 = Infinity;
    for (let u = 1; u <= g.n; u++) {
        if (bloss.state(bloss.outer(u)) == +1) d1 = Math.min(d1, z[u]);
    }
    if (d1 == Infinity) d1 = 0;

    let d2 = Infinity; let d3 = Infinity; let d4 = Infinity;
    let smallOddBloss = 0; // odd blossom with smallest z[b]
    for (let b = bloss.firstOuter(); b; b = bloss.nextOuter(b)) {
        if (bloss.state(b) == +1) {
            for (let u = bloss.firstIn(b); u; u = bloss.nextIn(b,u)) {
                for (let e = g.firstAt(u); e; e = g.nextAt(u,e)) {
                    let v = g.mate(u,e); let V = bloss.outer(v);
                    if (V == b) continue;
                    let sV = bloss.state(V);
                         if (sV == 0) d2 = Math.min(d2, slack[e]);
                    else if (sV == 1) d3 = Math.min(d3, slack[e]/2);
                }
            }
        } else if (b > g.n && bloss.state(b) == -1) {
            if (z[b]/2 < d4) {
                d4 = z[b]/2; smallOddBloss = b;
            }
        }
    }

    let delta = Math.min(d1,d2,d3,d4);

    // adjust the dual variables for vertices
    for (let u = 1; u <= g.n; u++) {
        if (bloss.state(bloss.outer(u)) == +1) z[u] -= delta;
        if (bloss.state(bloss.outer(u)) == -1) z[u] += delta;
    }

    // adjust dual variables for outer blossoms and
    // slacks of outer edge constraints (slacks for inner edges don't change)
    for (let b = bloss.firstOuter(); b; b = bloss.nextOuter(b)) {
        if (b > g.n) {
            if (bloss.state(b) == +1) z[b] += 2*delta;
            if (bloss.state(b) == -1) z[b] -= 2*delta;
        }
        for (let u = bloss.firstIn(b); u; u = bloss.nextIn(b,u)) {
            for (let e = g.firstAt(u); e; e = g.nextAt(u,e)) {
                if (u != g.left(e)) continue;
                let v = g.right(e); let V = bloss.outer(v);
                if (b == V) continue;
                let ss = bloss.state(b) + bloss.state(V);
                slack[e] -= ss * delta;
                if (slack[e] == 0 && !q.contains(e))
                    ss == 2 ? q.push(e) : q.enq(e);
            }
        }
    }

    if (delta == d1) {
        if (trace) traceString += ' and finished\n';
        return true; // we have max weight matching
    }

    // now, add new even edges to q
    if (delta == d2 || delta == d3) {
        for (let b = bloss.firstOuter(); b; b = bloss.nextOuter(b)) {
            if (bloss.state(b) == +1) add2q(b)
        }
    }

    if (delta == d4) {
        // expand an odd blossom with zero z
        let subs = bloss.expandOdd(smallOddBloss); deblossoms++;
        for (let b = subs.first(); b; b = subs.next(b)) {
            if (bloss.state(b) == +1) add2q(b);
        }
    }

    return false;
}

/** Add edges incident to an even blossom to q.
 *  @param b is an even blossom or sub-blossom.
 */
function add2q(b) {
    let B = bloss.outer(b);
    for (let u = bloss.firstIn(b); u; u = bloss.nextIn(b,u)) {
        for (let e = g.firstAt(u); e; e = g.nextAt(u,e)) {
            let v = g.mate(u,e); let V = bloss.outer(v);
            if (bloss.state(V) >= 0 && V != B && slack[e] == 0) {
                if (!q.contains(e)) q.enq(e);
            }
        }
    }
}

/** Find the nearest common ancestor of two vertices in
 *  the current graph.
 *  To avoid excessive search time, search upwards from both vertices in
 *  parallel, using mark bits to identify the nca. Before returning,
 *  clear the mark bits by traversing the paths a second time.
 *  @param U is an outer blossom
 *  @param V is another outer blossom
 *  @returns the nearest common ancestor of u and v or 0 if none
 */
function nca(U, V) {
    let result;
    // first pass to find the nca
    let X = U; let [x,ex] = bloss.link(X);
    let Y = V; let [y,ey] = bloss.link(Y);
    while (true) {
        steps++;
        if (X == Y) { result = X; break; }
        if (mark[X]) { result = X; break; }
        if (mark[Y]) { result = Y; break; }
        if (!x && !y) { result = 0; break; }
        if (x) {
            mark[X] = true;
            X = bloss.outer(g.mate(x,ex)); [x,ex] = bloss.link(X);
        }
        if (y) {
            mark[Y] = true;
            Y = bloss.outer(g.mate(y,ey)); [y,ey] = bloss.link(Y);
        }
    }
    // second pass to clear mark bits
    X = U; [x,ex] = bloss.link(X);
    while (mark[X]) {
        mark[X] = false;
        X = bloss.outer(g.mate(x,ex)); [x,ex] = bloss.link(X);
    }
    Y = V; [y,ey] = bloss.link(Y);
    while (mark[Y]) {
        mark[Y] = false;
        Y = bloss.outer(g.mate(y,ey)); [y,ey] = bloss.link(Y);
    }
    return result;
}
</textarea> <p>
The program can be demonstrated using the following script.
<pre style="padding-left:5%">
let g = new Graph();
g.fromString('{ a[b:3 h:1 j:3 k:3] b[a:3 f:2 g:1 j:3 p:2] c[f:1 m:1] ' +
             'd[g:2] e[f:2 g:3 i:2 n:1] f[b:2 c:1 e:2 l:2 o:3 p:1] ' +
             'g[b:1 d:2 e:3 k:1 o:1] h[a:1] i[e:2] j[a:3 b:3] ' +
             'k[a:3 g:1 p:3] l[f:2 o:1] m[c:1 o:2 p:1] n[e:1] ' +
             'o[f:3 g:1 l:1 m:2] p[b:2 f:1 k:3 m:1] }');
let [match,ts] = wmatchE(g,1);
log(ts);
</pre>
The output appears below.
<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
{
a[b:3 h:1 j:3 k:3] b[a:3 f:2 g:1 j:3 p:2] c[f:1 m:1] d[g:2]
e[f:2 g:3 i:2 n:1] f[b:2 c:1 e:2 l:2 o:3 p:1] g[b:1 d:2 e:3 k:1 o:1] h[a:1]
i[e:2] j[a:3 b:3] k[a:3 g:1 p:3] l[f:2 o:1]
m[c:1 o:2 p:1] n[e:1] o[f:3 g:1 l:1 m:2] p[b:2 f:1 k:3 m:1]
}
eligible: [ab aj ak bj eg fo kp]

augment: {a,b,3} a--b
augment: {k,p,3} k--p
augment: {f,o,3} f--o
augment: {e,g,3} e--g

branch: j a b
blossom: {b,j,3} j A[j a b]
branch: Aa k p
relab(1.5 1 0.5 Infinity)
    [bp eg fo kp ak]
blossom: {b,p,2} A B[aAb p k]
relab(1 0.5 0.5 Infinity)
    [mp ah cm dg eg ei fl fo mo bf]
augment: {m,p,1} m--pB
    {[B(A(j a b) p! k)]}

branch: c m p.B
augment: {a,h,1}
    {[c(m(B))]}
    c m aBp--h
    [fo eg ah cm]
    {[B(A(j a! b) p k)]}

branch: d g e
augment: {e,i,2}
    {[d(g(e))]}
    d g e--i
    [fo ah cm ei dg]
    {[B(A(j a! b) p k)]}

branch: l f o
branch: o m c
relab(0.5 1 0.5 Infinity) and finished

final matching: [fo ah cm ei dg kp bj]
</textarea> <p>
The output starts with the graph and a list of the
edges that are initially eligible for use.
Trivial augmenting paths (those with a single edge) are shown by
lines like
<pre style="padding-left:5%">
augment: {a,b,3} a--b
</pre>
which shows the edge joining the trees and the augmenting path.
A non-trivial augmenting path produces more output.
<pre style="padding-left:5%">
augment: {a,h,1}
    {[c(m(B))]}
    c m aBp--h
    [fo eg ah cm]
    {[B(A(j a! b) p k)]}
</pre>
Here, the first line shows the edge joining the two trees;
the second shows the nontrivial external trees before the matching is augmented;
the third shows the augmenting path in the outer graph
with the inter-tree edge highlighted with a pair of dashes;
the fourth line shows the matching edges in the outer graph
resulting from the augmentation;
and the last shows the unexpanded outer blossoms at the start of
the next phase.
<p>
When a new blossom is added, the trace output shows the edge used to
form the new blossom, the nearest common ancestor of its endpoints
and the new blossom.
If this results in an outer graph with 
non-trivial alternating path trees, they are shown on a separate line.
For example:
<pre style="padding-left:5%">
blossom: {b,p,2} A B[aAb p k]
</pre>
When a relabeling operation occurs, the values of the $\delta_i$ are shown,
followed by the eligible edges, the non-trivial blossoms (if any)
and the alternating path trees (if any).
<pre style="padding-left:5%">
relab(1 0.5 0.5 Infinity)
    [mp ah cm dg ei fl mo bf]
</pre>
<p>
The following script can be used to examine the performance of
Edmond's algorithm on random graphs.
<pre style="padding-left:5%">
let n=400; let d=20; let weights=n;
let g = randomGraph(n,d); 
g.randomWeights(randomInteger,1,weights);
let t = Date.now(); let [,,stats] = wmatchE(g); t = Date.now() - t;
log(`n=${n} d=${d} weights=${weights} branches=${stats.branches} ` +
    `blossoms=${stats.blossoms} relabels=${stats.relabels} ` +
    `deblossoms=${stats.deblossoms}\n           ` +
    `steps=${stats.steps} ${t}ms`);
</pre>
Some sample output appears below.
<pre style="padding-left:5%">
n=400 d=20 weights=400 branches=3504 blossoms=30 relabels=199 deblossoms=16
           steps=1633969 128ms 
n=400 d=20 weights=400 branches=2875 blossoms=19 relabels=181 deblossoms=9
           steps=1546400 121ms 
n=400 d=20 weights=400 branches=2780 blossoms=28 relabels=202 deblossoms=20
           steps=1608773 124ms 
n=400 d=20 weights=400 branches=3648 blossoms=95 relabels=241 deblossoms=28
           steps=1965693 148ms 
</pre>
A few things to note. First, the number of blossoms is highly variable
and much smaller than the worst-case of $n^2/4$. The number of relabels
is also far smaller than the worst-case and deblossoms (expansion of
odd blossoms) are even less common.
Also note that the number of branches formed is far fewer than
$n/2$ per phase (on average).
During the early phases, one can expect augmenting paths to be found
quite quickly, since eligible edges are likely to have two unmatched
endpoints. So many of these early phases will have no branches at all,
or very few.
As the number of matching edges grows, more branches are formed and the
alternating path trees get larger, which also makes blossom formation
and relabels more frequent.
<p>
In the results below the edge density is doubled repeatedly,
leading to a rougly proportional increase in the running time.
<pre style="padding-left:5%">
n=400 d=40 weights=400 branches=2424 blossoms=56 relabels=129 deblossoms=2
           steps=2492984 157ms 
n=400 d=80 weights=400 branches=2044 blossoms=30 relabels=60 deblossoms=10
           steps=3955142 212ms 
n=400 d=160 weights=400 branches=1765 blossoms=116 relabels=36 deblossoms=2
           steps=7892744 402ms 
n=400 d=320 weights=400 branches=1452 blossoms=72 relabels=15 deblossoms=1
           steps=14055387 711ms 
</pre>
Doubling the number of vertices, while keeping the same edge density,
increases the time required to find a matching by roughly a factor of four,
or half what one would predict based on the worst-case analysis.
Blossom formation, expansion and relabeling all remain fairly rare,
reducing the time spent on these steps and also reducing the time
needed to maintain the mapping from vertices to outer blossoms.
Consequently, the time to compute the matching can be largely attributed
to the time spent adding edges to the queue, which is $O(mn)$.
This is consistent with the observed performance.
<pre style="padding-left:5%">
n=200 d=20 weights=200 branches=876 blossoms=8 relabels=87 deblossoms=1
           steps=396878 37ms 
n=400 d=20 weights=400 branches=3620 blossoms=31 relabels=195 deblossoms=7
           steps=1682711 132ms
n=800 d=20 weights=800 branches=12337 blossoms=117 relabels=514 deblossoms=44
           steps=7231811 577ms 
n=1600 d=20 weights=1600 branches=47156 blossoms=36 relabels=857 deblossoms=25
           steps=25592211 2173ms 
</pre>
Finally, increasing the number of distinct edge weights increases the
number of relabeling operations, as the example below demonstrates.
<pre style="padding-left:5%">
n=400 d=20 weights=400 branches=3329 blossoms=28 relabels=211 deblossoms=18
           steps=1641904 131ms 
n=400 d=20 weights=1600 branches=4726 blossoms=53 relabels=472 deblossoms=31
           steps=2768003 220ms 
n=400 d=20 weights=6400 branches=4572 blossoms=62 relabels=571 deblossoms=25
           steps=3367966 260ms 
n=400 d=20 weights=25600 branches=4373 blossoms=13 relabels=536 deblossoms=5
           steps=3138486 253ms 
</pre>
Once the number of weights becomes large enough for most edges to have
distinct weights, the number of relabeling operations stops growing.
So for random graphs, the number of blossoms and relabels both seem
to grow no more than linearly in $n$, leading to an overall running
time of $O(mn)$ for Edmonds algorithm on these random graphs.

<h3>Galil, Micali and Gabow's Algorithm</h3>
While Edmond's algorithm performs quite well on random graphs, there is
considerable room to improve its worst-case performance.
Galil, Micali and Gabow [GMG86] showed how the worst-case performance
could be improved to $O(mn\log n)$ with the help of some additional
data structures.
<p>
First, the algorithm maintains several heaps to speedup the relabeling process.
An $\ArrayHeap$ with an $\add2keys$
method (allowing an offset to be added to the keys of all items in
constant time) is suitable for this purpose.
Here is a list of heaps that can be used.
<ul>
<li> $\evh$ contains all the even vertices,
    where the key of vertex $u$ is $z[u]$,
    the dual variable for $u$.
<li> $\ovh$ contains all the odd vertices,
    where the key of vertex $u$ is $z[u]$,
    the dual variable for $u$.
<li> $\ebh$ contains all the even non-trivial blossoms,
    where the key of blossom $b$ is $z[b]/2$,
    the dual variable for $b$.
<li> $\obh$ contains all the odd non-trivial blossoms,
    where the key of vertex $b$ is $z[b]/2$,
    the dual variable for $b$.
<li> $\euh$ contains all the external edges with one even
    and one unbound endpoint,
    where the key of edge $e$ is $slack(e)$,
    the slack of the dual constraint for $e$.
<li> $\eeh$ contains all the external edges with two even endpoints,
    where the key of edge $e$ is $slack(e)/2$.
</ul>
Notice that this allows $\delta_1$ to be computed as $\findmin(\evh)$,
$\delta_2$ as $\findmin(\euh)$ and 
$\delta_3$ as $\findmin(\eeh)$ and 
$\delta_4$ as $\findmin(\obh)$.
Also, dual variables can be effectively updated by adding or subtracting
$\delta$ or $2\delta$ to all the keys in each of these heaps.
This ensures that the key values in the heaps
continue to reflect the proper dual variable values or slacks.
Of course, when a vertex or blossom is removed from its heap,
its key value must be transferred to the array of dual variable values.
This approach also incurs some additional overhead, since a change to the
state of a vertex or blossom now requires updates to one or more heaps.
Still, the overall effect is to significantly reduce the worst-case time bound.
<p>
Now, there is one issue with the heap $\euh$.
When an odd blossom $B$ is expanded, the sub-blossoms of $B$ can become
even or unbound. The unbound sub-blossoms can later become odd.
Consequently, vertices can alternate between odd and unreached many times
during the course of a phase, and that means that edges may alternate
between being even-to-odd and even-to-unbound many times per phase.
That in turn means that they may have to be added and removed from
$\euh$ many times, leading to a potentially large added overhead.
<p>
The issue can be addressed by replacing $\euh$ with a
<a href="../../dataStructures/specialty/specialty.html#GroupHeap">group heap</a>,
$\exh$. 
Recall that in a group heap, the heap items are partitioned into groups
that are classified as <i>active</i> or <i>inactive</i>.
The usual heap operations effectively ignore inactive items, allowing one
to quickly find an active item with the smallest key and to update the
keys of all active items. In the context of Edmond's algorithm,
the heap items are edges with one even endpoint and another that is
either odd or unbound.
There is a group for the edges incident to each odd or unbound outer blossom,
with the groups for the edges at unbound blossoms treated as active.
When an odd blossom is expanded, its edges in $\exh$ are
distributed according to which of the new outer blossoms they are incident to.
<ul>
<li> If the new outer blossom is odd, a new inactive group is created for it.
<li> If the new outer blossom is unbound, a new active group is created for it.
<li> If the new outer blossom is even, its edges are added to $\eeh$,
     the heap for even-to-even edges.
</ul>
To make this operation efficient, the algorithm uses the
group heap's $\divide$ operation
and orders the edges within each group so that those edges incident to
a particular sub-blossom are grouped together.
More precisely, if $e_1=\{u_1,v_1\}$ and $e_2=\{u_2,v_2\}$ are edges
incident to an odd or unbound outer blossom $B$,
where $v_1$ and $v_2$ are the endpoints in $B$,
and $v_1$ precedes $v_2$ in the blossom structure tree for $B$,
then $e_1$ appears before $e_2$ in the group of edges for $B$'s group
in $\exh$. When an odd blossom is expanded, its blossom structure
tree is split into smaller trees and its group in $\exh$ is
similarly split into smaller groups. Because the edges for these smaller
groups are ordered in the same way as the blossom structure tree,
they can be separated using the divide operation.
<p>
There is another issue about the implementation of $\exh$
that must be addressed.
There are times when an individual edge $e$ must be added to a group $g$
associated with outer blossom $B$.
In order to maintain the edge ordering, $e$
must be inserted with the other edges with which it shares an endpoint in $B$.
To facilitate this, $g$ includes a <i>marker</i> item $v_m$
for every vertex in $B$. So, when inserting $e=\{u,v\}$ with $v\in B$,
$e$ is inserted immediately after the marker item $v_m$.
<p>
There is one last issue that must be addressed to achieve the
target performance bound of $O(mn\log n)$.
That involves the computation of the outer blossom containing
a given vertex or inner blossom. Recall that the method used in the standard
version of Edmond's algorithm maintains a mapping from each vertex
to its outer blossom. Since maintaining that mapping can take time
proportional to $n^3$ in the worst-case, that method does not meet
the performance objective.
<p>
An alternative method is to maintain a separate
<a href="../../dataStructures/trees/trees.html#BalancedForest">balanced binary forest</a> in which the vertex set of each tree
corresponds to the vertices and inner blossoms contained in an outer blossom.
If the vertices in such an <i>outer blossom tree</i> are maintained in the
same order as the vertices within the corresponding blossom structure tree,
the outer blossom trees can be easily combined as new blossoms are formed,
or split apart as blossoms are expanded.
An array is maintained that maps the root of each outer blossom tree to its
outer blossom, allowing one to easily identify the outer blossom
containing a given vertex or inner blossom.
An additional array maps each outer blossom to the root of its
outer blossom tree.
Since all the tree operations take $O(\log n)$ time and the number of
tree operations performed by Edmond's algorithm is $O(mn)$,
the time required for using and maintaining this data structure
is $O(mn\log n)$.
<p>
Here is a <i>Javascript</i> implementation of this algorithm.
<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
let g             // shared copy of graph
let match;        // Matching object representing matching for graph
let bloss;        // Blossoms object representing blossoms and matching trees

let Z;            // Z[b] is dual variable for blossom (or vertex) b
                  // for all b not currently in a vertex or blossom heap
let bq;           // temporary list of blossoms
let mark;         // temporary array of flags

let ovh;          // heap of odd vertices, with key[u]=Z[u]
let evh;          // heap of even vertices, with key[u]=Z[u]
let obh;          // heap of odd outer blossoms, with key[b]=Z[b]/2
let ebh;          // heap of even outer blossoms, with key[b]=Z[b]/2

let eeh;          // heap of external edges with two even endpoints
                  // key(e) = slack(e)/2
let exh;          // GroupHeap object containing a group of edges incident
                  // to each odd or unbound blossom, key(e) = slack(e)
let firstVertex;  // firstVertex[b] is first vertex in blossom b

export default function wmatchGMG(G ..) {
    g = G;
    match = new Matching(g);
    bloss = new Blossoms(g, match, 2);
    Z = new Float32Array(bloss.n+1);
    bq = new List(bloss.n);
    mark = new Int8Array(bloss.n+1).fill(false);
    
    ovh = new ArrayHeap(g.n);
    evh = new ArrayHeap(g.n);
    obh = new ArrayHeap(bloss.n);
    ebh = new ArrayHeap(bloss.n);
    eeh = new ArrayHeap(g.edgeRange);
    exh = new GroupHeap(g.edgeRange+g.n, bloss.n);
    firstVertex = new Int32Array(bloss.n+1);

    let maxwt = 0;
    for (let e = g.first(); e; e = g.next(e)) {
        maxwt = Math.max(g.weight(e),maxwt);
    }
    if (maxwt == 0)
        return [match, trace ? 'no positive weights\n' : '', {'weight':0}];
    Z.fill(maxwt/2.0,1,g.n+1);

    for (let u = 1; u <= g.n; u++) {
        evh.insert(u,Z[u]); firstVertex[u] = u;
    }
    for (let e = g.first(); e; e = g.next(e)) {
        eeh.insert(e,slack(e)/2);
    }

    let finished = false;
    while (!finished) {
        // process eligible edges with an even endpoint
        while (true) {
            let ee = eeh.findmin();
            let eu = exh.findmin();
            if (ee && eeh.key(ee) == 0) {
                eeh.delete(ee)
                let [u,v] = [g.left(ee),g.right(ee)];
                let [U,V] = [bloss.outer(u),bloss.outer(v)];
                if (U == V) continue;
                let A = nca(U,V);
                if (A == 0) {
                    // augment matching and prepare for next phase
                    augment(ee); newPhase();
                    continue;
                }

                // add new blossom
                let [B,subs] = bloss.addBlossom(ee,A);
                Z[B] = 0; ebh.insert(B,0);
                firstVertex[B] = firstVertex[subs.first()];

                // now update heaps for former outer blossoms
                for (let b = subs.first(); b; b = subs.next(b)) {
                    if (b > g.n && ebh.contains(b)) {
                        // b no longer an outer blossom, but its vertices
                        // are still even
                        Z[b] = 2*ebh.key(b); ebh.delete(b);
                    } if (b >  g.n && obh.contains(b) ||
                          b <= g.n && ovh.contains(b)) {
                        if (b > g.n) {
                            Z[b] = 2*obh.key(b); obh.delete(b);
                        }
                        for (let u = bloss.firstIn(b); u;
                                     u = bloss.nextIn(b,u)) {
                            Z[u] = ovh.key(u); ovh.delete(u);
                            evh.insert(u, Z[u]);
                        }
                        exh.clear(b); addEXedges(b)
                    }
                }
            } else if (eu && eu <= g.edgeRange) {
                let [u,v] = [g.left(eu),g.right(eu)];
                let [U,V] = [bloss.outer(u),bloss.outer(v)];
                if (bloss.state(U) != +1) [u,v,U,V] = [v,u,V,U];
                if (exh.key(eu, V) != 0) break;
                let W = bloss.addBranch(eu,v,V);

                // update heaps
                if (V > g.n)
                    obh.insert(V, Z[V]/2);
                for (let u = bloss.firstIn(V); u; u = bloss.nextIn(V,u))
                    ovh.insert(u,Z[u]);
                exh.delete(eu, V); exh.deactivate(V);

                if (W > g.n) ebh.insert(W, Z[W]/2);
                for (let u = bloss.firstIn(W); u; u = bloss.nextIn(W,u))
                    evh.insert(u,Z[u]);
                exh.clear(W); addEXedges(W);
            } else {
                break;
            }
        }

        // adjust vertex/blossom labels, creating more eligible edges
        // and/or reducing number of vertices in odd blossoms
        finished = relabel();
    }

    bloss.rematchAll(); // make matching consistent

    return [match ..];
}

function slack(e) {
    return s = z(g.left(e)) + z(g.right(e)) - g.weight(e);
}

function z(b) {
    if (b <= g.n) {
        return  evh.contains(b) ? evh.key(b) :
               (ovh.contains(b) ? ovh.key(b) : Z[b]);
    } else {
        return  ebh.contains(b) ? 2*ebh.key(b) :
               (obh.contains(b) ? 2*obh.key(b) : Z[b]);
    }
}

function augment(e) {
    // trace paths up to tree roots and update matching
    match.add(e);
    let x = g.left(e); let X = bloss.outer(x);
    let [y,ee] = bloss.link(X);
    while (y) {
        if (match.contains(ee)) {
            match.drop(ee); bloss.base(X,x);
        } else {
            match.add(ee); bloss.base(X,y);
        }
        x = g.mate(y,ee); X = bloss.outer(x); [y,ee] = bloss.link(X);
    }
    bloss.base(X,x);

    x = g.right(e); X = bloss.outer(x); [y,ee] = bloss.link(X);
    while (y) {
        if (match.contains(ee)) {
            match.drop(ee); bloss.base(X,x);
        } else {
            match.add(ee); bloss.base(X,y);
        }
        x = g.mate(y,ee); X = bloss.outer(x); [y,ee] = bloss.link(X);
    }
    bloss.base(X,x);
}

/** Prepare for next phase, following an augmentation.
 *  Expand all outer blossoms with z==0, set states of remaining outer
 *  blossoms to unbound or even and their links to null.
 */
function newPhase() {
    // expand outer blossoms with z == 0 (note: these are even)
    bq.clear();
    for (let b = bloss.firstOuter(); b; b = bloss.nextOuter(b)) {
        if (b > g.n && z(b) == 0) bq.enq(b);
    }
    while (!bq.empty()) {
        let b = bq.deq();
        let subs = bloss.expand(b); 
        for (let sb = subs.first(); sb; sb = subs.next(sb)) {
            if (z(sb) == 0 && sb > g.n) bq.enq(sb);
        }
    }

    // set states/links of remaining outer blossoms based on matching status
    for (let b = bloss.firstOuter(); b; b = bloss.nextOuter(b)) {
        bloss.state(b, match.at(bloss.base(b)) ? 0 : +1); bloss.link(b,[0,0]);
    }

    // rebuild the heaps from scratch
    // update the z variables while clearing the vertex and blossom heaps
    for (let u = evh.findmin(); u; u = evh.findmin()) {
        Z[u] = evh.key(u); evh.delete(u);
    }
    for (let u = ovh.findmin(); u; u = ovh.findmin(u)) {
        Z[u] = ovh.key(u); ovh.delete(u);
    }
    for (let b = ebh.findmin(); b; b = ebh.findmin(b)) {
        Z[b] = 2*ebh.key(b); ebh.delete(b);
    }
    for (let b = obh.findmin(); b; b = obh.findmin(b)) {
        Z[b] = 2*obh.key(b); obh.delete(b);
    }
    exh.clear(); eeh.clear();
    // rebuild vertex heaps and edge heaps, using new states
    for (let b = bloss.firstOuter(); b; b = bloss.nextOuter(b)) {
        if (bloss.state(b) == +1) {
            if (b > g.n) ebh.insert(b, Z[b]/2);
            // add ee edges to eeh
            for (let u = bloss.firstIn(b); u; u = bloss.nextIn(b,u)) {
                evh.insert(u,Z[u]);
                for (let e = g.firstAt(u); e; e = g.nextAt(u,e)) {
                    let v = g.mate(u,e); let V = bloss.outer(v);
                    if (V != b && bloss.state(V) == +1 && !eeh.contains(e))
                        eeh.insert(e, slack(e)/2);
                }
            }
        } else {
            // build subheaps for unbound blossoms in exh
            // order of edges with subheaps matches order of vertices within
            // outer blossoms
            let laste = 0;
            for (let u = bloss.firstIn(b); u; u = bloss.nextIn(b,u)) {
                // insert dummy edge for u in b's subheap within exh
                let e = u + g.edgeRange;
                exh.insertAfter(e, b, Infinity, laste); laste = e;
                for (let e = g.firstAt(u); e; e = g.nextAt(u,e)) {
                    let v = g.mate(u,e); let V = bloss.outer(v);
                    if (bloss.state(V) == +1) {
                        exh.insertAfter(e, b, slack(e), laste); laste = e;
                    }
                }
            }
            exh.activate(b);
        }
    }
}

function relabel() {
    let d1 = evh.empty() ? 0 : evh.key(evh.findmin());

    let e = exh.findmin();
    let d2 = (e && e <= g.edgeRange ? slack(e) : Infinity);

    e = eeh.findmin();
    while (e && bloss.outer(g.left(e)) == bloss.outer(g.right(e))) {
        eeh.delete(e); e = eeh.findmin();
    }
    let d3 = (e ? eeh.key(e) : Infinity);

    let d4 = obh.empty() ? Infinity : obh.key(obh.findmin());

    let delta = Math.min(d1,d2,d3,d4);

    evh.add2keys(-delta);   ovh.add2keys(+delta);
    ebh.add2keys(+delta);   obh.add2keys(-delta);
    eeh.add2keys(-delta);   exh.add2keys(-delta);

    if (delta == d4) {
        let b = obh.deletemin();
        Z[b] = 0; 
        let subs = bloss.expandOdd(b);
        for (let sb = subs.first(); sb; sb = subs.next(sb)) {
            let next = subs.next(sb);
            let e = (next ? firstVertex[next] + g.edgeRange : 0);
            exh.divide(b, e, sb);
        }
        for (let sb = subs.first(); sb; sb = subs.next(sb)) {
            if (bloss.state(sb) == -1) {
                if (sb > g.n) obh.insert(sb,Z[sb]/2);
            } else {
                for (let u = bloss.firstIn(sb); u; u = bloss.nextIn(sb,u)) {
                    Z[u] = ovh.key(u); ovh.delete(u);
                    if (bloss.state(sb) == +1) evh.insert(u,Z[u]);
                }
                if (bloss.state(sb) == 0) {
                    exh.activate(sb);
                } else {
                    if (sb > g.n) ebh.insert(sb,Z[sb]/2);
                    exh.clear(sb); addEXedges(sb);
                }
            }
        }
    }

    if (delta == d1) {
        return true; // we have max weight matching
    }
    return false;
}

/** Add ex edges incident to an even blossom or sub-blossom.
 *  @param b is an even blossom or sub-blossom; edges
 *  incident to b in a different outer blossom are added
 *  to either eeh or exh as appropriate 
 */
function addEXedges(b) {
    let bb = bloss.outer(b);
    for (let u = bloss.firstIn(b); u; u = bloss.nextIn(b,u)) {
        for (let e = g.firstAt(u); e; e = g.nextAt(u,e)) {
            steps++;
            let v = g.mate(u,e); let V = bloss.outer(v);
            if (V == bb) continue;
            if (bloss.state(V) == +1)
                eeh.insert(e,slack(e)/2);
            else {// V is odd or unbound
                exh.insertAfter(e, V, slack(e), v+g.edgeRange);
            }
        }
    }
}

/** Find the nearest common ancestor of two vertices in
 *  the current graph.
 *  To avoid excessive search time, search upwards from both vertices in
 *  parallel, using mark bits to identify the nca. Before returning,
 *  clear the mark bits by traversing the paths a second time.
 *  @param U is an outer blossom
 *  @param V is another outer blossom
 *  @returns the nearest common ancestor of u and v or 0 if none
 */
function nca(U, V) {
    let result;
    // first pass to find the nca
    let X = U; let [x,ex] = bloss.link(X);
    let Y = V; let [y,ey] = bloss.link(Y);
    while (true) {
        steps++;
        if (X == Y) { result = X; break; }
        if (mark[X]) { result = X; break; }
        if (mark[Y]) { result = Y; break; }
        if (!x && !y) { result = 0; break; }
        if (x) {
            mark[X] = true;
            X = bloss.outer(g.mate(x,ex)); [x,ex] = bloss.link(X);
        }
        if (y) {
            mark[Y] = true;
            Y = bloss.outer(g.mate(y,ey)); [y,ey] = bloss.link(Y);
        }
    }
    // second pass to clear mark bits
    X = U; [x,ex] = bloss.link(X);
    while (mark[X]) {
        mark[X] = false;
        X = bloss.outer(g.mate(x,ex)); [x,ex] = bloss.link(X);
    }
    Y = V; [y,ey] = bloss.link(Y);
    while (mark[Y]) {
        mark[Y] = false;
        Y = bloss.outer(g.mate(y,ey)); [y,ey] = bloss.link(Y);
    }
    return result;
}
</textarea> <p>
An example of trace output from a demonstration run is shown below.
<p> <textarea rows="20" cols="80" readonly
        style="font-size: 95%;background-color:lightCyan">
{
a[b:3 h:1 j:3 k:3] b[a:3 f:2 g:1 j:3 p:2] c[f:1 m:1] d[g:2]
e[f:2 g:3 i:2 n:1] f[b:2 c:1 e:2 l:2 o:3 p:1] g[b:1 d:2 e:3 k:1 o:1] h[a:1]
i[e:2] j[a:3 b:3] k[a:3 g:1 p:3] l[f:2 o:1]
m[c:1 o:2 p:1] n[e:1] o[f:3 g:1 l:1 m:2] p[b:2 f:1 k:3 m:1]
}
eligible: [ab bj aj ak kp eg fo]

augment: {a,b,3} a--b
augment: {e,g,3} e--g
augment: {f,o,3} f--o
augment: {k,p,3} k--p

branch: j a b
blossom: {b,j,3} j A[j a b]
branch: Aa k p
relab(1.5 1 0.5 Infinity)
    [bp]
blossom: {b,p,2} A B[aAb p k]
relab(1 0.5 0.5 Infinity)
    [ei bf fl dg mo mp ah cm]
augment: {m,p,1} m--pB
	{[B(aAb(j a b) p! k)]}

branch: i e g
augment: {d,g,2}
    {[i(e(g))]}
    d--g e i
    [fo mp dg ei]
	{[B(aAb(j a b) p! k)]}

branch: l f o
branch: o m pB
augment: {a,h,1}
    {[l(f(o(m(B))))]}
    l f o m aBp--h
    [dg ei ah mo fl]
	{[B(aAb(j a! b) p k)]}

branch: c m o
branch: o f l
relab(0.5 1 0.5 Infinity) and finished

final matching: [dg ei ah mo fl kp bj]
</textarea> <p>
This is similar (but not identical) to the trace output for the original
version of Edmond's algorithm.
The following script can be used to compare the performance of
the two versions of Edmond's algorithm on random graphs.
<pre style="padding-left:5%">
let n=200; let d=40; let weights=d*n;
let g = randomGraph(n,d); 
g.randomWeights(randomInteger,1,weights);
let t = Date.now(); let [,,stats] = wmatchE(g); t = Date.now() - t;
log(`e   n=${n} d=${d} weights=${weights} branches=${stats.branches} ` +
    `blossoms=${stats.blossoms} relabels=${stats.relabels} ` +
    `deblossoms=${stats.deblossoms}\n             ` +
    `steps=${stats.steps} ${t}ms`);
t = Date.now(); [,,stats] = wmatchGMG(g); t = Date.now() - t;
log(`gmg n=${n} d=${d} weights=${weights} branches=${stats.branches} ` +
    `blossoms=${stats.blossoms} relabels=${stats.relabels} ` +
    `deblossoms=${stats.deblossoms}\n             ` +
    `steps=${stats.steps} ${t}ms`);
</pre>
Some sample output appears below.
<pre style="padding-left:5%">
e   n=200 d=40 weights=200 branches=753 blossoms=20 relabels=58 deblossoms=11
             steps=616734 36ms 
gmg n=200 d=40 weights=200 branches=767 blossoms=18 relabels=58 deblossoms=11
             steps=112030 147ms
e   n=200 d=40 weights=8000 branches=1152 blossoms=50 relabels=283 deblossoms=2
             steps=1760098 110ms 
gmg n=200 d=40 weights=8000 branches=1164 blossoms=50 relabels=283 deblossoms=2
             steps=133276 162ms 
</pre>
The first two lines show a case where the original version of Edmond's
algorithm substantially out-performs the second one.
The next two lines show a case where the gap in performance is 
substantially smaller.
The second case uses a larger range of edge weights, which produces
a larger number of relabel operations. Since relabel operations
are quite expensive for the original version, this slows it down
by roughly a factor of three.
However, because the number of relabels remains far less
than its worst-case value, the original is still fully competitive
with the improved algorithm, since it does not have the overhead
of the heap operations and the additional $\log n$ factor that the heaps
contribute to the run time.

<h2>References</h2>
<dl>
<dt> [Edmonds65]
<dd>    &ldquo;Maximum Matching and a Polyhedron With O,1-Vertices,&lrquo;
    by J. Edmonds. In <i>Journal Of Research of the National Bureau of
    Standards-B. Mathematics and Mathematical Physics</i>, 1965
<dt> [Gabow76]
<dd> &ldquo;An efficient implementation of Edmond's algorithm for
     maximum matching on graphs,&rdquo; by H. N. Gabow.
     In <i>Journal of the ACM</i>, 1976.
<dt> [GMG86]
<dd> &ldquo;An $O(EV \log V)$ Algorithm for finding a maximal weighted
     matching in general graphs,&rdquo; by Zvi Galil, S. Micali and H. N. Gabow.
     In <i>SIAM Journal on Computing</i>, 1986.
<dt> [Kuhn55]
<dd> &ldquo;The Hungarian method for the assignment problem,&rdquo;
     by H. W. Kuhn. In <i>Naval Research Logistics</i>, 1955.
<dt> [MV80]
<dd> &ldquo;An $O(|E||V|^{1/2})$ algorithm for finding maximal
     matchings in general graphs,&rdquo; by Silvio Micali and Vijay Vazirani.
     In <i>Proceedings of the IEEE Symposium on Foundations of
     Computer Science</i>, 1980.
<dt> [Tarjan87]
<dd> <i>Network Algorithms and Data Structures</i> by Robert E. Tarjan.
     Society for Industrial and Applied Mathematics, 1987.
</dl>
<hr> <h4>&copy; Jonathan Turner - 2022</h4>
<script src="../../googleAnalytics.js"></script>
</body>
</html>
